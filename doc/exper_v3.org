#+title: Experiment Version 3: General-purpose training & Watermarks.
#+date: Tue Jun 11 09:26:47 2024
#+author: Zi Liang
#+email: zi1415926.liang@connect.polyu.hk
#+latex_class: elegantpaper
#+filetags: ::




* General Train
** SHORT-TEXT [0609][on huggingface benchmark]
*** DONE NewTemperatureNewTauLoRD-VIIINewLoss___period500
CLOSED: [2024-06-11 Tue 09:49]
shot: 25, batch_size: auto (16)
|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |    25|acc     |0.5862|±  |0.0144|
|             |       |none  |    25|acc_norm|0.6425|±  |0.0140|

|  Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|
|---------|------:|------|-----:|--------|-----:|---|-----:|
|hellaswag|      1|none  |    10|acc     |0.5747|±  |0.0049|
|         |       |none  |    10|acc_norm|0.8254|±  |0.0038|

|      Tasks      |Version|Filter|n-shot|  Metric   | Value |   |Stderr|
|-----------------|-------|------|-----:|-----------|------:|---|-----:|
|truthfulqa       |N/A    |none  |     0|rouge1_acc | 0.4541|±  |0.0174|
|                 |       |none  |     0|rouge1_max |38.7072|±  |0.8778|
|                 |       |none  |     0|rouge1_diff|-1.3491|±  |0.7542|
|                 |       |none  |     0|bleu_diff  |-0.9982|±  |0.5289|
|                 |       |none  |     0|bleu_acc   | 0.4590|±  |0.0174|
|                 |       |none  |     0|rouge2_max |22.8817|±  |0.8979|
|                 |       |none  |     0|rouge2_acc | 0.3195|±  |0.0163|
|                 |       |none  |     0|rougeL_diff|-1.7465|±  |0.7481|
|                 |       |none  |     0|rougeL_max |35.8348|±  |0.8648|
|                 |       |none  |     0|rouge2_diff|-2.9596|±  |0.7834|
|                 |       |none  |     0|bleu_max   |17.2006|±  |0.6632|
|                 |       |none  |     0|acc        | 0.4393|±  |0.0113|
|                 |       |none  |     0|rougeL_acc | 0.4480|±  |0.0174|
| - truthfulqa_gen|      3|none  |     0|bleu_max   |17.2006|±  |0.6632|
|                 |       |none  |     0|bleu_acc   | 0.4590|±  |0.0174|
|                 |       |none  |     0|bleu_diff  |-0.9982|±  |0.5289|
|                 |       |none  |     0|rouge1_max |38.7072|±  |0.8778|
|                 |       |none  |     0|rouge1_acc | 0.4541|±  |0.0174|
|                 |       |none  |     0|rouge1_diff|-1.3491|±  |0.7542|
|                 |       |none  |     0|rouge2_max |22.8817|±  |0.8979|
|                 |       |none  |     0|rouge2_acc | 0.3195|±  |0.0163|
|                 |       |none  |     0|rouge2_diff|-2.9596|±  |0.7834|
|                 |       |none  |     0|rougeL_max |35.8348|±  |0.8648|
|                 |       |none  |     0|rougeL_acc | 0.4480|±  |0.0174|
|                 |       |none  |     0|rougeL_diff|-1.7465|±  |0.7481|
| - truthfulqa_mc1|      2|none  |     0|acc        | 0.3537|±  |0.0167|
| - truthfulqa_mc2|      2|none  |     0|acc        | 0.5248|±  |0.0151|

|  Groups  |Version|Filter|n-shot|  Metric   | Value |   |Stderr|
|----------|-------|------|-----:|-----------|------:|---|-----:|
|truthfulqa|N/A    |none  |     0|rouge1_acc | 0.4541|±  |0.0174|
|          |       |none  |     0|rouge1_max |38.7072|±  |0.8778|
|          |       |none  |     0|rouge1_diff|-1.3491|±  |0.7542|
|          |       |none  |     0|bleu_diff  |-0.9982|±  |0.5289|
|          |       |none  |     0|bleu_acc   | 0.4590|±  |0.0174|
|          |       |none  |     0|rouge2_max |22.8817|±  |0.8979|
|          |       |none  |     0|rouge2_acc | 0.3195|±  |0.0163|
|          |       |none  |     0|rougeL_diff|-1.7465|±  |0.7481|
|          |       |none  |     0|rougeL_max |35.8348|±  |0.8648|
|          |       |none  |     0|rouge2_diff|-2.9596|±  |0.7834|
|          |       |none  |     0|bleu_max   |17.2006|±  |0.6632|
|          |       |none  |     0|acc        | 0.4393|±  |0.0113|
|          |       |none  |     0|rougeL_acc | 0.4480|±  |0.0174|

|                 Tasks                 |Version|Filter|n-shot|Metric|Value |   |Stderr|
|---------------------------------------|-------|------|-----:|------|-----:|---|-----:|
|mmlu                                   |N/A    |none  |     0|acc   |0.6493|±  |0.0038|
|  - abstract_algebra                   |      0|none  |     5|acc   |0.3400|±  |0.0476|
|  - anatomy                            |      0|none  |     5|acc   |0.6148|±  |0.0420|
|  - astronomy                          |      0|none  |     5|acc   |0.7171|±  |0.0367|
|  - business_ethics                    |      0|none  |     5|acc   |0.7000|±  |0.0461|
|  - clinical_knowledge                 |      0|none  |     5|acc   |0.7509|±  |0.0266|
|  - college_biology                    |      0|none  |     5|acc   |0.7986|±  |0.0335|
|  - college_chemistry                  |      0|none  |     5|acc   |0.5100|±  |0.0502|
|  - college_computer_science           |      0|none  |     5|acc   |0.5500|±  |0.0500|
|  - college_mathematics                |      0|none  |     5|acc   |0.4000|±  |0.0492|
|  - college_medicine                   |      0|none  |     5|acc   |0.6532|±  |0.0363|
|  - college_physics                    |      0|none  |     5|acc   |0.5196|±  |0.0497|
|  - computer_security                  |      0|none  |     5|acc   |0.7800|±  |0.0416|
|  - conceptual_physics                 |      0|none  |     5|acc   |0.6213|±  |0.0317|
|  - econometrics                       |      0|none  |     5|acc   |0.5702|±  |0.0466|
|  - electrical_engineering             |      0|none  |     5|acc   |0.6276|±  |0.0403|
|  - elementary_mathematics             |      0|none  |     5|acc   |0.4233|±  |0.0254|
|  - formal_logic                       |      0|none  |     5|acc   |0.5079|±  |0.0447|
|  - global_facts                       |      0|none  |     5|acc   |0.4300|±  |0.0498|
|  - high_school_biology                |      0|none  |     5|acc   |0.7839|±  |0.0234|
|  - high_school_chemistry              |      0|none  |     5|acc   |0.5419|±  |0.0351|
|  - high_school_computer_science       |      0|none  |     5|acc   |0.7100|±  |0.0456|
|  - high_school_european_history       |      0|none  |     5|acc   |0.7697|±  |0.0329|
|  - high_school_geography              |      0|none  |     5|acc   |0.8182|±  |0.0275|
|  - high_school_government_and_politics|      0|none  |     5|acc   |0.8964|±  |0.0220|
|  - high_school_macroeconomics         |      0|none  |     5|acc   |0.6590|±  |0.0240|
|  - high_school_mathematics            |      0|none  |     5|acc   |0.3704|±  |0.0294|
|  - high_school_microeconomics         |      0|none  |     5|acc   |0.7479|±  |0.0282|
|  - high_school_physics                |      0|none  |     5|acc   |0.4172|±  |0.0403|
|  - high_school_psychology             |      0|none  |     5|acc   |0.8385|±  |0.0158|
|  - high_school_statistics             |      0|none  |     5|acc   |0.5324|±  |0.0340|
|  - high_school_us_history             |      0|none  |     5|acc   |0.8529|±  |0.0249|
|  - high_school_world_history          |      0|none  |     5|acc   |0.8228|±  |0.0249|
|  - human_aging                        |      0|none  |     5|acc   |0.7130|±  |0.0304|
|  - human_sexuality                    |      0|none  |     5|acc   |0.8015|±  |0.0350|
| - humanities                          |N/A    |none  |     5|acc   |0.5960|±  |0.0068|
|  - international_law                  |      0|none  |     5|acc   |0.7934|±  |0.0370|
|  - jurisprudence                      |      0|none  |     5|acc   |0.7778|±  |0.0402|
|  - logical_fallacies                  |      0|none  |     5|acc   |0.7975|±  |0.0316|
|  - machine_learning                   |      0|none  |     5|acc   |0.5357|±  |0.0473|
|  - management                         |      0|none  |     5|acc   |0.7961|±  |0.0399|
|  - marketing                          |      0|none  |     5|acc   |0.8974|±  |0.0199|
|  - medical_genetics                   |      0|none  |     5|acc   |0.7900|±  |0.0409|
|  - miscellaneous                      |      0|none  |     5|acc   |0.7752|±  |0.0149|
|  - moral_disputes                     |      0|none  |     5|acc   |0.7341|±  |0.0238|
|  - moral_scenarios                    |      0|none  |     5|acc   |0.4168|±  |0.0165|
|  - nutrition                          |      0|none  |     5|acc   |0.7484|±  |0.0248|
| - other                               |N/A    |none  |     5|acc   |0.7100|±  |0.0079|
|  - philosophy                         |      0|none  |     5|acc   |0.6945|±  |0.0262|
|  - prehistory                         |      0|none  |     5|acc   |0.7099|±  |0.0253|
|  - professional_accounting            |      0|none  |     5|acc   |0.5071|±  |0.0298|
|  - professional_law                   |      0|none  |     5|acc   |0.4759|±  |0.0128|
|  - professional_medicine              |      0|none  |     5|acc   |0.7132|±  |0.0275|
|  - professional_psychology            |      0|none  |     5|acc   |0.6879|±  |0.0187|
|  - public_relations                   |      0|none  |     5|acc   |0.6727|±  |0.0449|
|  - security_studies                   |      0|none  |     5|acc   |0.7265|±  |0.0285|
| - social_sciences                     |N/A    |none  |     5|acc   |0.7576|±  |0.0076|
|  - sociology                          |      0|none  |     5|acc   |0.8657|±  |0.0241|
| - stem                                |N/A    |none  |     5|acc   |0.5636|±  |0.0085|
|  - us_foreign_policy                  |      0|none  |     5|acc   |0.8700|±  |0.0338|
|  - virology                           |      0|none  |     5|acc   |0.4699|±  |0.0389|
|  - world_religions                    |      0|none  |     5|acc   |0.7661|±  |0.0325|

|      Groups      |Version|Filter|n-shot|Metric|Value |   |Stderr|
|------------------|-------|------|-----:|------|-----:|---|-----:|
|mmlu              |N/A    |none  |     0|acc   |0.6493|±  |0.0038|
| - humanities     |N/A    |none  |     5|acc   |0.5960|±  |0.0068|
| - other          |N/A    |none  |     5|acc   |0.7100|±  |0.0079|
| - social_sciences|N/A    |none  |     5|acc   |0.7576|±  |0.0076|
| - stem           |N/A    |none  |     5|acc   |0.5636|±  |0.0085|


| Tasks      | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|------------+---------+--------+--------+--------+--------+---+--------|
| winogrande |       1 | none   |      5 | acc    | 0.7766 | ± | 0.0117 |

| Tasks | Version | Filter           | n-shot | Metric      |  Value |   | Stderr |
|-------+---------+------------------+--------+-------------+--------+---+--------|
| gsm8k |       3 | strict-match     |      5 | exact_match | 0.7111 | ± | 0.0125 |
|       |         | flexible-extract |      5 | exact_match | 0.7195 | ± | 0.0124 |
*** DONE NewTemperatureNewLoss___period500
CLOSED: [2024-06-11 Tue 09:49]

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |     25 | acc      | 0.5725 | ± | 0.0145 |
|               |         | none   |     25 | acc_norm | 0.6246 | ± | 0.0142 |

| Tasks     | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|-----------+---------+--------+--------+----------+--------+---+--------|
| hellaswag |       1 | none   |     10 | acc      | 0.5782 | ± | 0.0049 |
|           |         | none   |     10 | acc_norm | 0.7946 | ± | 0.0040 |

| Tasks            | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa       |     N/A | none   |      0 | rougeL_max  | 44.7747 | ± | 0.9330 |
|                  |         | none   |      0 | acc         |  0.4307 | ± | 0.0111 |
|                  |         | none   |      0 | rouge1_diff |  3.4765 | ± | 1.0987 |
|                  |         | none   |      0 | rouge2_max  | 30.5415 | ± | 1.0850 |
|                  |         | none   |      0 | rouge2_acc  |  0.3562 | ± | 0.0168 |
|                  |         | none   |      0 | rougeL_acc  |  0.5031 | ± | 0.0175 |
|                  |         | none   |      0 | bleu_max    | 23.8401 | ± | 0.7920 |
|                  |         | none   |      0 | rouge1_acc  |  0.5129 | ± | 0.0175 |
|                  |         | none   |      0 | bleu_diff   |  2.0199 | ± | 0.7919 |
|                  |         | none   |      0 | rouge2_diff |  1.3540 | ± | 1.1675 |
|                  |         | none   |      0 | rougeL_diff |  3.0595 | ± | 1.1036 |
|                  |         | none   |      0 | rouge1_max  | 47.4034 | ± | 0.9230 |
|                  |         | none   |      0 | bleu_acc    |  0.5141 | ± | 0.0175 |
| - truthfulqa_gen |       3 | none   |      0 | bleu_max    | 23.8401 | ± | 0.7920 |
|                  |         | none   |      0 | bleu_acc    |  0.5141 | ± | 0.0175 |
|                  |         | none   |      0 | bleu_diff   |  2.0199 | ± | 0.7919 |
|                  |         | none   |      0 | rouge1_max  | 47.4034 | ± | 0.9230 |
|                  |         | none   |      0 | rouge1_acc  |  0.5129 | ± | 0.0175 |
|                  |         | none   |      0 | rouge1_diff |  3.4765 | ± | 1.0987 |
|                  |         | none   |      0 | rouge2_max  | 30.5415 | ± | 1.0850 |
|                  |         | none   |      0 | rouge2_acc  |  0.3562 | ± | 0.0168 |
|                  |         | none   |      0 | rouge2_diff |  1.3540 | ± | 1.1675 |
|                  |         | none   |      0 | rougeL_max  | 44.7747 | ± | 0.9330 |
|                  |         | none   |      0 | rougeL_acc  |  0.5031 | ± | 0.0175 |
|                  |         | none   |      0 | rougeL_diff |  3.0595 | ± | 1.1036 |
| - truthfulqa_mc1 |       2 | none   |      0 | acc         |  0.3427 | ± | 0.0166 |
| - truthfulqa_mc2 |       2 | none   |      0 | acc         |  0.5186 | ± | 0.0147 |

| Groups     | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa | N/A     | none   |      0 | rougeL_max  | 44.7747 | ± | 0.9330 |
|            |         | none   |      0 | acc         |  0.4307 | ± | 0.0111 |
|            |         | none   |      0 | rouge1_diff |  3.4765 | ± | 1.0987 |
|            |         | none   |      0 | rouge2_max  | 30.5415 | ± | 1.0850 |
|            |         | none   |      0 | rouge2_acc  |  0.3562 | ± | 0.0168 |
|            |         | none   |      0 | rougeL_acc  |  0.5031 | ± | 0.0175 |
|            |         | none   |      0 | bleu_max    | 23.8401 | ± | 0.7920 |
|            |         | none   |      0 | rouge1_acc  |  0.5129 | ± | 0.0175 |
|            |         | none   |      0 | bleu_diff   |  2.0199 | ± | 0.7919 |
|            |         | none   |      0 | rouge2_diff |  1.3540 | ± | 1.1675 |
|            |         | none   |      0 | rougeL_diff |  3.0595 | ± | 1.1036 |
|            |         | none   |      0 | rouge1_max  | 47.4034 | ± | 0.9230 |
|            |         | none   |      0 | bleu_acc    |  0.5141 | ± | 0.0175 |

| Tasks                                 | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|---------------------------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu                                  |     N/A | none   |      0 | acc    | 0.6583 | ± | 0.0038 |
| - abstract_algebra                    |       0 | none   |      5 | acc    | 0.2700 | ± | 0.0446 |
| - anatomy                             |       0 | none   |      5 | acc    | 0.6444 | ± | 0.0414 |
| - astronomy                           |       0 | none   |      5 | acc    | 0.7368 | ± | 0.0358 |
| - business_ethics                     |       0 | none   |      5 | acc    | 0.7000 | ± | 0.0461 |
| - clinical_knowledge                  |       0 | none   |      5 | acc    | 0.7509 | ± | 0.0266 |
| - college_biology                     |       0 | none   |      5 | acc    | 0.7986 | ± | 0.0335 |
| - college_chemistry                   |       0 | none   |      5 | acc    | 0.4800 | ± | 0.0502 |
| - college_computer_science            |       0 | none   |      5 | acc    | 0.5700 | ± | 0.0498 |
| - college_mathematics                 |       0 | none   |      5 | acc    | 0.3700 | ± | 0.0485 |
| - college_medicine                    |       0 | none   |      5 | acc    | 0.6416 | ± | 0.0366 |
| - college_physics                     |       0 | none   |      5 | acc    | 0.4902 | ± | 0.0497 |
| - computer_security                   |       0 | none   |      5 | acc    | 0.7600 | ± | 0.0429 |
| - conceptual_physics                  |       0 | none   |      5 | acc    | 0.6128 | ± | 0.0318 |
| - econometrics                        |       0 | none   |      5 | acc    | 0.5877 | ± | 0.0463 |
| - electrical_engineering              |       0 | none   |      5 | acc    | 0.6345 | ± | 0.0401 |
| - elementary_mathematics              |       0 | none   |      5 | acc    | 0.4524 | ± | 0.0256 |
| - formal_logic                        |       0 | none   |      5 | acc    | 0.5079 | ± | 0.0447 |
| - global_facts                        |       0 | none   |      5 | acc    | 0.4100 | ± | 0.0494 |
| - high_school_biology                 |       0 | none   |      5 | acc    | 0.7839 | ± | 0.0234 |
| - high_school_chemistry               |       0 | none   |      5 | acc    | 0.5271 | ± | 0.0351 |
| - high_school_computer_science        |       0 | none   |      5 | acc    | 0.7700 | ± | 0.0423 |
| - high_school_european_history        |       0 | none   |      5 | acc    | 0.7394 | ± | 0.0343 |
| - high_school_geography               |       0 | none   |      5 | acc    | 0.8384 | ± | 0.0262 |
| - high_school_government_and_politics |       0 | none   |      5 | acc    | 0.9067 | ± | 0.0210 |
| - high_school_macroeconomics          |       0 | none   |      5 | acc    | 0.6513 | ± | 0.0242 |
| - high_school_mathematics             |       0 | none   |      5 | acc    | 0.3815 | ± | 0.0296 |
| - high_school_microeconomics          |       0 | none   |      5 | acc    | 0.7689 | ± | 0.0274 |
| - high_school_physics                 |       0 | none   |      5 | acc    | 0.4305 | ± | 0.0404 |
| - high_school_psychology              |       0 | none   |      5 | acc    | 0.8495 | ± | 0.0153 |
| - high_school_statistics              |       0 | none   |      5 | acc    | 0.5602 | ± | 0.0339 |
| - high_school_us_history              |       0 | none   |      5 | acc    | 0.8529 | ± | 0.0249 |
| - high_school_world_history           |       0 | none   |      5 | acc    | 0.8481 | ± | 0.0234 |
| - human_aging                         |       0 | none   |      5 | acc    | 0.7309 | ± | 0.0298 |
| - human_sexuality                     |       0 | none   |      5 | acc    | 0.7863 | ± | 0.0360 |
| - humanities                          |     N/A | none   |      5 | acc    | 0.6055 | ± | 0.0068 |
| - international_law                   |       0 | none   |      5 | acc    | 0.8099 | ± | 0.0358 |
| - jurisprudence                       |       0 | none   |      5 | acc    | 0.7778 | ± | 0.0402 |
| - logical_fallacies                   |       0 | none   |      5 | acc    | 0.7853 | ± | 0.0323 |
| - machine_learning                    |       0 | none   |      5 | acc    | 0.5268 | ± | 0.0474 |
| - management                          |       0 | none   |      5 | acc    | 0.7864 | ± | 0.0406 |
| - marketing                           |       0 | none   |      5 | acc    | 0.9188 | ± | 0.0179 |
| - medical_genetics                    |       0 | none   |      5 | acc    | 0.8000 | ± | 0.0402 |
| - miscellaneous                       |       0 | none   |      5 | acc    | 0.7931 | ± | 0.0145 |
| - moral_disputes                      |       0 | none   |      5 | acc    | 0.7457 | ± | 0.0234 |
| - moral_scenarios                     |       0 | none   |      5 | acc    | 0.4380 | ± | 0.0166 |
| - nutrition                           |       0 | none   |      5 | acc    | 0.7647 | ± | 0.0243 |
| - other                               |     N/A | none   |      5 | acc    | 0.7216 | ± | 0.0078 |
| - philosophy                          |       0 | none   |      5 | acc    | 0.7267 | ± | 0.0253 |
| - prehistory                          |       0 | none   |      5 | acc    | 0.7222 | ± | 0.0249 |
| - professional_accounting             |       0 | none   |      5 | acc    | 0.5284 | ± | 0.0298 |
| - professional_law                    |       0 | none   |      5 | acc    | 0.4798 | ± | 0.0128 |
| - professional_medicine               |       0 | none   |      5 | acc    | 0.7169 | ± | 0.0274 |
| - professional_psychology             |       0 | none   |      5 | acc    | 0.7092 | ± | 0.0184 |
| - public_relations                    |       0 | none   |      5 | acc    | 0.6545 | ± | 0.0455 |
| - security_studies                    |       0 | none   |      5 | acc    | 0.7469 | ± | 0.0278 |
| - social_sciences                     |     N/A | none   |      5 | acc    | 0.7676 | ± | 0.0075 |
| - sociology                           |       0 | none   |      5 | acc    | 0.8806 | ± | 0.0229 |
| - stem                                |     N/A | none   |      5 | acc    | 0.5680 | ± | 0.0084 |
| - us_foreign_policy                   |       0 | none   |      5 | acc    | 0.8500 | ± | 0.0359 |
| - virology                            |       0 | none   |      5 | acc    | 0.5000 | ± | 0.0389 |
| - world_religions                     |       0 | none   |      5 | acc    | 0.7719 | ± | 0.0322 |

| Groups            | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|-------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu              | N/A     | none   |      0 | acc    | 0.6583 | ± | 0.0038 |
| - humanities      | N/A     | none   |      5 | acc    | 0.6055 | ± | 0.0068 |
| - other           | N/A     | none   |      5 | acc    | 0.7216 | ± | 0.0078 |
| - social_sciences | N/A     | none   |      5 | acc    | 0.7676 | ± | 0.0075 |
| - stem            | N/A     | none   |      5 | acc    | 0.5680 | ± | 0.0084 |

| Tasks      | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|------------+---------+--------+--------+--------+--------+---+--------|
| winogrande |       1 | none   |      5 | acc    | 0.7632 | ± | 0.0119 |

|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|
|-----|------:|----------------|-----:|-----------|-----:|---|-----:|
|gsm8k|      3|strict-match    |     5|exact_match|0.7293|±  |0.0122|
|     |       |flexible-extract|     5|exact_match|0.7422|±  |0.0120|
*** TODO NewTemperatureNewLoss___finally
*** TODO Pretrained llama3-8B
** TODO Longtext
** GPT-4 cleaned
*** TODO With 256 samples
*** TODO With 1000 samples
** Watermark: data-to-text
We expect in the watermark experiments:
+ P-value: LoRD> vanilla
+ Green-word fraction: LoRD < vanilla
+ Z-score: LoRD<vanilla
*** DONE Now the results: with 64 times of querys
CLOSED: [2024-06-11 Tue 09:49]
#+begin_src python

{'allenai/common_gen-----__watermark__d2t_ckpts__D2TTTallenai__common_gen@wrmk641LoRD-VI___period512_____allenai__common_gen@wrmk_d2t_infer_resjson': {'bertscore': {'f1': 0.8722925782203674,
                                                                                                                                                                     'p': 0.8345624208450317,
                                                                                                                                                                     'r': 0.9137428998947144},
                                                                                                                                                       'bleu': {'1': 0.35043698900479286,
                                                                                                                                                                '2': 0.2032080966682189,
                                                                                                                                                                '3': 0.12093166283138004,
                                                                                                                                                                '4': 0.07363683945946936},
                                                                                                                                                       'green_fraction': 0.2647058823529412,
                                                                                                                                                       'num_green_tokens': 9,
                                                                                                                                                       'num_tokens_scored': 34,
                                                                                                                                                       'p_value': 0.42151098818486243,
                                                                                                                                                       'prediction': False,
                                                                                                                                                       'rouge-l': {'f1': 0.3796643044590478,
                                                                                                                                                                   'p': 0.35598089286897266,
                                                                                                                                                                   'r': 0.4237586268588298},
                                                                                                                                                       'z_score': 0.19802950859533489},
 'allenai/common_gen-----__watermark__d2t_ckpts__D2TTTallenai__common_gen@wrmk641vanilla___finally_____allenai__common_gen@wrmk_d2t_infer_resjson': {'bertscore': {'f1': 0.8726513385772705,
                                                                                                                                                                   'p': 0.8354629278182983,
                                                                                                                                                                   'r': 0.9134514927864075},
                                                                                                                                                     'bleu': {'1': 0.3178174295319369,
                                                                                                                                                              '2': 0.17969801208376904,
                                                                                                                                                              '3': 0.10361766613745924,
                                                                                                                                                              '4': 0.06146900055516714},
                                                                                                                                                     'green_fraction': 0.38235294117647056,
                                                                                                                                                     'num_green_tokens': 13,
                                                                                                                                                     'num_tokens_scored': 34,
                                                                                                                                                     'p_value': 0.03735296665606522,
                                                                                                                                                     'prediction': False,
                                                                                                                                                     'rouge-l': {'f1': 0.35694429477240597,
                                                                                                                                                                 'p': 0.323578978766058,
                                                                                                                                                                 'r': 0.41506365007215607},
                                                                                                                                                     'z_score': 1.7822655773580138},
 'e2e_nlg-----__watermark__d2t_ckpts__D2TTTe2e_nlg@wrmk641LoRD-VI___period512_____e2e_nlg@wrmk_d2t_infer_resjson': {'bertscore': {'f1': 0.9073704481124878,
                                                                                                                                  'p': 0.8802347779273987,
                                                                                                                                  'r': 0.9364007115364075},
                                                                                                                    'bleu': {'1': 0.49008658053436366,
                                                                                                                             '2': 0.33756781232336586,
                                                                                                                             '3': 0.23743468812796212,
                                                                                                                             '4': 0.1619372156200498},
                                                                                                                    'green_fraction': 0.27906976744186046,
                                                                                                                    'num_green_tokens': 12,
                                                                                                                    'num_tokens_scored': 43,
                                                                                                                    'p_value': 0.3298869132560869,
                                                                                                                    'prediction': False,
                                                                                                                    'rouge-l': {'f1': 0.4443390372515331,
                                                                                                                                'p': 0.44010526350393564,
                                                                                                                                'r': 0.459521135438056},
                                                                                                                    'z_score': 0.4402254531628119},
 'e2e_nlg-----__watermark__d2t_ckpts__D2TTTe2e_nlg@wrmk641vanilla___finally_____e2e_nlg@wrmk_d2t_infer_resjson': {'bertscore': {'f1': 0.909862220287323,
                                                                                                                                'p': 0.8831789493560791,
                                                                                                                                'r': 0.9383627772331238},
                                                                                                                  'bleu': {'1': 0.504872797454937,
                                                                                                                           '2': 0.34240284009039,
                                                                                                                           '3': 0.23611685741884592,
                                                                                                                           '4': 0.1581794014781304},
                                                                                                                  'green_fraction': 0.3023255813953488,
                                                                                                                  'num_green_tokens': 13,
                                                                                                                  'num_tokens_scored': 43,
                                                                                                                  'p_value': 0.21406204473308316,
                                                                                                                  'prediction': False,
                                                                                                                  'rouge-l': {'f1': 0.4512897178346773,
                                                                                                                              'p': 0.4456838476446302,
                                                                                                                              'r': 0.4678684280080462},
                                                                                                                  'z_score': 0.7924058156930615}}


#+end_src
*** TODO Now the results: varying sequence length
** Watermark: Machine translation
*** TODO Now the results: varying sequence length

