#+title: Experiment Version 3: General-purpose training & Watermarks.
#+date: Tue Jun 11 09:26:47 2024
#+author: Zi Liang
#+email: zi1415926.liang@connect.polyu.hk
#+latex_class: elegantpaper
#+filetags: :ExperimentResults:GeneralTraining:


* General Train
** Phi-3 MINI experiments
*** LoRD-VI
|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |    25|acc     |0.6135|±  |0.0142|
|             |       |none  |    25|acc_norm|0.6348|±  |0.0141|
*** Vanilla
|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |    25|acc     |0.6305|±  |0.0141|
|             |       |none  |    25|acc_norm|0.6416|±  |0.0140|

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |     25 | acc      | 0.6254 | ± | 0.0141 |
|               |         | none   |     25 | acc_norm | 0.6459 | ± | 0.0140 |
*** LoRD-IX 
*** LoRD-VII
*** LoRD-VIII

** SHORT-TEXT [0609][on huggingface benchmark]
*** DONE Pretrained llama3-8B
CLOSED: [2024-06-11 Tue 19:55]
| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |     25 | acc      | 0.5819 | ± | 0.0144 |
|               |         | none   |     25 | acc_norm | 0.6212 | ± | 0.0142 |

| Tasks     | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|-----------+---------+--------+--------+----------+--------+---+--------|
| hellaswag |       1 | none   |     10 | acc      | 0.5866 | ± | 0.0049 |
|           |         | none   |     10 | acc_norm | 0.7880 | ± | 0.0041 |

| Tasks            | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa       |     N/A | none   |      0 | rouge1_diff | -0.5778 | ± | 0.8583 |
|                  |         | none   |      0 | rougeL_diff | -0.9836 | ± | 0.8640 |
|                  |         | none   |      0 | bleu_max    | 20.2271 | ± | 0.7331 |
|                  |         | none   |      0 | bleu_acc    |  0.4676 | ± | 0.0175 |
|                  |         | none   |      0 | rouge1_acc  |  0.4933 | ± | 0.0175 |
|                  |         | none   |      0 | rouge1_max  | 43.3302 | ± | 0.8727 |
|                  |         | none   |      0 | rouge2_max  | 27.3249 | ± | 0.9582 |
|                  |         | none   |      0 | bleu_diff   | -0.5367 | ± | 0.6300 |
|                  |         | none   |      0 | rougeL_acc  |  0.4798 | ± | 0.0175 |
|                  |         | none   |      0 | rouge2_acc  |  0.3611 | ± | 0.0168 |
|                  |         | none   |      0 | rouge2_diff | -1.8765 | ± | 0.9087 |
|                  |         | none   |      0 | rougeL_max  | 40.4405 | ± | 0.8715 |
|                  |         | none   |      0 | acc         |  0.4382 | ± | 0.0113 |
| - truthfulqa_gen |       3 | none   |      0 | bleu_max    | 20.2271 | ± | 0.7331 |
|                  |         | none   |      0 | bleu_acc    |  0.4676 | ± | 0.0175 |
|                  |         | none   |      0 | bleu_diff   | -0.5367 | ± | 0.6300 |
|                  |         | none   |      0 | rouge1_max  | 43.3302 | ± | 0.8727 |
|                  |         | none   |      0 | rouge1_acc  |  0.4933 | ± | 0.0175 |
|                  |         | none   |      0 | rouge1_diff | -0.5778 | ± | 0.8583 |
|                  |         | none   |      0 | rouge2_max  | 27.3249 | ± | 0.9582 |
|                  |         | none   |      0 | rouge2_acc  |  0.3611 | ± | 0.0168 |
|                  |         | none   |      0 | rouge2_diff | -1.8765 | ± | 0.9087 |
|                  |         | none   |      0 | rougeL_max  | 40.4405 | ± | 0.8715 |
|                  |         | none   |      0 | rougeL_acc  |  0.4798 | ± | 0.0175 |
|                  |         | none   |      0 | rougeL_diff | -0.9836 | ± | 0.8640 |
| - truthfulqa_mc1 |       2 | none   |      0 | acc         |  0.3599 | ± | 0.0168 |
| - truthfulqa_mc2 |       2 | none   |      0 | acc         |  0.5166 | ± | 0.0152 |



| Groups     | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa | N/A     | none   |      0 | rouge1_diff | -0.5778 | ± | 0.8583 |
|            |         | none   |      0 | rougeL_diff | -0.9836 | ± | 0.8640 |
|            |         | none   |      0 | bleu_max    | 20.2271 | ± | 0.7331 |
|            |         | none   |      0 | bleu_acc    |  0.4676 | ± | 0.0175 |
|            |         | none   |      0 | rouge1_acc  |  0.4933 | ± | 0.0175 |
|            |         | none   |      0 | rouge1_max  | 43.3302 | ± | 0.8727 |
|            |         | none   |      0 | rouge2_max  | 27.3249 | ± | 0.9582 |
|            |         | none   |      0 | bleu_diff   | -0.5367 | ± | 0.6300 |
|            |         | none   |      0 | rougeL_acc  |  0.4798 | ± | 0.0175 |
|            |         | none   |      0 | rouge2_acc  |  0.3611 | ± | 0.0168 |
|            |         | none   |      0 | rouge2_diff | -1.8765 | ± | 0.9087 |
|            |         | none   |      0 | rougeL_max  | 40.4405 | ± | 0.8715 |
|            |         | none   |      0 | acc         |  0.4382 | ± | 0.0113 |

| Tasks                                 | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|---------------------------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu                                  |     N/A | none   |      0 | acc    | 0.6581 | ± | 0.0038 |
| - abstract_algebra                    |       0 | none   |      5 | acc    | 0.3300 | ± | 0.0473 |
| - anatomy                             |       0 | none   |      5 | acc    | 0.6444 | ± | 0.0414 |
| - astronomy                           |       0 | none   |      5 | acc    | 0.6974 | ± | 0.0374 |
| - business_ethics                     |       0 | none   |      5 | acc    | 0.7000 | ± | 0.0461 |
| - clinical_knowledge                  |       0 | none   |      5 | acc    | 0.7472 | ± | 0.0267 |
| - college_biology                     |       0 | none   |      5 | acc    | 0.7917 | ± | 0.0340 |
| - college_chemistry                   |       0 | none   |      5 | acc    | 0.4800 | ± | 0.0502 |
| - college_computer_science            |       0 | none   |      5 | acc    | 0.5900 | ± | 0.0494 |
| - college_mathematics                 |       0 | none   |      5 | acc    | 0.3800 | ± | 0.0488 |
| - college_medicine                    |       0 | none   |      5 | acc    | 0.6358 | ± | 0.0367 |
| - college_physics                     |       0 | none   |      5 | acc    | 0.5294 | ± | 0.0497 |
| - computer_security                   |       0 | none   |      5 | acc    | 0.7700 | ± | 0.0423 |
| - conceptual_physics                  |       0 | none   |      5 | acc    | 0.5957 | ± | 0.0321 |
| - econometrics                        |       0 | none   |      5 | acc    | 0.6228 | ± | 0.0456 |
| - electrical_engineering              |       0 | none   |      5 | acc    | 0.6414 | ± | 0.0400 |
| - elementary_mathematics              |       0 | none   |      5 | acc    | 0.4418 | ± | 0.0256 |
| - formal_logic                        |       0 | none   |      5 | acc    | 0.5000 | ± | 0.0447 |
| - global_facts                        |       0 | none   |      5 | acc    | 0.4300 | ± | 0.0498 |
| - high_school_biology                 |       0 | none   |      5 | acc    | 0.7839 | ± | 0.0234 |
| - high_school_chemistry               |       0 | none   |      5 | acc    | 0.5123 | ± | 0.0352 |
| - high_school_computer_science        |       0 | none   |      5 | acc    | 0.7500 | ± | 0.0435 |
| - high_school_european_history        |       0 | none   |      5 | acc    | 0.7515 | ± | 0.0337 |
| - high_school_geography               |       0 | none   |      5 | acc    | 0.8384 | ± | 0.0262 |
| - high_school_government_and_politics |       0 | none   |      5 | acc    | 0.9119 | ± | 0.0205 |
| - high_school_macroeconomics          |       0 | none   |      5 | acc    | 0.6615 | ± | 0.0240 |
| - high_school_mathematics             |       0 | none   |      5 | acc    | 0.3778 | ± | 0.0296 |
| - high_school_microeconomics          |       0 | none   |      5 | acc    | 0.7647 | ± | 0.0276 |
| - high_school_physics                 |       0 | none   |      5 | acc    | 0.4437 | ± | 0.0406 |
| - high_school_psychology              |       0 | none   |      5 | acc    | 0.8440 | ± | 0.0156 |
| - high_school_statistics              |       0 | none   |      5 | acc    | 0.5278 | ± | 0.0340 |
| - high_school_us_history              |       0 | none   |      5 | acc    | 0.8578 | ± | 0.0245 |
| - high_school_world_history           |       0 | none   |      5 | acc    | 0.8439 | ± | 0.0236 |
| - human_aging                         |       0 | none   |      5 | acc    | 0.7175 | ± | 0.0302 |
| - human_sexuality                     |       0 | none   |      5 | acc    | 0.7939 | ± | 0.0355 |
| - humanities                          |     N/A | none   |      5 | acc    | 0.6055 | ± | 0.0068 |
| - international_law                   |       0 | none   |      5 | acc    | 0.8182 | ± | 0.0352 |
| - jurisprudence                       |       0 | none   |      5 | acc    | 0.7778 | ± | 0.0402 |
| - logical_fallacies                   |       0 | none   |      5 | acc    | 0.7791 | ± | 0.0326 |
| - machine_learning                    |       0 | none   |      5 | acc    | 0.5446 | ± | 0.0473 |
| - management                          |       0 | none   |      5 | acc    | 0.7864 | ± | 0.0406 |
| - marketing                           |       0 | none   |      5 | acc    | 0.9017 | ± | 0.0195 |
| - medical_genetics                    |       0 | none   |      5 | acc    | 0.8300 | ± | 0.0378 |
| - miscellaneous                       |       0 | none   |      5 | acc    | 0.8020 | ± | 0.0142 |
| - moral_disputes                      |       0 | none   |      5 | acc    | 0.7457 | ± | 0.0234 |
| - moral_scenarios                     |       0 | none   |      5 | acc    | 0.4291 | ± | 0.0166 |
| - nutrition                           |       0 | none   |      5 | acc    | 0.7516 | ± | 0.0247 |
| - other                               |     N/A | none   |      5 | acc    | 0.7229 | ± | 0.0078 |
| - philosophy                          |       0 | none   |      5 | acc    | 0.7203 | ± | 0.0255 |
| - prehistory                          |       0 | none   |      5 | acc    | 0.7407 | ± | 0.0244 |
| - professional_accounting             |       0 | none   |      5 | acc    | 0.5355 | ± | 0.0298 |
| - professional_law                    |       0 | none   |      5 | acc    | 0.4817 | ± | 0.0128 |
| - professional_medicine               |       0 | none   |      5 | acc    | 0.7243 | ± | 0.0271 |
| - professional_psychology             |       0 | none   |      5 | acc    | 0.7092 | ± | 0.0184 |
| - public_relations                    |       0 | none   |      5 | acc    | 0.6636 | ± | 0.0453 |
| - security_studies                    |       0 | none   |      5 | acc    | 0.7388 | ± | 0.0281 |
| - social_sciences                     |     N/A | none   |      5 | acc    | 0.7683 | ± | 0.0075 |
| - sociology                           |       0 | none   |      5 | acc    | 0.8607 | ± | 0.0245 |
| - stem                                |     N/A | none   |      5 | acc    | 0.5652 | ± | 0.0085 |
| - us_foreign_policy                   |       0 | none   |      5 | acc    | 0.8600 | ± | 0.0349 |
| - virology                            |       0 | none   |      5 | acc    | 0.5060 | ± | 0.0389 |
| - world_religions                     |       0 | none   |      5 | acc    | 0.7719 | ± | 0.0322 |

| Groups            | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|-------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu              | N/A     | none   |      0 | acc    | 0.6581 | ± | 0.0038 |
| - humanities      | N/A     | none   |      5 | acc    | 0.6055 | ± | 0.0068 |
| - other           | N/A     | none   |      5 | acc    | 0.7229 | ± | 0.0078 |
| - social_sciences | N/A     | none   |      5 | acc    | 0.7683 | ± | 0.0075 |
| - stem            | N/A     | none   |      5 | acc    | 0.5652 | ± | 0.0085 |

| Tasks      | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|------------+---------+--------+--------+--------+--------+---+--------|
| winogrande |       1 | none   |      5 | acc    | 0.7569 | ± | 0.0121 |


| Tasks | Version | Filter           | n-shot | Metric      |  Value |   | Stderr |
|-------+---------+------------------+--------+-------------+--------+---+--------|
| gsm8k |       3 | strict-match     |      5 | exact_match | 0.7642 | ± | 0.0117 |
|       |         | flexible-extract |      5 | exact_match | 0.7627 | ± | 0.0117 |



*** DONE NewTemperatureNewLoss___finally
CLOSED: [2024-06-12 Wed 09:43]

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |     25 | acc      | 0.6101 | ± | 0.0143 |
|               |         | none   |     25 | acc_norm | 0.6587 | ± | 0.0139 |


| Tasks     | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|-----------+---------+--------+--------+----------+--------+---+--------|
| hellaswag |       1 | none   |     10 | acc      | 0.6082 | ± | 0.0049 |
|           |         | none   |     10 | acc_norm | 0.8191 | ± | 0.0038 |

| Tasks            | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa       |     N/A | none   |      0 | rouge2_max  | 27.3829 | ± | 1.0120 |
|                  |         | none   |      0 | rougeL_diff |  1.8931 | ± | 0.9971 |
|                  |         | none   |      0 | rougeL_max  | 41.0841 | ± | 0.9036 |
|                  |         | none   |      0 | acc         |  0.4312 | ± | 0.0113 |
|                  |         | none   |      0 | rouge1_diff |  2.2425 | ± | 0.9897 |
|                  |         | none   |      0 | rouge2_acc  |  0.3476 | ± | 0.0167 |
|                  |         | none   |      0 | rougeL_acc  |  0.4859 | ± | 0.0175 |
|                  |         | none   |      0 | bleu_max    | 20.5577 | ± | 0.7412 |
|                  |         | none   |      0 | rouge1_max  | 43.9085 | ± | 0.8992 |
|                  |         | none   |      0 | rouge1_acc  |  0.5043 | ± | 0.0175 |
|                  |         | none   |      0 | bleu_acc    |  0.5092 | ± | 0.0175 |
|                  |         | none   |      0 | bleu_diff   |  1.3924 | ± | 0.6992 |
|                  |         | none   |      0 | rouge2_diff |  0.4032 | ± | 1.0551 |
| - truthfulqa_gen |       3 | none   |      0 | bleu_max    | 20.5577 | ± | 0.7412 |
|                  |         | none   |      0 | bleu_acc    |  0.5092 | ± | 0.0175 |
|                  |         | none   |      0 | bleu_diff   |  1.3924 | ± | 0.6992 |
|                  |         | none   |      0 | rouge1_max  | 43.9085 | ± | 0.8992 |
|                  |         | none   |      0 | rouge1_acc  |  0.5043 | ± | 0.0175 |
|                  |         | none   |      0 | rouge1_diff |  2.2425 | ± | 0.9897 |
|                  |         | none   |      0 | rouge2_max  | 27.3829 | ± | 1.0120 |
|                  |         | none   |      0 | rouge2_acc  |  0.3476 | ± | 0.0167 |
|                  |         | none   |      0 | rouge2_diff |  0.4032 | ± | 1.0551 |
|                  |         | none   |      0 | rougeL_max  | 41.0841 | ± | 0.9036 |
|                  |         | none   |      0 | rougeL_acc  |  0.4859 | ± | 0.0175 |
|                  |         | none   |      0 | rougeL_diff |  1.8931 | ± | 0.9971 |
| - truthfulqa_mc1 |       2 | none   |      0 | acc         |  0.3501 | ± | 0.0167 |
| - truthfulqa_mc2 |       2 | none   |      0 | acc         |  0.5123 | ± | 0.0151 |

| Groups     | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa | N/A     | none   |      0 | rouge2_max  | 27.3829 | ± | 1.0120 |
|            |         | none   |      0 | rougeL_diff |  1.8931 | ± | 0.9971 |
|            |         | none   |      0 | rougeL_max  | 41.0841 | ± | 0.9036 |
|            |         | none   |      0 | acc         |  0.4312 | ± | 0.0113 |
|            |         | none   |      0 | rouge1_diff |  2.2425 | ± | 0.9897 |
|            |         | none   |      0 | rouge2_acc  |  0.3476 | ± | 0.0167 |
|            |         | none   |      0 | rougeL_acc  |  0.4859 | ± | 0.0175 |
|            |         | none   |      0 | bleu_max    | 20.5577 | ± | 0.7412 |
|            |         | none   |      0 | rouge1_max  | 43.9085 | ± | 0.8992 |
|            |         | none   |      0 | rouge1_acc  |  0.5043 | ± | 0.0175 |
|            |         | none   |      0 | bleu_acc    |  0.5092 | ± | 0.0175 |
|            |         | none   |      0 | bleu_diff   |  1.3924 | ± | 0.6992 |
|            |         | none   |      0 | rouge2_diff |  0.4032 | ± | 1.0551 |

| Tasks                                 | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|---------------------------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu                                  |     N/A | none   |      0 | acc    | 0.6517 | ± | 0.0038 |
| - abstract_algebra                    |       0 | none   |      5 | acc    | 0.3300 | ± | 0.0473 |
| - anatomy                             |       0 | none   |      5 | acc    | 0.6444 | ± | 0.0414 |
| - astronomy                           |       0 | none   |      5 | acc    | 0.7434 | ± | 0.0355 |
| - business_ethics                     |       0 | none   |      5 | acc    | 0.6900 | ± | 0.0465 |
| - clinical_knowledge                  |       0 | none   |      5 | acc    | 0.7509 | ± | 0.0266 |
| - college_biology                     |       0 | none   |      5 | acc    | 0.8125 | ± | 0.0326 |
| - college_chemistry                   |       0 | none   |      5 | acc    | 0.4800 | ± | 0.0502 |
| - college_computer_science            |       0 | none   |      5 | acc    | 0.5300 | ± | 0.0502 |
| - college_mathematics                 |       0 | none   |      5 | acc    | 0.3700 | ± | 0.0485 |
| - college_medicine                    |       0 | none   |      5 | acc    | 0.6590 | ± | 0.0361 |
| - college_physics                     |       0 | none   |      5 | acc    | 0.4902 | ± | 0.0497 |
| - computer_security                   |       0 | none   |      5 | acc    | 0.7700 | ± | 0.0423 |
| - conceptual_physics                  |       0 | none   |      5 | acc    | 0.6128 | ± | 0.0318 |
| - econometrics                        |       0 | none   |      5 | acc    | 0.5526 | ± | 0.0468 |
| - electrical_engineering              |       0 | none   |      5 | acc    | 0.6414 | ± | 0.0400 |
| - elementary_mathematics              |       0 | none   |      5 | acc    | 0.4259 | ± | 0.0255 |
| - formal_logic                        |       0 | none   |      5 | acc    | 0.5159 | ± | 0.0447 |
| - global_facts                        |       0 | none   |      5 | acc    | 0.4000 | ± | 0.0492 |
| - high_school_biology                 |       0 | none   |      5 | acc    | 0.7871 | ± | 0.0233 |
| - high_school_chemistry               |       0 | none   |      5 | acc    | 0.5271 | ± | 0.0351 |
| - high_school_computer_science        |       0 | none   |      5 | acc    | 0.7000 | ± | 0.0461 |
| - high_school_european_history        |       0 | none   |      5 | acc    | 0.7636 | ± | 0.0332 |
| - high_school_geography               |       0 | none   |      5 | acc    | 0.8434 | ± | 0.0259 |
| - high_school_government_and_politics |       0 | none   |      5 | acc    | 0.9016 | ± | 0.0215 |
| - high_school_macroeconomics          |       0 | none   |      5 | acc    | 0.6590 | ± | 0.0240 |
| - high_school_mathematics             |       0 | none   |      5 | acc    | 0.3741 | ± | 0.0295 |
| - high_school_microeconomics          |       0 | none   |      5 | acc    | 0.7353 | ± | 0.0287 |
| - high_school_physics                 |       0 | none   |      5 | acc    | 0.3974 | ± | 0.0400 |
| - high_school_psychology              |       0 | none   |      5 | acc    | 0.8477 | ± | 0.0154 |
| - high_school_statistics              |       0 | none   |      5 | acc    | 0.5185 | ± | 0.0341 |
| - high_school_us_history              |       0 | none   |      5 | acc    | 0.8529 | ± | 0.0249 |
| - high_school_world_history           |       0 | none   |      5 | acc    | 0.8186 | ± | 0.0251 |
| - human_aging                         |       0 | none   |      5 | acc    | 0.7175 | ± | 0.0302 |
| - human_sexuality                     |       0 | none   |      5 | acc    | 0.7786 | ± | 0.0364 |
| - humanities                          |     N/A | none   |      5 | acc    | 0.5983 | ± | 0.0068 |
| - international_law                   |       0 | none   |      5 | acc    | 0.8017 | ± | 0.0364 |
| - jurisprudence                       |       0 | none   |      5 | acc    | 0.7685 | ± | 0.0408 |
| - logical_fallacies                   |       0 | none   |      5 | acc    | 0.7853 | ± | 0.0323 |
| - machine_learning                    |       0 | none   |      5 | acc    | 0.5357 | ± | 0.0473 |
| - management                          |       0 | none   |      5 | acc    | 0.7864 | ± | 0.0406 |
| - marketing                           |       0 | none   |      5 | acc    | 0.9060 | ± | 0.0191 |
| - medical_genetics                    |       0 | none   |      5 | acc    | 0.8000 | ± | 0.0402 |
| - miscellaneous                       |       0 | none   |      5 | acc    | 0.7829 | ± | 0.0147 |
| - moral_disputes                      |       0 | none   |      5 | acc    | 0.7312 | ± | 0.0239 |
| - moral_scenarios                     |       0 | none   |      5 | acc    | 0.4335 | ± | 0.0166 |
| - nutrition                           |       0 | none   |      5 | acc    | 0.7712 | ± | 0.0241 |
| - other                               |     N/A | none   |      5 | acc    | 0.7181 | ± | 0.0078 |
| - philosophy                          |       0 | none   |      5 | acc    | 0.7138 | ± | 0.0257 |
| - prehistory                          |       0 | none   |      5 | acc    | 0.7037 | ± | 0.0254 |
| - professional_accounting             |       0 | none   |      5 | acc    | 0.5106 | ± | 0.0298 |
| - professional_law                    |       0 | none   |      5 | acc    | 0.4746 | ± | 0.0128 |
| - professional_medicine               |       0 | none   |      5 | acc    | 0.7316 | ± | 0.0269 |
| - professional_psychology             |       0 | none   |      5 | acc    | 0.6912 | ± | 0.0187 |
| - public_relations                    |       0 | none   |      5 | acc    | 0.6818 | ± | 0.0446 |
| - security_studies                    |       0 | none   |      5 | acc    | 0.7469 | ± | 0.0278 |
| - social_sciences                     |     N/A | none   |      5 | acc    | 0.7598 | ± | 0.0075 |
| - sociology                           |       0 | none   |      5 | acc    | 0.8507 | ± | 0.0252 |
| - stem                                |     N/A | none   |      5 | acc    | 0.5604 | ± | 0.0085 |
| - us_foreign_policy                   |       0 | none   |      5 | acc    | 0.8600 | ± | 0.0349 |
| - virology                            |       0 | none   |      5 | acc    | 0.5060 | ± | 0.0389 |
| - world_religions                     |       0 | none   |      5 | acc    | 0.7544 | ± | 0.0330 |
                                                                                                                                                                                                                                    
| Groups            | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|-------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu              | N/A     | none   |      0 | acc    | 0.6517 | ± | 0.0038 |
| - humanities      | N/A     | none   |      5 | acc    | 0.5983 | ± | 0.0068 |
| - other           | N/A     | none   |      5 | acc    | 0.7181 | ± | 0.0078 |
| - social_sciences | N/A     | none   |      5 | acc    | 0.7598 | ± | 0.0075 |
| - stem            | N/A     | none   |      5 | acc    | 0.5604 | ± | 0.0085 |


| Tasks      | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|------------+---------+--------+--------+--------+--------+---+--------|
| winogrande |       1 | none   |      5 | acc    | 0.7727 | ± | 0.0118 |


| Tasks | Version | Filter           | n-shot | Metric      |  Value |   | Stderr |
|-------+---------+------------------+--------+-------------+--------+---+--------|
| gsm8k |       3 | strict-match     |      5 | exact_match | 0.7468 | ± |  0.012 |
|       |         | flexible-extract |      5 | exact_match | 0.7483 | ± |  0.012 |


*** TODO Vanilla [Epoch 1]

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |    25|acc     |0.6007|±  |0.0143|
|             |       |none  |    25|acc_norm|0.6459|±  |0.0140|

|  Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|
|---------|------:|------|-----:|--------|-----:|---|-----:|
|hellaswag|      1|none  |    10|acc     |0.5990|±  |0.0049|
|         |       |none  |    10|acc_norm|0.8102|±  |0.0039|

|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|
|-----|------:|----------------|-----:|-----------|-----:|---|-----:|
|gsm8k|      3|strict-match    |     5|exact_match|0.7271|±  |0.0123|
|     |       |flexible-extract|     5|exact_match|0.7286|±  |0.0122|

*** DONE NewTemperatureNewTauLoRD-VIIINewLoss___period500
CLOSED: [2024-06-11 Tue 09:49]
shot: 25, batch_size: auto (16)
|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |    25|acc     |0.5862|±  |0.0144|
|             |       |none  |    25|acc_norm|0.6425|±  |0.0140|

|  Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|
|---------|------:|------|-----:|--------|-----:|---|-----:|
|hellaswag|      1|none  |    10|acc     |0.5747|±  |0.0049|
|         |       |none  |    10|acc_norm|0.8254|±  |0.0038|

|      Tasks      |Version|Filter|n-shot|  Metric   | Value |   |Stderr|
|-----------------|-------|------|-----:|-----------|------:|---|-----:|
|truthfulqa       |N/A    |none  |     0|rouge1_acc | 0.4541|±  |0.0174|
|                 |       |none  |     0|rouge1_max |38.7072|±  |0.8778|
|                 |       |none  |     0|rouge1_diff|-1.3491|±  |0.7542|
|                 |       |none  |     0|bleu_diff  |-0.9982|±  |0.5289|
|                 |       |none  |     0|bleu_acc   | 0.4590|±  |0.0174|
|                 |       |none  |     0|rouge2_max |22.8817|±  |0.8979|
|                 |       |none  |     0|rouge2_acc | 0.3195|±  |0.0163|
|                 |       |none  |     0|rougeL_diff|-1.7465|±  |0.7481|
|                 |       |none  |     0|rougeL_max |35.8348|±  |0.8648|
|                 |       |none  |     0|rouge2_diff|-2.9596|±  |0.7834|
|                 |       |none  |     0|bleu_max   |17.2006|±  |0.6632|
|                 |       |none  |     0|acc        | 0.4393|±  |0.0113|
|                 |       |none  |     0|rougeL_acc | 0.4480|±  |0.0174|
| - truthfulqa_gen|      3|none  |     0|bleu_max   |17.2006|±  |0.6632|
|                 |       |none  |     0|bleu_acc   | 0.4590|±  |0.0174|
|                 |       |none  |     0|bleu_diff  |-0.9982|±  |0.5289|
|                 |       |none  |     0|rouge1_max |38.7072|±  |0.8778|
|                 |       |none  |     0|rouge1_acc | 0.4541|±  |0.0174|
|                 |       |none  |     0|rouge1_diff|-1.3491|±  |0.7542|
|                 |       |none  |     0|rouge2_max |22.8817|±  |0.8979|
|                 |       |none  |     0|rouge2_acc | 0.3195|±  |0.0163|
|                 |       |none  |     0|rouge2_diff|-2.9596|±  |0.7834|
|                 |       |none  |     0|rougeL_max |35.8348|±  |0.8648|
|                 |       |none  |     0|rougeL_acc | 0.4480|±  |0.0174|
|                 |       |none  |     0|rougeL_diff|-1.7465|±  |0.7481|
| - truthfulqa_mc1|      2|none  |     0|acc        | 0.3537|±  |0.0167|
| - truthfulqa_mc2|      2|none  |     0|acc        | 0.5248|±  |0.0151|

|  Groups  |Version|Filter|n-shot|  Metric   | Value |   |Stderr|
|----------|-------|------|-----:|-----------|------:|---|-----:|
|truthfulqa|N/A    |none  |     0|rouge1_acc | 0.4541|±  |0.0174|
|          |       |none  |     0|rouge1_max |38.7072|±  |0.8778|
|          |       |none  |     0|rouge1_diff|-1.3491|±  |0.7542|
|          |       |none  |     0|bleu_diff  |-0.9982|±  |0.5289|
|          |       |none  |     0|bleu_acc   | 0.4590|±  |0.0174|
|          |       |none  |     0|rouge2_max |22.8817|±  |0.8979|
|          |       |none  |     0|rouge2_acc | 0.3195|±  |0.0163|
|          |       |none  |     0|rougeL_diff|-1.7465|±  |0.7481|
|          |       |none  |     0|rougeL_max |35.8348|±  |0.8648|
|          |       |none  |     0|rouge2_diff|-2.9596|±  |0.7834|
|          |       |none  |     0|bleu_max   |17.2006|±  |0.6632|
|          |       |none  |     0|acc        | 0.4393|±  |0.0113|
|          |       |none  |     0|rougeL_acc | 0.4480|±  |0.0174|

|                 Tasks                 |Version|Filter|n-shot|Metric|Value |   |Stderr|
|---------------------------------------|-------|------|-----:|------|-----:|---|-----:|
|mmlu                                   |N/A    |none  |     0|acc   |0.6493|±  |0.0038|
|  - abstract_algebra                   |      0|none  |     5|acc   |0.3400|±  |0.0476|
|  - anatomy                            |      0|none  |     5|acc   |0.6148|±  |0.0420|
|  - astronomy                          |      0|none  |     5|acc   |0.7171|±  |0.0367|
|  - business_ethics                    |      0|none  |     5|acc   |0.7000|±  |0.0461|
|  - clinical_knowledge                 |      0|none  |     5|acc   |0.7509|±  |0.0266|
|  - college_biology                    |      0|none  |     5|acc   |0.7986|±  |0.0335|
|  - college_chemistry                  |      0|none  |     5|acc   |0.5100|±  |0.0502|
|  - college_computer_science           |      0|none  |     5|acc   |0.5500|±  |0.0500|
|  - college_mathematics                |      0|none  |     5|acc   |0.4000|±  |0.0492|
|  - college_medicine                   |      0|none  |     5|acc   |0.6532|±  |0.0363|
|  - college_physics                    |      0|none  |     5|acc   |0.5196|±  |0.0497|
|  - computer_security                  |      0|none  |     5|acc   |0.7800|±  |0.0416|
|  - conceptual_physics                 |      0|none  |     5|acc   |0.6213|±  |0.0317|
|  - econometrics                       |      0|none  |     5|acc   |0.5702|±  |0.0466|
|  - electrical_engineering             |      0|none  |     5|acc   |0.6276|±  |0.0403|
|  - elementary_mathematics             |      0|none  |     5|acc   |0.4233|±  |0.0254|
|  - formal_logic                       |      0|none  |     5|acc   |0.5079|±  |0.0447|
|  - global_facts                       |      0|none  |     5|acc   |0.4300|±  |0.0498|
|  - high_school_biology                |      0|none  |     5|acc   |0.7839|±  |0.0234|
|  - high_school_chemistry              |      0|none  |     5|acc   |0.5419|±  |0.0351|
|  - high_school_computer_science       |      0|none  |     5|acc   |0.7100|±  |0.0456|
|  - high_school_european_history       |      0|none  |     5|acc   |0.7697|±  |0.0329|
|  - high_school_geography              |      0|none  |     5|acc   |0.8182|±  |0.0275|
|  - high_school_government_and_politics|      0|none  |     5|acc   |0.8964|±  |0.0220|
|  - high_school_macroeconomics         |      0|none  |     5|acc   |0.6590|±  |0.0240|
|  - high_school_mathematics            |      0|none  |     5|acc   |0.3704|±  |0.0294|
|  - high_school_microeconomics         |      0|none  |     5|acc   |0.7479|±  |0.0282|
|  - high_school_physics                |      0|none  |     5|acc   |0.4172|±  |0.0403|
|  - high_school_psychology             |      0|none  |     5|acc   |0.8385|±  |0.0158|
|  - high_school_statistics             |      0|none  |     5|acc   |0.5324|±  |0.0340|
|  - high_school_us_history             |      0|none  |     5|acc   |0.8529|±  |0.0249|
|  - high_school_world_history          |      0|none  |     5|acc   |0.8228|±  |0.0249|
|  - human_aging                        |      0|none  |     5|acc   |0.7130|±  |0.0304|
|  - human_sexuality                    |      0|none  |     5|acc   |0.8015|±  |0.0350|
| - humanities                          |N/A    |none  |     5|acc   |0.5960|±  |0.0068|
|  - international_law                  |      0|none  |     5|acc   |0.7934|±  |0.0370|
|  - jurisprudence                      |      0|none  |     5|acc   |0.7778|±  |0.0402|
|  - logical_fallacies                  |      0|none  |     5|acc   |0.7975|±  |0.0316|
|  - machine_learning                   |      0|none  |     5|acc   |0.5357|±  |0.0473|
|  - management                         |      0|none  |     5|acc   |0.7961|±  |0.0399|
|  - marketing                          |      0|none  |     5|acc   |0.8974|±  |0.0199|
|  - medical_genetics                   |      0|none  |     5|acc   |0.7900|±  |0.0409|
|  - miscellaneous                      |      0|none  |     5|acc   |0.7752|±  |0.0149|
|  - moral_disputes                     |      0|none  |     5|acc   |0.7341|±  |0.0238|
|  - moral_scenarios                    |      0|none  |     5|acc   |0.4168|±  |0.0165|
|  - nutrition                          |      0|none  |     5|acc   |0.7484|±  |0.0248|
| - other                               |N/A    |none  |     5|acc   |0.7100|±  |0.0079|
|  - philosophy                         |      0|none  |     5|acc   |0.6945|±  |0.0262|
|  - prehistory                         |      0|none  |     5|acc   |0.7099|±  |0.0253|
|  - professional_accounting            |      0|none  |     5|acc   |0.5071|±  |0.0298|
|  - professional_law                   |      0|none  |     5|acc   |0.4759|±  |0.0128|
|  - professional_medicine              |      0|none  |     5|acc   |0.7132|±  |0.0275|
|  - professional_psychology            |      0|none  |     5|acc   |0.6879|±  |0.0187|
|  - public_relations                   |      0|none  |     5|acc   |0.6727|±  |0.0449|
|  - security_studies                   |      0|none  |     5|acc   |0.7265|±  |0.0285|
| - social_sciences                     |N/A    |none  |     5|acc   |0.7576|±  |0.0076|
|  - sociology                          |      0|none  |     5|acc   |0.8657|±  |0.0241|
| - stem                                |N/A    |none  |     5|acc   |0.5636|±  |0.0085|
|  - us_foreign_policy                  |      0|none  |     5|acc   |0.8700|±  |0.0338|
|  - virology                           |      0|none  |     5|acc   |0.4699|±  |0.0389|
|  - world_religions                    |      0|none  |     5|acc   |0.7661|±  |0.0325|

|      Groups      |Version|Filter|n-shot|Metric|Value |   |Stderr|
|------------------|-------|------|-----:|------|-----:|---|-----:|
|mmlu              |N/A    |none  |     0|acc   |0.6493|±  |0.0038|
| - humanities     |N/A    |none  |     5|acc   |0.5960|±  |0.0068|
| - other          |N/A    |none  |     5|acc   |0.7100|±  |0.0079|
| - social_sciences|N/A    |none  |     5|acc   |0.7576|±  |0.0076|
| - stem           |N/A    |none  |     5|acc   |0.5636|±  |0.0085|


| Tasks      | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|------------+---------+--------+--------+--------+--------+---+--------|
| winogrande |       1 | none   |      5 | acc    | 0.7766 | ± | 0.0117 |

| Tasks | Version | Filter           | n-shot | Metric      |  Value |   | Stderr |
|-------+---------+------------------+--------+-------------+--------+---+--------|
| gsm8k |       3 | strict-match     |      5 | exact_match | 0.7111 | ± | 0.0125 |
|       |         | flexible-extract |      5 | exact_match | 0.7195 | ± | 0.0124 |


*** CANCELED NewTemperatureNewLoss___period500
CLOSED: [2024-06-11 Tue 09:49]

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |     25 | acc      | 0.5725 | ± | 0.0145 |
|               |         | none   |     25 | acc_norm | 0.6246 | ± | 0.0142 |

| Tasks     | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|-----------+---------+--------+--------+----------+--------+---+--------|
| hellaswag |       1 | none   |     10 | acc      | 0.5782 | ± | 0.0049 |
|           |         | none   |     10 | acc_norm | 0.7946 | ± | 0.0040 |

| Tasks            | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa       |     N/A | none   |      0 | rougeL_max  | 44.7747 | ± | 0.9330 |
|                  |         | none   |      0 | acc         |  0.4307 | ± | 0.0111 |
|                  |         | none   |      0 | rouge1_diff |  3.4765 | ± | 1.0987 |
|                  |         | none   |      0 | rouge2_max  | 30.5415 | ± | 1.0850 |
|                  |         | none   |      0 | rouge2_acc  |  0.3562 | ± | 0.0168 |
|                  |         | none   |      0 | rougeL_acc  |  0.5031 | ± | 0.0175 |
|                  |         | none   |      0 | bleu_max    | 23.8401 | ± | 0.7920 |
|                  |         | none   |      0 | rouge1_acc  |  0.5129 | ± | 0.0175 |
|                  |         | none   |      0 | bleu_diff   |  2.0199 | ± | 0.7919 |
|                  |         | none   |      0 | rouge2_diff |  1.3540 | ± | 1.1675 |
|                  |         | none   |      0 | rougeL_diff |  3.0595 | ± | 1.1036 |
|                  |         | none   |      0 | rouge1_max  | 47.4034 | ± | 0.9230 |
|                  |         | none   |      0 | bleu_acc    |  0.5141 | ± | 0.0175 |
| - truthfulqa_gen |       3 | none   |      0 | bleu_max    | 23.8401 | ± | 0.7920 |
|                  |         | none   |      0 | bleu_acc    |  0.5141 | ± | 0.0175 |
|                  |         | none   |      0 | bleu_diff   |  2.0199 | ± | 0.7919 |
|                  |         | none   |      0 | rouge1_max  | 47.4034 | ± | 0.9230 |
|                  |         | none   |      0 | rouge1_acc  |  0.5129 | ± | 0.0175 |
|                  |         | none   |      0 | rouge1_diff |  3.4765 | ± | 1.0987 |
|                  |         | none   |      0 | rouge2_max  | 30.5415 | ± | 1.0850 |
|                  |         | none   |      0 | rouge2_acc  |  0.3562 | ± | 0.0168 |
|                  |         | none   |      0 | rouge2_diff |  1.3540 | ± | 1.1675 |
|                  |         | none   |      0 | rougeL_max  | 44.7747 | ± | 0.9330 |
|                  |         | none   |      0 | rougeL_acc  |  0.5031 | ± | 0.0175 |
|                  |         | none   |      0 | rougeL_diff |  3.0595 | ± | 1.1036 |
| - truthfulqa_mc1 |       2 | none   |      0 | acc         |  0.3427 | ± | 0.0166 |
| - truthfulqa_mc2 |       2 | none   |      0 | acc         |  0.5186 | ± | 0.0147 |

| Groups     | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa | N/A     | none   |      0 | rougeL_max  | 44.7747 | ± | 0.9330 |
|            |         | none   |      0 | acc         |  0.4307 | ± | 0.0111 |
|            |         | none   |      0 | rouge1_diff |  3.4765 | ± | 1.0987 |
|            |         | none   |      0 | rouge2_max  | 30.5415 | ± | 1.0850 |
|            |         | none   |      0 | rouge2_acc  |  0.3562 | ± | 0.0168 |
|            |         | none   |      0 | rougeL_acc  |  0.5031 | ± | 0.0175 |
|            |         | none   |      0 | bleu_max    | 23.8401 | ± | 0.7920 |
|            |         | none   |      0 | rouge1_acc  |  0.5129 | ± | 0.0175 |
|            |         | none   |      0 | bleu_diff   |  2.0199 | ± | 0.7919 |
|            |         | none   |      0 | rouge2_diff |  1.3540 | ± | 1.1675 |
|            |         | none   |      0 | rougeL_diff |  3.0595 | ± | 1.1036 |
|            |         | none   |      0 | rouge1_max  | 47.4034 | ± | 0.9230 |
|            |         | none   |      0 | bleu_acc    |  0.5141 | ± | 0.0175 |

| Tasks                                 | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|---------------------------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu                                  |     N/A | none   |      0 | acc    | 0.6583 | ± | 0.0038 |
| - abstract_algebra                    |       0 | none   |      5 | acc    | 0.2700 | ± | 0.0446 |
| - anatomy                             |       0 | none   |      5 | acc    | 0.6444 | ± | 0.0414 |
| - astronomy                           |       0 | none   |      5 | acc    | 0.7368 | ± | 0.0358 |
| - business_ethics                     |       0 | none   |      5 | acc    | 0.7000 | ± | 0.0461 |
| - clinical_knowledge                  |       0 | none   |      5 | acc    | 0.7509 | ± | 0.0266 |
| - college_biology                     |       0 | none   |      5 | acc    | 0.7986 | ± | 0.0335 |
| - college_chemistry                   |       0 | none   |      5 | acc    | 0.4800 | ± | 0.0502 |
| - college_computer_science            |       0 | none   |      5 | acc    | 0.5700 | ± | 0.0498 |
| - college_mathematics                 |       0 | none   |      5 | acc    | 0.3700 | ± | 0.0485 |
| - college_medicine                    |       0 | none   |      5 | acc    | 0.6416 | ± | 0.0366 |
| - college_physics                     |       0 | none   |      5 | acc    | 0.4902 | ± | 0.0497 |
| - computer_security                   |       0 | none   |      5 | acc    | 0.7600 | ± | 0.0429 |
| - conceptual_physics                  |       0 | none   |      5 | acc    | 0.6128 | ± | 0.0318 |
| - econometrics                        |       0 | none   |      5 | acc    | 0.5877 | ± | 0.0463 |
| - electrical_engineering              |       0 | none   |      5 | acc    | 0.6345 | ± | 0.0401 |
| - elementary_mathematics              |       0 | none   |      5 | acc    | 0.4524 | ± | 0.0256 |
| - formal_logic                        |       0 | none   |      5 | acc    | 0.5079 | ± | 0.0447 |
| - global_facts                        |       0 | none   |      5 | acc    | 0.4100 | ± | 0.0494 |
| - high_school_biology                 |       0 | none   |      5 | acc    | 0.7839 | ± | 0.0234 |
| - high_school_chemistry               |       0 | none   |      5 | acc    | 0.5271 | ± | 0.0351 |
| - high_school_computer_science        |       0 | none   |      5 | acc    | 0.7700 | ± | 0.0423 |
| - high_school_european_history        |       0 | none   |      5 | acc    | 0.7394 | ± | 0.0343 |
| - high_school_geography               |       0 | none   |      5 | acc    | 0.8384 | ± | 0.0262 |
| - high_school_government_and_politics |       0 | none   |      5 | acc    | 0.9067 | ± | 0.0210 |
| - high_school_macroeconomics          |       0 | none   |      5 | acc    | 0.6513 | ± | 0.0242 |
| - high_school_mathematics             |       0 | none   |      5 | acc    | 0.3815 | ± | 0.0296 |
| - high_school_microeconomics          |       0 | none   |      5 | acc    | 0.7689 | ± | 0.0274 |
| - high_school_physics                 |       0 | none   |      5 | acc    | 0.4305 | ± | 0.0404 |
| - high_school_psychology              |       0 | none   |      5 | acc    | 0.8495 | ± | 0.0153 |
| - high_school_statistics              |       0 | none   |      5 | acc    | 0.5602 | ± | 0.0339 |
| - high_school_us_history              |       0 | none   |      5 | acc    | 0.8529 | ± | 0.0249 |
| - high_school_world_history           |       0 | none   |      5 | acc    | 0.8481 | ± | 0.0234 |
| - human_aging                         |       0 | none   |      5 | acc    | 0.7309 | ± | 0.0298 |
| - human_sexuality                     |       0 | none   |      5 | acc    | 0.7863 | ± | 0.0360 |
| - humanities                          |     N/A | none   |      5 | acc    | 0.6055 | ± | 0.0068 |
| - international_law                   |       0 | none   |      5 | acc    | 0.8099 | ± | 0.0358 |
| - jurisprudence                       |       0 | none   |      5 | acc    | 0.7778 | ± | 0.0402 |
| - logical_fallacies                   |       0 | none   |      5 | acc    | 0.7853 | ± | 0.0323 |
| - machine_learning                    |       0 | none   |      5 | acc    | 0.5268 | ± | 0.0474 |
| - management                          |       0 | none   |      5 | acc    | 0.7864 | ± | 0.0406 |
| - marketing                           |       0 | none   |      5 | acc    | 0.9188 | ± | 0.0179 |
| - medical_genetics                    |       0 | none   |      5 | acc    | 0.8000 | ± | 0.0402 |
| - miscellaneous                       |       0 | none   |      5 | acc    | 0.7931 | ± | 0.0145 |
| - moral_disputes                      |       0 | none   |      5 | acc    | 0.7457 | ± | 0.0234 |
| - moral_scenarios                     |       0 | none   |      5 | acc    | 0.4380 | ± | 0.0166 |
| - nutrition                           |       0 | none   |      5 | acc    | 0.7647 | ± | 0.0243 |
| - other                               |     N/A | none   |      5 | acc    | 0.7216 | ± | 0.0078 |
| - philosophy                          |       0 | none   |      5 | acc    | 0.7267 | ± | 0.0253 |
| - prehistory                          |       0 | none   |      5 | acc    | 0.7222 | ± | 0.0249 |
| - professional_accounting             |       0 | none   |      5 | acc    | 0.5284 | ± | 0.0298 |
| - professional_law                    |       0 | none   |      5 | acc    | 0.4798 | ± | 0.0128 |
| - professional_medicine               |       0 | none   |      5 | acc    | 0.7169 | ± | 0.0274 |
| - professional_psychology             |       0 | none   |      5 | acc    | 0.7092 | ± | 0.0184 |
| - public_relations                    |       0 | none   |      5 | acc    | 0.6545 | ± | 0.0455 |
| - security_studies                    |       0 | none   |      5 | acc    | 0.7469 | ± | 0.0278 |
| - social_sciences                     |     N/A | none   |      5 | acc    | 0.7676 | ± | 0.0075 |
| - sociology                           |       0 | none   |      5 | acc    | 0.8806 | ± | 0.0229 |
| - stem                                |     N/A | none   |      5 | acc    | 0.5680 | ± | 0.0084 |
| - us_foreign_policy                   |       0 | none   |      5 | acc    | 0.8500 | ± | 0.0359 |
| - virology                            |       0 | none   |      5 | acc    | 0.5000 | ± | 0.0389 |
| - world_religions                     |       0 | none   |      5 | acc    | 0.7719 | ± | 0.0322 |

| Groups            | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|-------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu              | N/A     | none   |      0 | acc    | 0.6583 | ± | 0.0038 |
| - humanities      | N/A     | none   |      5 | acc    | 0.6055 | ± | 0.0068 |
| - other           | N/A     | none   |      5 | acc    | 0.7216 | ± | 0.0078 |
| - social_sciences | N/A     | none   |      5 | acc    | 0.7676 | ± | 0.0075 |
| - stem            | N/A     | none   |      5 | acc    | 0.5680 | ± | 0.0084 |

| Tasks      | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|------------+---------+--------+--------+--------+--------+---+--------|
| winogrande |       1 | none   |      5 | acc    | 0.7632 | ± | 0.0119 |

|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|
|-----|------:|----------------|-----:|-----------|-----:|---|-----:|
|gsm8k|      3|strict-match    |     5|exact_match|0.7293|±  |0.0122|
|     |       |flexible-extract|     5|exact_match|0.7422|±  |0.0120|
*** CANCELED LoRD-IX
CLOSED: [2024-06-24 Mon 08:34]
*** DONE LoRD-VI
CLOSED: [2024-06-25 Tue 14:33]

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |     25 | acc      | 0.5862 | ± | 0.0144 |
|               |         | none   |     25 | acc_norm | 0.6399 | ± | 0.0140 |


| Tasks     | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|-----------+---------+--------+--------+----------+--------+---+--------|
| hellaswag |       1 | none   |     10 | acc      | 0.5348 | ± | 0.0050 |
|           |         | none   |     10 | acc_norm | 0.8127 | ± | 0.0039 |


| Tasks | Version | Filter           | n-shot | Metric      |  Value |   | Stderr |
|-------+---------+------------------+--------+-------------+--------+---+--------|
| gsm8k |       3 | strict-match     |      5 | exact_match | 0.7202 | ± | 0.0124 |
|       |         | flexible-extract |      5 | exact_match | 0.7263 | ± | 0.0123 |

*** DONE LoRD-VII
CLOSED: [2024-06-25 Tue 14:33]

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |     25 | acc      | 0.5947 | ± | 0.0143 |
|               |         | none   |     25 | acc_norm | 0.6502 | ± | 0.0139 |

|  Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|
|---------|------:|------|-----:|--------|-----:|---|-----:|
|hellaswag|      1|none  |    10|acc     |0.5515|±  |0.0050|
|         |       |none  |    10|acc_norm|0.8224|±  |0.0038|

| Tasks | Version | Filter           | n-shot | Metric      |  Value |   | Stderr |
|-------+---------+------------------+--------+-------------+--------+---+--------|
| gsm8k |       3 | strict-match     |      5 | exact_match | 0.7180 | ± | 0.0124 |
|       |         | flexible-extract |      5 | exact_match | 0.7301 | ± | 0.0122 |


** SHORT-TEXT [Llama2-13B]
*** DONE Pretrained
CLOSED: [2024-06-13 Thu 07:32]

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |     25 | acc      | 0.5555 | ± | 0.0145 |
|               |         | none   |     25 | acc_norm | 0.6032 | ± | 0.0143 |

| Tasks     | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|-----------+---------+--------+--------+----------+--------+---+--------|
| hellaswag |       1 | none   |     10 | acc      | 0.6292 | ± | 0.0048 |
|           |         | none   |     10 | acc_norm | 0.8213 | ± | 0.0038 |


| Tasks            | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa       |     N/A | none   |      0 | rougeL_max  | 49.1747 | ± | 0.8444 |
|                  |         | none   |      0 | rougeL_diff | -3.1108 | ± | 0.8648 |
|                  |         | none   |      0 | bleu_max    | 26.1243 | ± | 0.7800 |
|                  |         | none   |      0 | rouge2_acc  |  0.3525 | ± | 0.0167 |
|                  |         | none   |      0 | rouge1_max  | 52.0315 | ± | 0.8207 |
|                  |         | none   |      0 | rouge1_acc  |  0.4259 | ± | 0.0173 |
|                  |         | none   |      0 | bleu_acc    |  0.4113 | ± | 0.0172 |
|                  |         | none   |      0 | rouge2_diff | -4.0291 | ± | 0.9909 |
|                  |         | none   |      0 | rouge2_max  | 36.5127 | ± | 0.9664 |
|                  |         | none   |      0 | rouge1_diff | -3.0754 | ± | 0.8534 |
|                  |         | none   |      0 | rougeL_acc  |  0.4211 | ± | 0.0173 |
|                  |         | none   |      0 | acc         |  0.3599 | ± | 0.0111 |
|                  |         | none   |      0 | bleu_diff   | -1.9615 | ± | 0.7198 |
| - truthfulqa_gen |       3 | none   |      0 | bleu_max    | 26.1243 | ± | 0.7800 |
|                  |         | none   |      0 | bleu_acc    |  0.4113 | ± | 0.0172 |
|                  |         | none   |      0 | bleu_diff   | -1.9615 | ± | 0.7198 |
|                  |         | none   |      0 | rouge1_max  | 52.0315 | ± | 0.8207 |
|                  |         | none   |      0 | rouge1_acc  |  0.4259 | ± | 0.0173 |
|                  |         | none   |      0 | rouge1_diff | -3.0754 | ± | 0.8534 |
|                  |         | none   |      0 | rouge2_max  | 36.5127 | ± | 0.9664 |
|                  |         | none   |      0 | rouge2_acc  |  0.3525 | ± | 0.0167 |
|                  |         | none   |      0 | rouge2_diff | -4.0291 | ± | 0.9909 |
|                  |         | none   |      0 | rougeL_max  | 49.1747 | ± | 0.8444 |
|                  |         | none   |      0 | rougeL_acc  |  0.4211 | ± | 0.0173 |
|                  |         | none   |      0 | rougeL_diff | -3.1108 | ± | 0.8648 |
| - truthfulqa_mc1 |       2 | none   |      0 | acc         |  0.2803 | ± | 0.0157 |
| - truthfulqa_mc2 |       2 | none   |      0 | acc         |  0.4396 | ± | 0.0157 |
                                                                                                                                                                                                                                    
| Groups     | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa | N/A     | none   |      0 | rougeL_max  | 49.1747 | ± | 0.8444 |
|            |         | none   |      0 | rougeL_diff | -3.1108 | ± | 0.8648 |
|            |         | none   |      0 | bleu_max    | 26.1243 | ± | 0.7800 |
|            |         | none   |      0 | rouge2_acc  |  0.3525 | ± | 0.0167 |
|            |         | none   |      0 | rouge1_max  | 52.0315 | ± | 0.8207 |
|            |         | none   |      0 | rouge1_acc  |  0.4259 | ± | 0.0173 |
|            |         | none   |      0 | bleu_acc    |  0.4113 | ± | 0.0172 |
|            |         | none   |      0 | rouge2_diff | -4.0291 | ± | 0.9909 |
|            |         | none   |      0 | rouge2_max  | 36.5127 | ± | 0.9664 |
|            |         | none   |      0 | rouge1_diff | -3.0754 | ± | 0.8534 |
|            |         | none   |      0 | rougeL_acc  |  0.4211 | ± | 0.0173 |
|            |         | none   |      0 | acc         |  0.3599 | ± | 0.0111 |
|            |         | none   |      0 | bleu_diff   | -1.9615 | ± | 0.7198 |

| Tasks                                 | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|---------------------------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu                                  |     N/A | none   |      0 | acc    | 0.5355 | ± | 0.0040 |
| - abstract_algebra                    |       0 | none   |      5 | acc    | 0.3100 | ± | 0.0465 |
| - anatomy                             |       0 | none   |      5 | acc    | 0.4741 | ± | 0.0431 |
| - astronomy                           |       0 | none   |      5 | acc    | 0.5724 | ± | 0.0403 |
| - business_ethics                     |       0 | none   |      5 | acc    | 0.5300 | ± | 0.0502 |
| - clinical_knowledge                  |       0 | none   |      5 | acc    | 0.5811 | ± | 0.0304 |
| - college_biology                     |       0 | none   |      5 | acc    | 0.5903 | ± | 0.0411 |
| - college_chemistry                   |       0 | none   |      5 | acc    | 0.3900 | ± | 0.0490 |
| - college_computer_science            |       0 | none   |      5 | acc    | 0.4600 | ± | 0.0501 |
| - college_mathematics                 |       0 | none   |      5 | acc    | 0.2600 | ± | 0.0441 |
| - college_medicine                    |       0 | none   |      5 | acc    | 0.4624 | ± | 0.0380 |
| - college_physics                     |       0 | none   |      5 | acc    | 0.3137 | ± | 0.0462 |
| - computer_security                   |       0 | none   |      5 | acc    | 0.6900 | ± | 0.0465 |
| - conceptual_physics                  |       0 | none   |      5 | acc    | 0.3872 | ± | 0.0318 |
| - econometrics                        |       0 | none   |      5 | acc    | 0.3158 | ± | 0.0437 |
| - electrical_engineering              |       0 | none   |      5 | acc    | 0.5034 | ± | 0.0417 |
| - elementary_mathematics              |       0 | none   |      5 | acc    | 0.3466 | ± | 0.0245 |
| - formal_logic                        |       0 | none   |      5 | acc    | 0.3016 | ± | 0.0410 |
| - global_facts                        |       0 | none   |      5 | acc    | 0.3300 | ± | 0.0473 |
| - high_school_biology                 |       0 | none   |      5 | acc    | 0.6516 | ± | 0.0271 |
| - high_school_chemistry               |       0 | none   |      5 | acc    | 0.4483 | ± | 0.0350 |
| - high_school_computer_science        |       0 | none   |      5 | acc    | 0.5900 | ± | 0.0494 |
| - high_school_european_history        |       0 | none   |      5 | acc    | 0.6848 | ± | 0.0363 |
| - high_school_geography               |       0 | none   |      5 | acc    | 0.7020 | ± | 0.0326 |
| - high_school_government_and_politics |       0 | none   |      5 | acc    | 0.7876 | ± | 0.0295 |
| - high_school_macroeconomics          |       0 | none   |      5 | acc    | 0.4872 | ± | 0.0253 |
| - high_school_mathematics             |       0 | none   |      5 | acc    | 0.3037 | ± | 0.0280 |
| - high_school_microeconomics          |       0 | none   |      5 | acc    | 0.5252 | ± | 0.0324 |
| - high_school_physics                 |       0 | none   |      5 | acc    | 0.3245 | ± | 0.0382 |
| - high_school_psychology              |       0 | none   |      5 | acc    | 0.7413 | ± | 0.0188 |
| - high_school_statistics              |       0 | none   |      5 | acc    | 0.3889 | ± | 0.0332 |
| - high_school_us_history              |       0 | none   |      5 | acc    | 0.7549 | ± | 0.0302 |
| - high_school_world_history           |       0 | none   |      5 | acc    | 0.7173 | ± | 0.0293 |
| - human_aging                         |       0 | none   |      5 | acc    | 0.6457 | ± | 0.0321 |
| - human_sexuality                     |       0 | none   |      5 | acc    | 0.6183 | ± | 0.0426 |
| - humanities                          |     N/A | none   |      5 | acc    | 0.4995 | ± | 0.0069 |
| - international_law                   |       0 | none   |      5 | acc    | 0.7686 | ± | 0.0385 |
| - jurisprudence                       |       0 | none   |      5 | acc    | 0.7130 | ± | 0.0437 |
| - logical_fallacies                   |       0 | none   |      5 | acc    | 0.6564 | ± | 0.0373 |
| - machine_learning                    |       0 | none   |      5 | acc    | 0.3571 | ± | 0.0455 |
| - management                          |       0 | none   |      5 | acc    | 0.7379 | ± | 0.0435 |
| - marketing                           |       0 | none   |      5 | acc    | 0.7821 | ± | 0.0270 |
| - medical_genetics                    |       0 | none   |      5 | acc    | 0.5600 | ± | 0.0499 |
| - miscellaneous                       |       0 | none   |      5 | acc    | 0.7497 | ± | 0.0155 |
| - moral_disputes                      |       0 | none   |      5 | acc    | 0.5954 | ± | 0.0264 |
| - moral_scenarios                     |       0 | none   |      5 | acc    | 0.3140 | ± | 0.0155 |
| - nutrition                           |       0 | none   |      5 | acc    | 0.6111 | ± | 0.0279 |
| - other                               |     N/A | none   |      5 | acc    | 0.6041 | ± | 0.0084 |
| - philosophy                          |       0 | none   |      5 | acc    | 0.6013 | ± | 0.0278 |
| - prehistory                          |       0 | none   |      5 | acc    | 0.6019 | ± | 0.0272 |
| - professional_accounting             |       0 | none   |      5 | acc    | 0.3865 | ± | 0.0290 |
| - professional_law                    |       0 | none   |      5 | acc    | 0.3911 | ± | 0.0125 |
| - professional_medicine               |       0 | none   |      5 | acc    | 0.4926 | ± | 0.0304 |
| - professional_psychology             |       0 | none   |      5 | acc    | 0.5343 | ± | 0.0202 |
| - public_relations                    |       0 | none   |      5 | acc    | 0.6545 | ± | 0.0455 |
| - security_studies                    |       0 | none   |      5 | acc    | 0.6327 | ± | 0.0309 |
| - social_sciences                     |     N/A | none   |      5 | acc    | 0.6214 | ± | 0.0085 |
| - sociology                           |       0 | none   |      5 | acc    | 0.7463 | ± | 0.0308 |
| - stem                                |     N/A | none   |      5 | acc    | 0.4380 | ± | 0.0086 |
| - us_foreign_policy                   |       0 | none   |      5 | acc    | 0.8100 | ± | 0.0394 |
| - virology                            |       0 | none   |      5 | acc    | 0.4880 | ± | 0.0389 |
| - world_religions                     |       0 | none   |      5 | acc    | 0.7544 | ± | 0.0330 |
                                                                                                                                                                                                                                    

| Groups            | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|-------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu              | N/A     | none   |      0 | acc    | 0.5355 | ± | 0.0040 |
| - humanities      | N/A     | none   |      5 | acc    | 0.4995 | ± | 0.0069 |
| - other           | N/A     | none   |      5 | acc    | 0.6041 | ± | 0.0084 |
| - social_sciences | N/A     | none   |      5 | acc    | 0.6214 | ± | 0.0085 |
| - stem            | N/A     | none   |      5 | acc    | 0.4380 | ± | 0.0086 |


| Tasks      | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|------------+---------+--------+--------+--------+--------+---+--------|
| winogrande |       1 | none   |      5 | acc    | 0.7435 | ± | 0.0123 |


| Tasks | Version | Filter           | n-shot | Metric      |  Value |   | Stderr |
|-------+---------+------------------+--------+-------------+--------+---+--------|
| gsm8k |       3 | strict-match     |      5 | exact_match | 0.3465 | ± | 0.0131 |
|       |         | flexible-extract |      5 | exact_match | 0.3548 | ± | 0.0132 |
*** DONE Pre-trained (NEW)
CLOSED: [2024-06-24 Mon 08:34]

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |    25|acc     |0.5555|±  |0.0145|
|             |       |none  |    25|acc_norm|0.6032|±  |0.0143|

|  Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|
|---------|------:|------|-----:|--------|-----:|---|-----:|
|hellaswag|      1|none  |    10|acc     |0.6292|±  |0.0048|
|         |       |none  |    10|acc_norm|0.8213|±  |0.0038|

|      Tasks      |Version|Filter|n-shot|  Metric   | Value |   |Stderr|
|-----------------|-------|------|-----:|-----------|------:|---|-----:|
|truthfulqa       |N/A    |none  |     0|rouge1_acc | 0.4259|±  |0.0173|
|                 |       |none  |     0|bleu_diff  |-1.9615|±  |0.7198|
|                 |       |none  |     0|bleu_max   |26.1243|±  |0.7800|
|                 |       |none  |     0|rouge1_diff|-3.0754|±  |0.8534|
|                 |       |none  |     0|rouge2_diff|-4.0291|±  |0.9909|
|                 |       |none  |     0|rougeL_acc | 0.4211|±  |0.0173|
|                 |       |none  |     0|rouge1_max |52.0315|±  |0.8207|
|                 |       |none  |     0|rougeL_max |49.1747|±  |0.8444|
|                 |       |none  |     0|acc        | 0.3599|±  |0.0111|
|                 |       |none  |     0|rouge2_max |36.5127|±  |0.9664|
|                 |       |none  |     0|rougeL_diff|-3.1108|±  |0.8648|
|                 |       |none  |     0|rouge2_acc | 0.3525|±  |0.0167|
|                 |       |none  |     0|bleu_acc   | 0.4113|±  |0.0172|
| - truthfulqa_gen|      3|none  |     0|bleu_max   |26.1243|±  |0.7800|
|                 |       |none  |     0|bleu_acc   | 0.4113|±  |0.0172|
|                 |       |none  |     0|bleu_diff  |-1.9615|±  |0.7198|
|                 |       |none  |     0|rouge1_max |52.0315|±  |0.8207|
|                 |       |none  |     0|rouge1_acc | 0.4259|±  |0.0173|
|                 |       |none  |     0|rouge1_diff|-3.0754|±  |0.8534|
|                 |       |none  |     0|rouge2_max |36.5127|±  |0.9664|
|                 |       |none  |     0|rouge2_acc | 0.3525|±  |0.0167|
|                 |       |none  |     0|rouge2_diff|-4.0291|±  |0.9909|
|                 |       |none  |     0|rougeL_max |49.1747|±  |0.8444|
|                 |       |none  |     0|rougeL_acc | 0.4211|±  |0.0173|
|                 |       |none  |     0|rougeL_diff|-3.1108|±  |0.8648|
| - truthfulqa_mc1|      2|none  |     0|acc        | 0.2803|±  |0.0157|
| - truthfulqa_mc2|      2|none  |     0|acc        | 0.4396|±  |0.0157|

|  Groups  |Version|Filter|n-shot|  Metric   | Value |   |Stderr|
|----------|-------|------|-----:|-----------|------:|---|-----:|
|truthfulqa|N/A    |none  |     0|rouge1_acc | 0.4259|±  |0.0173|
|          |       |none  |     0|bleu_diff  |-1.9615|±  |0.7198|
|          |       |none  |     0|bleu_max   |26.1243|±  |0.7800|
|          |       |none  |     0|rouge1_diff|-3.0754|±  |0.8534|
|          |       |none  |     0|rouge2_diff|-4.0291|±  |0.9909|
|          |       |none  |     0|rougeL_acc | 0.4211|±  |0.0173|
|          |       |none  |     0|rouge1_max |52.0315|±  |0.8207|
|          |       |none  |     0|rougeL_max |49.1747|±  |0.8444|
|          |       |none  |     0|acc        | 0.3599|±  |0.0111|
|          |       |none  |     0|rouge2_max |36.5127|±  |0.9664|
|          |       |none  |     0|rougeL_diff|-3.1108|±  |0.8648|
|          |       |none  |     0|rouge2_acc | 0.3525|±  |0.0167|
|          |       |none  |     0|bleu_acc   | 0.4113|±  |0.0172|

|                 Tasks                 |Version|Filter|n-shot|Metric|Value |   |Stderr|
|---------------------------------------|-------|------|-----:|------|-----:|---|-----:|
|mmlu                                   |N/A    |none  |     0|acc   |0.5355|±  |0.0040|
|  - abstract_algebra                   |      0|none  |     5|acc   |0.3100|±  |0.0465|
|  - anatomy                            |      0|none  |     5|acc   |0.4741|±  |0.0431|
|  - astronomy                          |      0|none  |     5|acc   |0.5724|±  |0.0403|
|  - business_ethics                    |      0|none  |     5|acc   |0.5300|±  |0.0502|
|  - clinical_knowledge                 |      0|none  |     5|acc   |0.5811|±  |0.0304|
|  - college_biology                    |      0|none  |     5|acc   |0.5903|±  |0.0411|
|  - college_chemistry                  |      0|none  |     5|acc   |0.3900|±  |0.0490|
|  - college_computer_science           |      0|none  |     5|acc   |0.4600|±  |0.0501|
|  - college_mathematics                |      0|none  |     5|acc   |0.2600|±  |0.0441|
|  - college_medicine                   |      0|none  |     5|acc   |0.4624|±  |0.0380|
|  - college_physics                    |      0|none  |     5|acc   |0.3137|±  |0.0462|
|  - computer_security                  |      0|none  |     5|acc   |0.6900|±  |0.0465|
|  - conceptual_physics                 |      0|none  |     5|acc   |0.3872|±  |0.0318|
|  - econometrics                       |      0|none  |     5|acc   |0.3158|±  |0.0437|
|  - electrical_engineering             |      0|none  |     5|acc   |0.5034|±  |0.0417|
|  - elementary_mathematics             |      0|none  |     5|acc   |0.3466|±  |0.0245|
|  - formal_logic                       |      0|none  |     5|acc   |0.3016|±  |0.0410|
|  - global_facts                       |      0|none  |     5|acc   |0.3300|±  |0.0473|
|  - high_school_biology                |      0|none  |     5|acc   |0.6516|±  |0.0271|
|  - high_school_chemistry              |      0|none  |     5|acc   |0.4483|±  |0.0350|
|  - high_school_computer_science       |      0|none  |     5|acc   |0.5900|±  |0.0494|
|  - high_school_european_history       |      0|none  |     5|acc   |0.6848|±  |0.0363|
|  - high_school_geography              |      0|none  |     5|acc   |0.7020|±  |0.0326|
|  - high_school_government_and_politics|      0|none  |     5|acc   |0.7876|±  |0.0295|
|  - high_school_macroeconomics         |      0|none  |     5|acc   |0.4872|±  |0.0253|
|  - high_school_mathematics            |      0|none  |     5|acc   |0.3037|±  |0.0280|
|  - high_school_microeconomics         |      0|none  |     5|acc   |0.5252|±  |0.0324|
|  - high_school_physics                |      0|none  |     5|acc   |0.3245|±  |0.0382|
|  - high_school_psychology             |      0|none  |     5|acc   |0.7413|±  |0.0188|
|  - high_school_statistics             |      0|none  |     5|acc   |0.3889|±  |0.0332|
|  - high_school_us_history             |      0|none  |     5|acc   |0.7549|±  |0.0302|
|  - high_school_world_history          |      0|none  |     5|acc   |0.7173|±  |0.0293|
|  - human_aging                        |      0|none  |     5|acc   |0.6457|±  |0.0321|
|  - human_sexuality                    |      0|none  |     5|acc   |0.6183|±  |0.0426|
| - humanities                          |N/A    |none  |     5|acc   |0.4995|±  |0.0069|
|  - international_law                  |      0|none  |     5|acc   |0.7686|±  |0.0385|
|  - jurisprudence                      |      0|none  |     5|acc   |0.7130|±  |0.0437|
|  - logical_fallacies                  |      0|none  |     5|acc   |0.6564|±  |0.0373|
|  - machine_learning                   |      0|none  |     5|acc   |0.3571|±  |0.0455|
|  - management                         |      0|none  |     5|acc   |0.7379|±  |0.0435|
|  - marketing                          |      0|none  |     5|acc   |0.7821|±  |0.0270|
|  - medical_genetics                   |      0|none  |     5|acc   |0.5600|±  |0.0499|
|  - miscellaneous                      |      0|none  |     5|acc   |0.7497|±  |0.0155|
|  - moral_disputes                     |      0|none  |     5|acc   |0.5954|±  |0.0264|
|  - moral_scenarios                    |      0|none  |     5|acc   |0.3140|±  |0.0155|
|  - nutrition                          |      0|none  |     5|acc   |0.6111|±  |0.0279|
| - other                               |N/A    |none  |     5|acc   |0.6041|±  |0.0084|
|  - philosophy                         |      0|none  |     5|acc   |0.6013|±  |0.0278|
|  - prehistory                         |      0|none  |     5|acc   |0.6019|±  |0.0272|
|  - professional_accounting            |      0|none  |     5|acc   |0.3865|±  |0.0290|
|  - professional_law                   |      0|none  |     5|acc   |0.3911|±  |0.0125|
|  - professional_medicine              |      0|none  |     5|acc   |0.4926|±  |0.0304|
|  - professional_psychology            |      0|none  |     5|acc   |0.5343|±  |0.0202|
|  - public_relations                   |      0|none  |     5|acc   |0.6545|±  |0.0455|
|  - security_studies                   |      0|none  |     5|acc   |0.6327|±  |0.0309|
| - social_sciences                     |N/A    |none  |     5|acc   |0.6214|±  |0.0085|
|  - sociology                          |      0|none  |     5|acc   |0.7463|±  |0.0308|
| - stem                                |N/A    |none  |     5|acc   |0.4380|±  |0.0086|
|  - us_foreign_policy                  |      0|none  |     5|acc   |0.8100|±  |0.0394|
|  - virology                           |      0|none  |     5|acc   |0.4880|±  |0.0389|
|  - world_religions                    |      0|none  |     5|acc   |0.7544|±  |0.0330|

|      Groups      |Version|Filter|n-shot|Metric|Value |   |Stderr|
|------------------|-------|------|-----:|------|-----:|---|-----:|
|mmlu              |N/A    |none  |     0|acc   |0.5355|±  |0.0040|
| - humanities     |N/A    |none  |     5|acc   |0.4995|±  |0.0069|
| - other          |N/A    |none  |     5|acc   |0.6041|±  |0.0084|
| - social_sciences|N/A    |none  |     5|acc   |0.6214|±  |0.0085|
| - stem           |N/A    |none  |     5|acc   |0.4380|±  |0.0086|


|  Tasks   |Version|Filter|n-shot|Metric|Value |   |Stderr|
|----------|------:|------|-----:|------|-----:|---|-----:|
|winogrande|      1|none  |     5|acc   |0.7435|±  |0.0123|

|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|
|-----|------:|----------------|-----:|-----------|-----:|---|-----:|
|gsm8k|      3|strict-match    |     5|exact_match|0.3472|±  |0.0131|
|     |       |flexible-extract|     5|exact_match|0.3556|±  |0.0132|



*** DONE LoRD-VIII
CLOSED: [2024-06-14 Fri 14:33]

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |    25|acc     |0.5486|±  |0.0145|
|             |       |none  |    25|acc_norm|0.5956|±  |0.0143|

|  Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|
|---------|------:|------|-----:|--------|-----:|---|-----:|
|hellaswag|      1|none  |    10|acc     |0.5959|±  |0.0049|
|         |       |none  |    10|acc_norm|0.8265|±  |0.0038|

|      Tasks      |Version|Filter|n-shot|  Metric   | Value |   |Stderr|
|-----------------|-------|------|-----:|-----------|------:|---|-----:|
|truthfulqa       |N/A    |none  |     0|rouge2_diff|-4.5748|±  |0.9516|
|                 |       |none  |     0|rouge1_diff|-3.1936|±  |0.8454|
|                 |       |none  |     0|rougeL_max |47.1425|±  |0.8364|
|                 |       |none  |     0|bleu_diff  |-2.0740|±  |0.6736|
|                 |       |none  |     0|rouge2_max |33.7758|±  |0.9531|
|                 |       |none  |     0|rougeL_diff|-3.4456|±  |0.8434|
|                 |       |none  |     0|rougeL_acc | 0.4174|±  |0.0173|
|                 |       |none  |     0|acc        | 0.3863|±  |0.0110|
|                 |       |none  |     0|bleu_max   |24.1206|±  |0.7485|
|                 |       |none  |     0|rouge2_acc | 0.3647|±  |0.0169|
|                 |       |none  |     0|rouge1_acc | 0.4345|±  |0.0174|
|                 |       |none  |     0|rouge1_max |50.2204|±  |0.8222|
|                 |       |none  |     0|bleu_acc   | 0.4321|±  |0.0173|
| - truthfulqa_gen|      3|none  |     0|bleu_max   |24.1206|±  |0.7485|
|                 |       |none  |     0|bleu_acc   | 0.4321|±  |0.0173|
|                 |       |none  |     0|bleu_diff  |-2.0740|±  |0.6736|
|                 |       |none  |     0|rouge1_max |50.2204|±  |0.8222|
|                 |       |none  |     0|rouge1_acc | 0.4345|±  |0.0174|
|                 |       |none  |     0|rouge1_diff|-3.1936|±  |0.8454|
|                 |       |none  |     0|rouge2_max |33.7758|±  |0.9531|
|                 |       |none  |     0|rouge2_acc | 0.3647|±  |0.0169|
|                 |       |none  |     0|rouge2_diff|-4.5748|±  |0.9516|
|                 |       |none  |     0|rougeL_max |47.1425|±  |0.8364|
|                 |       |none  |     0|rougeL_acc | 0.4174|±  |0.0173|
|                 |       |none  |     0|rougeL_diff|-3.4456|±  |0.8434|
| - truthfulqa_mc1|      2|none  |     0|acc        | 0.3023|±  |0.0161|
| - truthfulqa_mc2|      2|none  |     0|acc        | 0.4703|±  |0.0149|

|  Groups  |Version|Filter|n-shot|  Metric   | Value |   |Stderr|
|----------|-------|------|-----:|-----------|------:|---|-----:|
|truthfulqa|N/A    |none  |     0|rouge2_diff|-4.5748|±  |0.9516|
|          |       |none  |     0|rouge1_diff|-3.1936|±  |0.8454|
|          |       |none  |     0|rougeL_max |47.1425|±  |0.8364|
|          |       |none  |     0|bleu_diff  |-2.0740|±  |0.6736|
|          |       |none  |     0|rouge2_max |33.7758|±  |0.9531|
|          |       |none  |     0|rougeL_diff|-3.4456|±  |0.8434|
|          |       |none  |     0|rougeL_acc | 0.4174|±  |0.0173|
|          |       |none  |     0|acc        | 0.3863|±  |0.0110|
|          |       |none  |     0|bleu_max   |24.1206|±  |0.7485|
|          |       |none  |     0|rouge2_acc | 0.3647|±  |0.0169|
|          |       |none  |     0|rouge1_acc | 0.4345|±  |0.0174|
|          |       |none  |     0|rouge1_max |50.2204|±  |0.8222|
|          |       |none  |     0|bleu_acc   | 0.4321|±  |0.0173|

|                 Tasks                 |Version|Filter|n-shot|Metric|Value |   |Stderr|
|---------------------------------------|-------|------|-----:|------|-----:|---|-----:|
|mmlu                                   |N/A    |none  |     0|acc   |0.5327|±  |0.0040|
|  - abstract_algebra                   |      0|none  |     5|acc   |0.3800|±  |0.0488|
|  - anatomy                            |      0|none  |     5|acc   |0.4963|±  |0.0432|
|  - astronomy                          |      0|none  |     5|acc   |0.5592|±  |0.0404|
|  - business_ethics                    |      0|none  |     5|acc   |0.5100|±  |0.0502|
|  - clinical_knowledge                 |      0|none  |     5|acc   |0.5774|±  |0.0304|
|  - college_biology                    |      0|none  |     5|acc   |0.5833|±  |0.0412|
|  - college_chemistry                  |      0|none  |     5|acc   |0.3600|±  |0.0482|
|  - college_computer_science           |      0|none  |     5|acc   |0.5100|±  |0.0502|
|  - college_mathematics                |      0|none  |     5|acc   |0.2700|±  |0.0446|
|  - college_medicine                   |      0|none  |     5|acc   |0.4451|±  |0.0379|
|  - college_physics                    |      0|none  |     5|acc   |0.3333|±  |0.0469|
|  - computer_security                  |      0|none  |     5|acc   |0.6600|±  |0.0476|
|  - conceptual_physics                 |      0|none  |     5|acc   |0.4340|±  |0.0324|
|  - econometrics                       |      0|none  |     5|acc   |0.3158|±  |0.0437|
|  - electrical_engineering             |      0|none  |     5|acc   |0.5103|±  |0.0417|
|  - elementary_mathematics             |      0|none  |     5|acc   |0.3545|±  |0.0246|
|  - formal_logic                       |      0|none  |     5|acc   |0.3175|±  |0.0416|
|  - global_facts                       |      0|none  |     5|acc   |0.3300|±  |0.0473|
|  - high_school_biology                |      0|none  |     5|acc   |0.6355|±  |0.0274|
|  - high_school_chemistry              |      0|none  |     5|acc   |0.4532|±  |0.0350|
|  - high_school_computer_science       |      0|none  |     5|acc   |0.5800|±  |0.0496|
|  - high_school_european_history       |      0|none  |     5|acc   |0.6788|±  |0.0365|
|  - high_school_geography              |      0|none  |     5|acc   |0.7273|±  |0.0317|
|  - high_school_government_and_politics|      0|none  |     5|acc   |0.7668|±  |0.0305|
|  - high_school_macroeconomics         |      0|none  |     5|acc   |0.4949|±  |0.0253|
|  - high_school_mathematics            |      0|none  |     5|acc   |0.2852|±  |0.0275|
|  - high_school_microeconomics         |      0|none  |     5|acc   |0.5420|±  |0.0324|
|  - high_school_physics                |      0|none  |     5|acc   |0.3046|±  |0.0376|
|  - high_school_psychology             |      0|none  |     5|acc   |0.7248|±  |0.0191|
|  - high_school_statistics             |      0|none  |     5|acc   |0.4074|±  |0.0335|
|  - high_school_us_history             |      0|none  |     5|acc   |0.7255|±  |0.0313|
|  - high_school_world_history          |      0|none  |     5|acc   |0.7173|±  |0.0293|
|  - human_aging                        |      0|none  |     5|acc   |0.6413|±  |0.0322|
|  - human_sexuality                    |      0|none  |     5|acc   |0.5954|±  |0.0430|
| - humanities                          |N/A    |none  |     5|acc   |0.4893|±  |0.0069|
|  - international_law                  |      0|none  |     5|acc   |0.7355|±  |0.0403|
|  - jurisprudence                      |      0|none  |     5|acc   |0.7407|±  |0.0424|
|  - logical_fallacies                  |      0|none  |     5|acc   |0.6687|±  |0.0370|
|  - machine_learning                   |      0|none  |     5|acc   |0.3839|±  |0.0462|
|  - management                         |      0|none  |     5|acc   |0.7282|±  |0.0441|
|  - marketing                          |      0|none  |     5|acc   |0.8077|±  |0.0258|
|  - medical_genetics                   |      0|none  |     5|acc   |0.5500|±  |0.0500|
|  - miscellaneous                      |      0|none  |     5|acc   |0.7484|±  |0.0155|
|  - moral_disputes                     |      0|none  |     5|acc   |0.5867|±  |0.0265|
|  - moral_scenarios                    |      0|none  |     5|acc   |0.2793|±  |0.0150|
|  - nutrition                          |      0|none  |     5|acc   |0.6209|±  |0.0278|
| - other                               |N/A    |none  |     5|acc   |0.6048|±  |0.0084|
|  - philosophy                         |      0|none  |     5|acc   |0.5981|±  |0.0278|
|  - prehistory                         |      0|none  |     5|acc   |0.6080|±  |0.0272|
|  - professional_accounting            |      0|none  |     5|acc   |0.3865|±  |0.0290|
|  - professional_law                   |      0|none  |     5|acc   |0.3853|±  |0.0124|
|  - professional_medicine              |      0|none  |     5|acc   |0.4963|±  |0.0304|
|  - professional_psychology            |      0|none  |     5|acc   |0.5310|±  |0.0202|
|  - public_relations                   |      0|none  |     5|acc   |0.6455|±  |0.0458|
|  - security_studies                   |      0|none  |     5|acc   |0.6327|±  |0.0309|
| - social_sciences                     |N/A    |none  |     5|acc   |0.6175|±  |0.0085|
|  - sociology                          |      0|none  |     5|acc   |0.7264|±  |0.0315|
| - stem                                |N/A    |none  |     5|acc   |0.4437|±  |0.0086|
|  - us_foreign_policy                  |      0|none  |     5|acc   |0.8000|±  |0.0402|
|  - virology                           |      0|none  |     5|acc   |0.5000|±  |0.0389|
|  - world_religions                    |      0|none  |     5|acc   |0.7427|±  |0.0335|

|      Groups      |Version|Filter|n-shot|Metric|Value |   |Stderr|
|------------------|-------|------|-----:|------|-----:|---|-----:|
|mmlu              |N/A    |none  |     0|acc   |0.5327|±  |0.0040|
| - humanities     |N/A    |none  |     5|acc   |0.4893|±  |0.0069|
| - other          |N/A    |none  |     5|acc   |0.6048|±  |0.0084|
| - social_sciences|N/A    |none  |     5|acc   |0.6175|±  |0.0085|
| - stem           |N/A    |none  |     5|acc   |0.4437|±  |0.0086|

|  Tasks   |Version|Filter|n-shot|Metric|Value |   |Stderr|
|----------|------:|------|-----:|------|-----:|---|-----:|
|winogrande|      1|none  |     5|acc   |0.7585|±  | 0.012|

|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|
|-----|------:|----------------|-----:|-----------|-----:|---|-----:|
|gsm8k|      3|strict-match    |     5|exact_match|0.3518|±  |0.0132|
|     |       |flexible-extract|     5|exact_match|0.3624|±  |0.0132|

    
*** DONE LoRD-VII
CLOSED: [2024-06-14 Fri 14:33]

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |     25 | acc      | 0.5486 | ± | 0.0145 |
|               |         | none   |     25 | acc_norm | 0.5870 | ± | 0.0144 |


| Tasks     | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|-----------+---------+--------+--------+----------+--------+---+--------|
| hellaswag |       1 | none   |     10 | acc      | 0.6051 | ± | 0.0049 |
|           |         | none   |     10 | acc_norm | 0.8232 | ± | 0.0038 |

| Tasks            | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa       |     N/A | none   |      0 | rouge2_acc  |  0.3488 | ± | 0.0167 |
|                  |         | none   |      0 | bleu_acc    |  0.3978 | ± | 0.0171 |
|                  |         | none   |      0 | rougeL_max  | 49.8098 | ± | 0.8539 |
|                  |         | none   |      0 | rougeL_acc  |  0.3905 | ± | 0.0171 |
|                  |         | none   |      0 | rougeL_diff | -3.7717 | ± | 0.8806 |
|                  |         | none   |      0 | bleu_diff   | -2.3841 | ± | 0.7409 |
|                  |         | none   |      0 | acc         |  0.3712 | ± | 0.0109 |
|                  |         | none   |      0 | rouge1_diff | -3.7566 | ± | 0.8716 |
|                  |         | none   |      0 | rouge2_diff | -4.8063 | ± | 1.0099 |
|                  |         | none   |      0 | rouge2_max  | 37.0193 | ± | 0.9778 |
|                  |         | none   |      0 | rouge1_acc  |  0.4051 | ± | 0.0172 |
|                  |         | none   |      0 | bleu_max    | 26.6278 | ± | 0.7809 |
|                  |         | none   |      0 | rouge1_max  | 52.7716 | ± | 0.8335 |
| - truthfulqa_gen |       3 | none   |      0 | bleu_max    | 26.6278 | ± | 0.7809 |
|                  |         | none   |      0 | bleu_acc    |  0.3978 | ± | 0.0171 |
|                  |         | none   |      0 | bleu_diff   | -2.3841 | ± | 0.7409 |
|                  |         | none   |      0 | rouge1_max  | 52.7716 | ± | 0.8335 |
|                  |         | none   |      0 | rouge1_acc  |  0.4051 | ± | 0.0172 |
|                  |         | none   |      0 | rouge1_diff | -3.7566 | ± | 0.8716 |
|                  |         | none   |      0 | rouge2_max  | 37.0193 | ± | 0.9778 |
|                  |         | none   |      0 | rouge2_acc  |  0.3488 | ± | 0.0167 |
|                  |         | none   |      0 | rouge2_diff | -4.8063 | ± | 1.0099 |
|                  |         | none   |      0 | rougeL_max  | 49.8098 | ± | 0.8539 |
|                  |         | none   |      0 | rougeL_acc  |  0.3905 | ± | 0.0171 |
|                  |         | none   |      0 | rougeL_diff | -3.7717 | ± | 0.8806 |
| - truthfulqa_mc1 |       2 | none   |      0 | acc         |  0.2938 | ± | 0.0159 |
| - truthfulqa_mc2 |       2 | none   |      0 | acc         |  0.4487 | ± | 0.0149 |


| Groups     | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa | N/A     | none   |      0 | rouge2_acc  |  0.3488 | ± | 0.0167 |
|            |         | none   |      0 | bleu_acc    |  0.3978 | ± | 0.0171 |
|            |         | none   |      0 | rougeL_max  | 49.8098 | ± | 0.8539 |
|            |         | none   |      0 | rougeL_acc  |  0.3905 | ± | 0.0171 |
|            |         | none   |      0 | rougeL_diff | -3.7717 | ± | 0.8806 |
|            |         | none   |      0 | bleu_diff   | -2.3841 | ± | 0.7409 |
|            |         | none   |      0 | acc         |  0.3712 | ± | 0.0109 |
|            |         | none   |      0 | rouge1_diff | -3.7566 | ± | 0.8716 |
|            |         | none   |      0 | rouge2_diff | -4.8063 | ± | 1.0099 |
|            |         | none   |      0 | rouge2_max  | 37.0193 | ± | 0.9778 |
|            |         | none   |      0 | rouge1_acc  |  0.4051 | ± | 0.0172 |
|            |         | none   |      0 | bleu_max    | 26.6278 | ± | 0.7809 |
|            |         | none   |      0 | rouge1_max  | 52.7716 | ± | 0.8335 |

| Tasks                                 | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|---------------------------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu                                  |     N/A | none   |      0 | acc    | 0.5327 | ± | 0.0040 |
| - abstract_algebra                    |       0 | none   |      5 | acc    | 0.3300 | ± | 0.0473 |
| - anatomy                             |       0 | none   |      5 | acc    | 0.4963 | ± | 0.0432 |
| - astronomy                           |       0 | none   |      5 | acc    | 0.5789 | ± | 0.0402 |
| - business_ethics                     |       0 | none   |      5 | acc    | 0.5300 | ± | 0.0502 |
| - clinical_knowledge                  |       0 | none   |      5 | acc    | 0.5660 | ± | 0.0305 |
| - college_biology                     |       0 | none   |      5 | acc    | 0.5764 | ± | 0.0413 |
| - college_chemistry                   |       0 | none   |      5 | acc    | 0.3700 | ± | 0.0485 |
| - college_computer_science            |       0 | none   |      5 | acc    | 0.4800 | ± | 0.0502 |
| - college_mathematics                 |       0 | none   |      5 | acc    | 0.2800 | ± | 0.0451 |
| - college_medicine                    |       0 | none   |      5 | acc    | 0.4509 | ± | 0.0379 |
| - college_physics                     |       0 | none   |      5 | acc    | 0.3137 | ± | 0.0462 |
| - computer_security                   |       0 | none   |      5 | acc    | 0.6600 | ± | 0.0476 |
| - conceptual_physics                  |       0 | none   |      5 | acc    | 0.4043 | ± | 0.0321 |
| - econometrics                        |       0 | none   |      5 | acc    | 0.3246 | ± | 0.0440 |
| - electrical_engineering              |       0 | none   |      5 | acc    | 0.5103 | ± | 0.0417 |
| - elementary_mathematics              |       0 | none   |      5 | acc    | 0.3466 | ± | 0.0245 |
| - formal_logic                        |       0 | none   |      5 | acc    | 0.3254 | ± | 0.0419 |
| - global_facts                        |       0 | none   |      5 | acc    | 0.3200 | ± | 0.0469 |
| - high_school_biology                 |       0 | none   |      5 | acc    | 0.6484 | ± | 0.0272 |
| - high_school_chemistry               |       0 | none   |      5 | acc    | 0.4384 | ± | 0.0349 |
| - high_school_computer_science        |       0 | none   |      5 | acc    | 0.5700 | ± | 0.0498 |
| - high_school_european_history        |       0 | none   |      5 | acc    | 0.6545 | ± | 0.0371 |
| - high_school_geography               |       0 | none   |      5 | acc    | 0.7020 | ± | 0.0326 |
| - high_school_government_and_politics |       0 | none   |      5 | acc    | 0.7876 | ± | 0.0295 |
| - high_school_macroeconomics          |       0 | none   |      5 | acc    | 0.4974 | ± | 0.0254 |
| - high_school_mathematics             |       0 | none   |      5 | acc    | 0.3037 | ± | 0.0280 |
| - high_school_microeconomics          |       0 | none   |      5 | acc    | 0.5252 | ± | 0.0324 |
| - high_school_physics                 |       0 | none   |      5 | acc    | 0.3377 | ± | 0.0386 |
| - high_school_psychology              |       0 | none   |      5 | acc    | 0.7376 | ± | 0.0189 |
| - high_school_statistics              |       0 | none   |      5 | acc    | 0.4259 | ± | 0.0337 |
| - high_school_us_history              |       0 | none   |      5 | acc    | 0.7255 | ± | 0.0313 |
| - high_school_world_history           |       0 | none   |      5 | acc    | 0.7173 | ± | 0.0293 |
| - human_aging                         |       0 | none   |      5 | acc    | 0.6323 | ± | 0.0324 |
| - human_sexuality                     |       0 | none   |      5 | acc    | 0.5954 | ± | 0.0430 |
| - humanities                          |     N/A | none   |      5 | acc    | 0.4903 | ± | 0.0069 |
| - international_law                   |       0 | none   |      5 | acc    | 0.7521 | ± | 0.0394 |
| - jurisprudence                       |       0 | none   |      5 | acc    | 0.7222 | ± | 0.0433 |
| - logical_fallacies                   |       0 | none   |      5 | acc    | 0.6380 | ± | 0.0378 |
| - machine_learning                    |       0 | none   |      5 | acc    | 0.3393 | ± | 0.0449 |
| - management                          |       0 | none   |      5 | acc    | 0.7476 | ± | 0.0430 |
| - marketing                           |       0 | none   |      5 | acc    | 0.7821 | ± | 0.0270 |
| - medical_genetics                    |       0 | none   |      5 | acc    | 0.5600 | ± | 0.0499 |
| - miscellaneous                       |       0 | none   |      5 | acc    | 0.7510 | ± | 0.0155 |
| - moral_disputes                      |       0 | none   |      5 | acc    | 0.5983 | ± | 0.0264 |
| - moral_scenarios                     |       0 | none   |      5 | acc    | 0.2983 | ± | 0.0153 |
| - nutrition                           |       0 | none   |      5 | acc    | 0.6144 | ± | 0.0279 |
| - other                               |     N/A | none   |      5 | acc    | 0.6035 | ± | 0.0084 |
| - philosophy                          |       0 | none   |      5 | acc    | 0.6109 | ± | 0.0277 |
| - prehistory                          |       0 | none   |      5 | acc    | 0.5926 | ± | 0.0273 |
| - professional_accounting             |       0 | none   |      5 | acc    | 0.3936 | ± | 0.0291 |
| - professional_law                    |       0 | none   |      5 | acc    | 0.3833 | ± | 0.0124 |
| - professional_medicine               |       0 | none   |      5 | acc    | 0.5074 | ± | 0.0304 |
| - professional_psychology             |       0 | none   |      5 | acc    | 0.5278 | ± | 0.0202 |
| - public_relations                    |       0 | none   |      5 | acc    | 0.6545 | ± | 0.0455 |
| - security_studies                    |       0 | none   |      5 | acc    | 0.6408 | ± | 0.0307 |
| - social_sciences                     |     N/A | none   |      5 | acc    | 0.6194 | ± | 0.0085 |
| - sociology                           |       0 | none   |      5 | acc    | 0.7264 | ± | 0.0315 |
| - stem                                |     N/A | none   |      5 | acc    | 0.4415 | ± | 0.0086 |
| - us_foreign_policy                   |       0 | none   |      5 | acc    | 0.8100 | ± | 0.0394 |
| - virology                            |       0 | none   |      5 | acc    | 0.4819 | ± | 0.0389 |
| - world_religions                     |       0 | none   |      5 | acc    | 0.7193 | ± | 0.0345 |
                                                                                                                                                                                                                                    
| Groups            | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|-------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu              | N/A     | none   |      0 | acc    | 0.5327 | ± | 0.0040 |
| - humanities      | N/A     | none   |      5 | acc    | 0.4903 | ± | 0.0069 |
| - other           | N/A     | none   |      5 | acc    | 0.6035 | ± | 0.0084 |
| - social_sciences | N/A     | none   |      5 | acc    | 0.6194 | ± | 0.0085 |
| - stem            | N/A     | none   |      5 | acc    | 0.4415 | ± | 0.0086 |


| Tasks      | Version | Filter | n-shot | Metric | Value |   | Stderr |
|------------+---------+--------+--------+--------+-------+---+--------|
| winogrande |       1 | none   |      5 | acc    | 0.753 | ± | 0.0121 |


| Tasks | Version | Filter           | n-shot | Metric      |  Value |   | Stderr |
|-------+---------+------------------+--------+-------------+--------+---+--------|
| gsm8k |       3 | strict-match     |      5 | exact_match | 0.3412 | ± | 0.0131 |
|       |         | flexible-extract |      5 | exact_match | 0.3480 | ± | 0.0131 |

*** DONE Vanilla
CLOSED: [2024-06-14 Fri 14:35]


| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |     25 | acc      | 0.5555 | ± | 0.0145 |
|               |         | none   |     25 | acc_norm | 0.6041 | ± | 0.0143 |



| Tasks     | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|-----------+---------+--------+--------+----------+--------+---+--------|
| hellaswag |       1 | none   |     10 | acc      | 0.6059 | ± | 0.0049 |
|           |         | none   |     10 | acc_norm | 0.8214 | ± | 0.0038 |


| Tasks            | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa       |     N/A | none   |      0 | bleu_acc    |  0.3770 | ± | 0.0170 |
|                  |         | none   |      0 | rougeL_acc  |  0.3978 | ± | 0.0171 |
|                  |         | none   |      0 | rouge1_acc  |  0.4064 | ± | 0.0172 |
|                  |         | none   |      0 | rouge2_diff | -2.7640 | ± | 0.9504 |
|                  |         | none   |      0 | rougeL_diff | -2.0250 | ± | 0.8208 |
|                  |         | none   |      0 | bleu_max    | 23.1928 | ± | 0.8090 |
|                  |         | none   |      0 | rouge1_max  | 47.1508 | ± | 0.9493 |
|                  |         | none   |      0 | bleu_diff   | -1.4562 | ± | 0.7096 |
|                  |         | none   |      0 | rouge2_acc  |  0.3170 | ± | 0.0163 |
|                  |         | none   |      0 | rouge2_max  | 32.0114 | ± | 1.0256 |
|                  |         | none   |      0 | rouge1_diff | -1.8701 | ± | 0.8138 |
|                  |         | none   |      0 | rougeL_max  | 44.2941 | ± | 0.9446 |
|                  |         | none   |      0 | acc         |  0.3841 | ± | 0.0111 |
| - truthfulqa_gen |       3 | none   |      0 | bleu_max    | 23.1928 | ± | 0.8090 |
|                  |         | none   |      0 | bleu_acc    |  0.3770 | ± | 0.0170 |
|                  |         | none   |      0 | bleu_diff   | -1.4562 | ± | 0.7096 |
|                  |         | none   |      0 | rouge1_max  | 47.1508 | ± | 0.9493 |
|                  |         | none   |      0 | rouge1_acc  |  0.4064 | ± | 0.0172 |
|                  |         | none   |      0 | rouge1_diff | -1.8701 | ± | 0.8138 |
|                  |         | none   |      0 | rouge2_max  | 32.0114 | ± | 1.0256 |
|                  |         | none   |      0 | rouge2_acc  |  0.3170 | ± | 0.0163 |
|                  |         | none   |      0 | rouge2_diff | -2.7640 | ± | 0.9504 |
|                  |         | none   |      0 | rougeL_max  | 44.2941 | ± | 0.9446 |
|                  |         | none   |      0 | rougeL_acc  |  0.3978 | ± | 0.0171 |
|                  |         | none   |      0 | rougeL_diff | -2.0250 | ± | 0.8208 |
| - truthfulqa_mc1 |       2 | none   |      0 | acc         |  0.2999 | ± | 0.0160 |
| - truthfulqa_mc2 |       2 | none   |      0 | acc         |  0.4684 | ± | 0.0154 |
                                                                                                                                                                                                                                    
| Groups     | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa | N/A     | none   |      0 | bleu_acc    |  0.3770 | ± | 0.0170 |
|            |         | none   |      0 | rougeL_acc  |  0.3978 | ± | 0.0171 |
|            |         | none   |      0 | rouge1_acc  |  0.4064 | ± | 0.0172 |
|            |         | none   |      0 | rouge2_diff | -2.7640 | ± | 0.9504 |
|            |         | none   |      0 | rougeL_diff | -2.0250 | ± | 0.8208 |
|            |         | none   |      0 | bleu_max    | 23.1928 | ± | 0.8090 |
|            |         | none   |      0 | rouge1_max  | 47.1508 | ± | 0.9493 |
|            |         | none   |      0 | bleu_diff   | -1.4562 | ± | 0.7096 |
|            |         | none   |      0 | rouge2_acc  |  0.3170 | ± | 0.0163 |
|            |         | none   |      0 | rouge2_max  | 32.0114 | ± | 1.0256 |
|            |         | none   |      0 | rouge1_diff | -1.8701 | ± | 0.8138 |
|            |         | none   |      0 | rougeL_max  | 44.2941 | ± | 0.9446 |
|            |         | none   |      0 | acc         |  0.3841 | ± | 0.0111 |
                                                                                                                                                                                                                                    

| Tasks                                 | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|---------------------------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu                                  |     N/A | none   |      0 | acc    | 0.5217 | ± | 0.0040 |
| - abstract_algebra                    |       0 | none   |      5 | acc    | 0.4200 | ± | 0.0496 |
| - anatomy                             |       0 | none   |      5 | acc    | 0.4889 | ± | 0.0432 |
| - astronomy                           |       0 | none   |      5 | acc    | 0.5724 | ± | 0.0403 |
| - business_ethics                     |       0 | none   |      5 | acc    | 0.5400 | ± | 0.0501 |
| - clinical_knowledge                  |       0 | none   |      5 | acc    | 0.5472 | ± | 0.0306 |
| - college_biology                     |       0 | none   |      5 | acc    | 0.5694 | ± | 0.0414 |
| - college_chemistry                   |       0 | none   |      5 | acc    | 0.3600 | ± | 0.0482 |
| - college_computer_science            |       0 | none   |      5 | acc    | 0.4800 | ± | 0.0502 |
| - college_mathematics                 |       0 | none   |      5 | acc    | 0.3100 | ± | 0.0465 |
| - college_medicine                    |       0 | none   |      5 | acc    | 0.4162 | ± | 0.0376 |
| - college_physics                     |       0 | none   |      5 | acc    | 0.3431 | ± | 0.0472 |
| - computer_security                   |       0 | none   |      5 | acc    | 0.6500 | ± | 0.0479 |
| - conceptual_physics                  |       0 | none   |      5 | acc    | 0.4043 | ± | 0.0321 |
| - econometrics                        |       0 | none   |      5 | acc    | 0.3333 | ± | 0.0443 |
| - electrical_engineering              |       0 | none   |      5 | acc    | 0.5448 | ± | 0.0415 |
| - elementary_mathematics              |       0 | none   |      5 | acc    | 0.3333 | ± | 0.0243 |
| - formal_logic                        |       0 | none   |      5 | acc    | 0.3095 | ± | 0.0413 |
| - global_facts                        |       0 | none   |      5 | acc    | 0.3400 | ± | 0.0476 |
| - high_school_biology                 |       0 | none   |      5 | acc    | 0.6355 | ± | 0.0274 |
| - high_school_chemistry               |       0 | none   |      5 | acc    | 0.4236 | ± | 0.0348 |
| - high_school_computer_science        |       0 | none   |      5 | acc    | 0.5500 | ± | 0.0500 |
| - high_school_european_history        |       0 | none   |      5 | acc    | 0.6545 | ± | 0.0371 |
| - high_school_geography               |       0 | none   |      5 | acc    | 0.6970 | ± | 0.0327 |
| - high_school_government_and_politics |       0 | none   |      5 | acc    | 0.7513 | ± | 0.0312 |
| - high_school_macroeconomics          |       0 | none   |      5 | acc    | 0.5026 | ± | 0.0254 |
| - high_school_mathematics             |       0 | none   |      5 | acc    | 0.3037 | ± | 0.0280 |
| - high_school_microeconomics          |       0 | none   |      5 | acc    | 0.5294 | ± | 0.0324 |
| - high_school_physics                 |       0 | none   |      5 | acc    | 0.3510 | ± | 0.0390 |
| - high_school_psychology              |       0 | none   |      5 | acc    | 0.6991 | ± | 0.0197 |
| - high_school_statistics              |       0 | none   |      5 | acc    | 0.3889 | ± | 0.0332 |
| - high_school_us_history              |       0 | none   |      5 | acc    | 0.6814 | ± | 0.0327 |
| - high_school_world_history           |       0 | none   |      5 | acc    | 0.7004 | ± | 0.0298 |
| - human_aging                         |       0 | none   |      5 | acc    | 0.6368 | ± | 0.0323 |
| - human_sexuality                     |       0 | none   |      5 | acc    | 0.5878 | ± | 0.0432 |
| - humanities                          |     N/A | none   |      5 | acc    | 0.4793 | ± | 0.0069 |
| - international_law                   |       0 | none   |      5 | acc    | 0.6942 | ± | 0.0421 |
| - jurisprudence                       |       0 | none   |      5 | acc    | 0.7222 | ± | 0.0433 |
| - logical_fallacies                   |       0 | none   |      5 | acc    | 0.6564 | ± | 0.0373 |
| - machine_learning                    |       0 | none   |      5 | acc    | 0.3571 | ± | 0.0455 |
| - management                          |       0 | none   |      5 | acc    | 0.6602 | ± | 0.0469 |
| - marketing                           |       0 | none   |      5 | acc    | 0.7735 | ± | 0.0274 |
| - medical_genetics                    |       0 | none   |      5 | acc    | 0.5300 | ± | 0.0502 |
| - miscellaneous                       |       0 | none   |      5 | acc    | 0.7318 | ± | 0.0158 |
| - moral_disputes                      |       0 | none   |      5 | acc    | 0.5780 | ± | 0.0266 |
| - moral_scenarios                     |       0 | none   |      5 | acc    | 0.2737 | ± | 0.0149 |
| - nutrition                           |       0 | none   |      5 | acc    | 0.5980 | ± | 0.0281 |
| - other                               |     N/A | none   |      5 | acc    | 0.5861 | ± | 0.0085 |
| - philosophy                          |       0 | none   |      5 | acc    | 0.6141 | ± | 0.0276 |
| - prehistory                          |       0 | none   |      5 | acc    | 0.5864 | ± | 0.0274 |
| - professional_accounting             |       0 | none   |      5 | acc    | 0.3901 | ± | 0.0291 |
| - professional_law                    |       0 | none   |      5 | acc    | 0.3781 | ± | 0.0124 |
| - professional_medicine               |       0 | none   |      5 | acc    | 0.4706 | ± | 0.0303 |
| - professional_psychology             |       0 | none   |      5 | acc    | 0.5196 | ± | 0.0202 |
| - public_relations                    |       0 | none   |      5 | acc    | 0.6182 | ± | 0.0465 |
| - security_studies                    |       0 | none   |      5 | acc    | 0.6082 | ± | 0.0313 |
| - social_sciences                     |     N/A | none   |      5 | acc    | 0.6048 | ± | 0.0086 |
| - sociology                           |       0 | none   |      5 | acc    | 0.7264 | ± | 0.0315 |
| - stem                                |     N/A | none   |      5 | acc    | 0.4405 | ± | 0.0086 |
| - us_foreign_policy                   |       0 | none   |      5 | acc    | 0.7900 | ± | 0.0409 |
| - virology                            |       0 | none   |      5 | acc    | 0.4699 | ± | 0.0389 |
| - world_religions                     |       0 | none   |      5 | acc    | 0.7485 | ± | 0.0333 |
                                                                                                                                                                                                                                    
| Groups            | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|-------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu              | N/A     | none   |      0 | acc    | 0.5217 | ± | 0.0040 |
| - humanities      | N/A     | none   |      5 | acc    | 0.4793 | ± | 0.0069 |
| - other           | N/A     | none   |      5 | acc    | 0.5861 | ± | 0.0085 |
| - social_sciences | N/A     | none   |      5 | acc    | 0.6048 | ± | 0.0086 |
| - stem            | N/A     | none   |      5 | acc    | 0.4405 | ± | 0.0086 |

** SHORT-TEXT [Mistral-22B]
*** DONE Pretrained
CLOSED: [2024-06-18 Tue 09:03]

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |     25 | acc      | 0.2133 | ± | 0.0120 |
|               |         | none   |     25 | acc_norm | 0.2406 | ± | 0.0125 |

| Tasks     | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|-----------+---------+--------+--------+----------+--------+---+--------|
| hellaswag |       1 | none   |     10 | acc      | 0.2740 | ± | 0.0045 |
|           |         | none   |     10 | acc_norm | 0.2869 | ± | 0.0045 |

| Tasks            | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa       |     N/A | none   |      0 | rouge1_acc  |  0.3084 | ± | 0.0162 |
|                  |         | none   |      0 | rouge1_max  |  2.6076 | ± | 0.0982 |
|                  |         | none   |      0 | acc         |  0.3847 | ± | 0.0114 |
|                  |         | none   |      0 | bleu_diff   | -0.0462 | ± | 0.0270 |
|                  |         | none   |      0 | rouge2_acc  |  0.0685 | ± | 0.0088 |
|                  |         | none   |      0 | rougeL_max  |  2.3364 | ± | 0.0798 |
|                  |         | none   |      0 | rouge2_diff | -0.1233 | ± | 0.0354 |
|                  |         | none   |      0 | rougeL_diff | -0.2100 | ± | 0.0578 |
|                  |         | none   |      0 | bleu_max    |  0.4196 | ± | 0.0274 |
|                  |         | none   |      0 | rouge2_max  |  0.3726 | ± | 0.0370 |
|                  |         | none   |      0 | rouge1_diff | -0.2219 | ± | 0.0658 |
|                  |         | none   |      0 | bleu_acc    |  0.2142 | ± | 0.0144 |
|                  |         | none   |      0 | rougeL_acc  |  0.2889 | ± | 0.0159 |
| - truthfulqa_gen |       3 | none   |      0 | bleu_max    |  0.4196 | ± | 0.0274 |
|                  |         | none   |      0 | bleu_acc    |  0.2142 | ± | 0.0144 |
|                  |         | none   |      0 | bleu_diff   | -0.0462 | ± | 0.0270 |
|                  |         | none   |      0 | rouge1_max  |  2.6076 | ± | 0.0982 |
|                  |         | none   |      0 | rouge1_acc  |  0.3084 | ± | 0.0162 |
|                  |         | none   |      0 | rouge1_diff | -0.2219 | ± | 0.0658 |
|                  |         | none   |      0 | rouge2_max  |  0.3726 | ± | 0.0370 |
|                  |         | none   |      0 | rouge2_acc  |  0.0685 | ± | 0.0088 |
|                  |         | none   |      0 | rouge2_diff | -0.1233 | ± | 0.0354 |
|                  |         | none   |      0 | rougeL_max  |  2.3364 | ± | 0.0798 |
|                  |         | none   |      0 | rougeL_acc  |  0.2889 | ± | 0.0159 |
|                  |         | none   |      0 | rougeL_diff | -0.2100 | ± | 0.0578 |
| - truthfulqa_mc1 |       2 | none   |      0 | acc         |  0.2815 | ± | 0.0157 |
| - truthfulqa_mc2 |       2 | none   |      0 | acc         |  0.4879 | ± | 0.0165 |
                                                                                                                                                                                                                                    
| Groups     | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa | N/A     | none   |      0 | rouge1_acc  |  0.3084 | ± | 0.0162 |
|            |         | none   |      0 | rouge1_max  |  2.6076 | ± | 0.0982 |
|            |         | none   |      0 | acc         |  0.3847 | ± | 0.0114 |
|            |         | none   |      0 | bleu_diff   | -0.0462 | ± | 0.0270 |
|            |         | none   |      0 | rouge2_acc  |  0.0685 | ± | 0.0088 |
|            |         | none   |      0 | rougeL_max  |  2.3364 | ± | 0.0798 |
|            |         | none   |      0 | rouge2_diff | -0.1233 | ± | 0.0354 |
|            |         | none   |      0 | rougeL_diff | -0.2100 | ± | 0.0578 |
|            |         | none   |      0 | bleu_max    |  0.4196 | ± | 0.0274 |
|            |         | none   |      0 | rouge2_max  |  0.3726 | ± | 0.0370 |
|            |         | none   |      0 | rouge1_diff | -0.2219 | ± | 0.0658 |
|            |         | none   |      0 | bleu_acc    |  0.2142 | ± | 0.0144 |
|            |         | none   |      0 | rougeL_acc  |  0.2889 | ± | 0.0159 |

| Tasks                                 | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|---------------------------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu                                  |     N/A | none   |      0 | acc    | 0.2499 | ± | 0.0036 |
| - abstract_algebra                    |       0 | none   |      5 | acc    | 0.2200 | ± | 0.0416 |
| - anatomy                             |       0 | none   |      5 | acc    | 0.2519 | ± | 0.0375 |
| - astronomy                           |       0 | none   |      5 | acc    | 0.1776 | ± | 0.0311 |
| - business_ethics                     |       0 | none   |      5 | acc    | 0.2700 | ± | 0.0446 |
| - clinical_knowledge                  |       0 | none   |      5 | acc    | 0.2679 | ± | 0.0273 |
| - college_biology                     |       0 | none   |      5 | acc    | 0.2222 | ± | 0.0348 |
| - college_chemistry                   |       0 | none   |      5 | acc    | 0.2000 | ± | 0.0402 |
| - college_computer_science            |       0 | none   |      5 | acc    | 0.1500 | ± | 0.0359 |
| - college_mathematics                 |       0 | none   |      5 | acc    | 0.2300 | ± | 0.0423 |
| - college_medicine                    |       0 | none   |      5 | acc    | 0.1792 | ± | 0.0292 |
| - college_physics                     |       0 | none   |      5 | acc    | 0.2157 | ± | 0.0409 |
| - computer_security                   |       0 | none   |      5 | acc    | 0.1600 | ± | 0.0368 |
| - conceptual_physics                  |       0 | none   |      5 | acc    | 0.2638 | ± | 0.0288 |
| - econometrics                        |       0 | none   |      5 | acc    | 0.2544 | ± | 0.0410 |
| - electrical_engineering              |       0 | none   |      5 | acc    | 0.2414 | ± | 0.0357 |
| - elementary_mathematics              |       0 | none   |      5 | acc    | 0.2566 | ± | 0.0225 |
| - formal_logic                        |       0 | none   |      5 | acc    | 0.1984 | ± | 0.0357 |
| - global_facts                        |       0 | none   |      5 | acc    | 0.1800 | ± | 0.0386 |
| - high_school_biology                 |       0 | none   |      5 | acc    | 0.3129 | ± | 0.0264 |
| - high_school_chemistry               |       0 | none   |      5 | acc    | 0.2414 | ± | 0.0301 |
| - high_school_computer_science        |       0 | none   |      5 | acc    | 0.2200 | ± | 0.0416 |
| - high_school_european_history        |       0 | none   |      5 | acc    | 0.2242 | ± | 0.0326 |
| - high_school_geography               |       0 | none   |      5 | acc    | 0.2172 | ± | 0.0294 |
| - high_school_government_and_politics |       0 | none   |      5 | acc    | 0.2487 | ± | 0.0312 |
| - high_school_macroeconomics          |       0 | none   |      5 | acc    | 0.3385 | ± | 0.0240 |
| - high_school_mathematics             |       0 | none   |      5 | acc    | 0.2630 | ± | 0.0268 |
| - high_school_microeconomics          |       0 | none   |      5 | acc    | 0.2311 | ± | 0.0274 |
| - high_school_physics                 |       0 | none   |      5 | acc    | 0.1987 | ± | 0.0326 |
| - high_school_psychology              |       0 | none   |      5 | acc    | 0.2385 | ± | 0.0183 |
| - high_school_statistics              |       0 | none   |      5 | acc    | 0.4722 | ± | 0.0340 |
| - high_school_us_history              |       0 | none   |      5 | acc    | 0.2353 | ± | 0.0298 |
| - high_school_world_history           |       0 | none   |      5 | acc    | 0.2616 | ± | 0.0286 |
| - human_aging                         |       0 | none   |      5 | acc    | 0.3184 | ± | 0.0313 |
| - human_sexuality                     |       0 | none   |      5 | acc    | 0.2443 | ± | 0.0377 |
| - humanities                          |     N/A | none   |      5 | acc    | 0.2406 | ± | 0.0062 |
| - international_law                   |       0 | none   |      5 | acc    | 0.2479 | ± | 0.0394 |
| - jurisprudence                       |       0 | none   |      5 | acc    | 0.2130 | ± | 0.0396 |
| - logical_fallacies                   |       0 | none   |      5 | acc    | 0.2454 | ± | 0.0338 |
| - machine_learning                    |       0 | none   |      5 | acc    | 0.1875 | ± | 0.0370 |
| - management                          |       0 | none   |      5 | acc    | 0.1748 | ± | 0.0376 |
| - marketing                           |       0 | none   |      5 | acc    | 0.2051 | ± | 0.0265 |
| - medical_genetics                    |       0 | none   |      5 | acc    | 0.3000 | ± | 0.0461 |
| - miscellaneous                       |       0 | none   |      5 | acc    | 0.2414 | ± | 0.0153 |
| - moral_disputes                      |       0 | none   |      5 | acc    | 0.2514 | ± | 0.0234 |
| - moral_scenarios                     |       0 | none   |      5 | acc    | 0.2447 | ± | 0.0144 |
| - nutrition                           |       0 | none   |      5 | acc    | 0.2288 | ± | 0.0241 |
| - other                               |     N/A | none   |      5 | acc    | 0.2636 | ± | 0.0078 |
| - philosophy                          |       0 | none   |      5 | acc    | 0.2154 | ± | 0.0234 |
| - prehistory                          |       0 | none   |      5 | acc    | 0.2346 | ± | 0.0236 |
| - professional_accounting             |       0 | none   |      5 | acc    | 0.2553 | ± | 0.0260 |
| - professional_law                    |       0 | none   |      5 | acc    | 0.2490 | ± | 0.0110 |
| - professional_medicine               |       0 | none   |      5 | acc    | 0.4596 | ± | 0.0303 |
| - professional_psychology             |       0 | none   |      5 | acc    | 0.2500 | ± | 0.0175 |
| - public_relations                    |       0 | none   |      5 | acc    | 0.2091 | ± | 0.0390 |
| - security_studies                    |       0 | none   |      5 | acc    | 0.1714 | ± | 0.0241 |
| - social_sciences                     |     N/A | none   |      5 | acc    | 0.2473 | ± | 0.0078 |
| - sociology                           |       0 | none   |      5 | acc    | 0.2388 | ± | 0.0301 |
| - stem                                |     N/A | none   |      5 | acc    | 0.2528 | ± | 0.0077 |
| - us_foreign_policy                   |       0 | none   |      5 | acc    | 0.2600 | ± | 0.0441 |
| - virology                            |       0 | none   |      5 | acc    | 0.2952 | ± | 0.0355 |
| - world_religions                     |       0 | none   |      5 | acc    | 0.2105 | ± | 0.0313 |
                                                                                                                                                                                                                                    
| Groups            | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|-------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu              | N/A     | none   |      0 | acc    | 0.2499 | ± | 0.0036 |
| - humanities      | N/A     | none   |      5 | acc    | 0.2406 | ± | 0.0062 |
| - other           | N/A     | none   |      5 | acc    | 0.2636 | ± | 0.0078 |
| - social_sciences | N/A     | none   |      5 | acc    | 0.2473 | ± | 0.0078 |
| - stem            | N/A     | none   |      5 | acc    | 0.2528 | ± | 0.0077 |

|  Tasks   |Version|Filter|n-shot|Metric|Value |   |Stderr|
|----------|------:|------|-----:|------|-----:|---|-----:|
|winogrande|      1|none  |     5|acc   |0.5075|±  |0.0141|

| Tasks | Version | Filter           | n-shot | Metric      |  Value |   | Stderr |
|-------+---------+------------------+--------+-------------+--------+---+--------|
| gsm8k |       3 | strict-match     |      5 | exact_match | 0.0000 | ± | 0.0000 |
|       |         | flexible-extract |      5 | exact_match | 0.0076 | ± | 0.0024 |
*** DONE LoRD-VIII
CLOSED: [2024-06-18 Tue 09:09]

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |    25|acc     |0.2278|±  |0.0123|
|             |       |none  |    25|acc_norm|0.2560|±  |0.0128|

|  Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|
|---------|------:|------|-----:|--------|-----:|---|-----:|
|hellaswag|      1|none  |    10|acc     |0.2680|±  |0.0044|
|         |       |none  |    10|acc_norm|0.2769|±  |0.0045|

|      Tasks      |Version|Filter|n-shot|  Metric   |Value |   |Stderr|
|-----------------|-------|------|-----:|-----------|-----:|---|-----:|
|truthfulqa       |N/A    |none  |     0|rouge1_acc |0.3647|±  |0.0169|
|                 |       |none  |     0|rouge2_diff|0.1311|±  |0.0549|
|                 |       |none  |     0|rouge2_max |0.7904|±  |0.0698|
|                 |       |none  |     0|rougeL_diff|0.0734|±  |0.0772|
|                 |       |none  |     0|rouge1_diff|0.0591|±  |0.0847|
|                 |       |none  |     0|rouge1_max |2.8330|±  |0.1224|
|                 |       |none  |     0|rouge2_acc |0.1603|±  |0.0128|
|                 |       |none  |     0|rougeL_acc |0.3464|±  |0.0167|
|                 |       |none  |     0|bleu_acc   |0.2815|±  |0.0157|
|                 |       |none  |     0|acc        |0.3866|±  |0.0114|
|                 |       |none  |     0|bleu_diff  |0.0505|±  |0.0202|
|                 |       |none  |     0|bleu_max   |0.5461|±  |0.0323|
|                 |       |none  |     0|rougeL_max |2.6550|±  |0.1139|
| - truthfulqa_gen|      3|none  |     0|bleu_max   |0.5461|±  |0.0323|
|                 |       |none  |     0|bleu_acc   |0.2815|±  |0.0157|
|                 |       |none  |     0|bleu_diff  |0.0505|±  |0.0202|
|                 |       |none  |     0|rouge1_max |2.8330|±  |0.1224|
|                 |       |none  |     0|rouge1_acc |0.3647|±  |0.0169|
|                 |       |none  |     0|rouge1_diff|0.0591|±  |0.0847|
|                 |       |none  |     0|rouge2_max |0.7904|±  |0.0698|
|                 |       |none  |     0|rouge2_acc |0.1603|±  |0.0128|
|                 |       |none  |     0|rouge2_diff|0.1311|±  |0.0549|
|                 |       |none  |     0|rougeL_max |2.6550|±  |0.1139|
|                 |       |none  |     0|rougeL_acc |0.3464|±  |0.0167|
|                 |       |none  |     0|rougeL_diff|0.0734|±  |0.0772|
| - truthfulqa_mc1|      2|none  |     0|acc        |0.2754|±  |0.0156|
| - truthfulqa_mc2|      2|none  |     0|acc        |0.4978|±  |0.0165|

|  Groups  |Version|Filter|n-shot|  Metric   |Value |   |Stderr|
|----------|-------|------|-----:|-----------|-----:|---|-----:|
|truthfulqa|N/A    |none  |     0|rouge1_acc |0.3647|±  |0.0169|
|          |       |none  |     0|rouge2_diff|0.1311|±  |0.0549|
|          |       |none  |     0|rouge2_max |0.7904|±  |0.0698|
|          |       |none  |     0|rougeL_diff|0.0734|±  |0.0772|
|          |       |none  |     0|rouge1_diff|0.0591|±  |0.0847|
|          |       |none  |     0|rouge1_max |2.8330|±  |0.1224|
|          |       |none  |     0|rouge2_acc |0.1603|±  |0.0128|
|          |       |none  |     0|rougeL_acc |0.3464|±  |0.0167|
|          |       |none  |     0|bleu_acc   |0.2815|±  |0.0157|
|          |       |none  |     0|acc        |0.3866|±  |0.0114|
|          |       |none  |     0|bleu_diff  |0.0505|±  |0.0202|
|          |       |none  |     0|bleu_max   |0.5461|±  |0.0323|
|          |       |none  |     0|rougeL_max |2.6550|±  |0.1139|

|  Tasks   |Version|Filter|n-shot|Metric|Value |   |Stderr|
|----------|------:|------|-----:|------|-----:|---|-----:|
|winogrande|      1|none  |     5|acc   |0.5107|±  | 0.014|

|                 Tasks                 |Version|Filter|n-shot|Metric|Value |   |Stderr|
|---------------------------------------|-------|------|-----:|------|-----:|---|-----:|
|mmlu                                   |N/A    |none  |     0|acc   |0.2553|±  |0.0037|
|  - abstract_algebra                   |      0|none  |     5|acc   |0.3000|±  |0.0461|
|  - anatomy                            |      0|none  |     5|acc   |0.2519|±  |0.0375|
|  - astronomy                          |      0|none  |     5|acc   |0.1776|±  |0.0311|
|  - business_ethics                    |      0|none  |     5|acc   |0.2700|±  |0.0446|
|  - clinical_knowledge                 |      0|none  |     5|acc   |0.2679|±  |0.0273|
|  - college_biology                    |      0|none  |     5|acc   |0.2222|±  |0.0348|
|  - college_chemistry                  |      0|none  |     5|acc   |0.2700|±  |0.0446|
|  - college_computer_science           |      0|none  |     5|acc   |0.1500|±  |0.0359|
|  - college_mathematics                |      0|none  |     5|acc   |0.2300|±  |0.0423|
|  - college_medicine                   |      0|none  |     5|acc   |0.1850|±  |0.0296|
|  - college_physics                    |      0|none  |     5|acc   |0.1667|±  |0.0371|
|  - computer_security                  |      0|none  |     5|acc   |0.2500|±  |0.0435|
|  - conceptual_physics                 |      0|none  |     5|acc   |0.3191|±  |0.0305|
|  - econometrics                       |      0|none  |     5|acc   |0.2807|±  |0.0423|
|  - electrical_engineering             |      0|none  |     5|acc   |0.2207|±  |0.0346|
|  - elementary_mathematics             |      0|none  |     5|acc   |0.2566|±  |0.0225|
|  - formal_logic                       |      0|none  |     5|acc   |0.1825|±  |0.0346|
|  - global_facts                       |      0|none  |     5|acc   |0.3100|±  |0.0465|
|  - high_school_biology                |      0|none  |     5|acc   |0.3129|±  |0.0264|
|  - high_school_chemistry              |      0|none  |     5|acc   |0.3103|±  |0.0326|
|  - high_school_computer_science       |      0|none  |     5|acc   |0.2400|±  |0.0429|
|  - high_school_european_history       |      0|none  |     5|acc   |0.2182|±  |0.0323|
|  - high_school_geography              |      0|none  |     5|acc   |0.2172|±  |0.0294|
|  - high_school_government_and_politics|      0|none  |     5|acc   |0.2332|±  |0.0305|
|  - high_school_macroeconomics         |      0|none  |     5|acc   |0.3026|±  |0.0233|
|  - high_school_mathematics            |      0|none  |     5|acc   |0.2296|±  |0.0256|
|  - high_school_microeconomics         |      0|none  |     5|acc   |0.2311|±  |0.0274|
|  - high_school_physics                |      0|none  |     5|acc   |0.1987|±  |0.0326|
|  - high_school_psychology             |      0|none  |     5|acc   |0.2367|±  |0.0182|
|  - high_school_statistics             |      0|none  |     5|acc   |0.4722|±  |0.0340|
|  - high_school_us_history             |      0|none  |     5|acc   |0.2353|±  |0.0298|
|  - high_school_world_history          |      0|none  |     5|acc   |0.2616|±  |0.0286|
|  - human_aging                        |      0|none  |     5|acc   |0.3139|±  |0.0311|
|  - human_sexuality                    |      0|none  |     5|acc   |0.2595|±  |0.0384|
| - humanities                          |N/A    |none  |     5|acc   |0.2419|±  |0.0062|
|  - international_law                  |      0|none  |     5|acc   |0.2479|±  |0.0394|
|  - jurisprudence                      |      0|none  |     5|acc   |0.2130|±  |0.0396|
|  - logical_fallacies                  |      0|none  |     5|acc   |0.2454|±  |0.0338|
|  - machine_learning                   |      0|none  |     5|acc   |0.2054|±  |0.0383|
|  - management                         |      0|none  |     5|acc   |0.1748|±  |0.0376|
|  - marketing                          |      0|none  |     5|acc   |0.2265|±  |0.0274|
|  - medical_genetics                   |      0|none  |     5|acc   |0.3000|±  |0.0461|
|  - miscellaneous                      |      0|none  |     5|acc   |0.2682|±  |0.0158|
|  - moral_disputes                     |      0|none  |     5|acc   |0.2312|±  |0.0227|
|  - moral_scenarios                    |      0|none  |     5|acc   |0.2425|±  |0.0143|
|  - nutrition                          |      0|none  |     5|acc   |0.2255|±  |0.0239|
| - other                               |N/A    |none  |     5|acc   |0.2771|±  |0.0080|
|  - philosophy                         |      0|none  |     5|acc   |0.2862|±  |0.0257|
|  - prehistory                         |      0|none  |     5|acc   |0.2747|±  |0.0248|
|  - professional_accounting            |      0|none  |     5|acc   |0.2553|±  |0.0260|
|  - professional_law                   |      0|none  |     5|acc   |0.2379|±  |0.0109|
|  - professional_medicine              |      0|none  |     5|acc   |0.4412|±  |0.0302|
|  - professional_psychology            |      0|none  |     5|acc   |0.2500|±  |0.0175|
|  - public_relations                   |      0|none  |     5|acc   |0.2364|±  |0.0407|
|  - security_studies                   |      0|none  |     5|acc   |0.1714|±  |0.0241|
| - social_sciences                     |N/A    |none  |     5|acc   |0.2441|±  |0.0077|
|  - sociology                          |      0|none  |     5|acc   |0.2388|±  |0.0301|
| - stem                                |N/A    |none  |     5|acc   |0.2648|±  |0.0078|
|  - us_foreign_policy                  |      0|none  |     5|acc   |0.2600|±  |0.0441|
|  - virology                           |      0|none  |     5|acc   |0.3494|±  |0.0371|
|  - world_religions                    |      0|none  |     5|acc   |0.2105|±  |0.0313|

|      Groups      |Version|Filter|n-shot|Metric|Value |   |Stderr|
|------------------|-------|------|-----:|------|-----:|---|-----:|
|mmlu              |N/A    |none  |     0|acc   |0.2553|±  |0.0037|
| - humanities     |N/A    |none  |     5|acc   |0.2419|±  |0.0062|
| - other          |N/A    |none  |     5|acc   |0.2771|±  |0.0080|
| - social_sciences|N/A    |none  |     5|acc   |0.2441|±  |0.0077|
| - stem           |N/A    |none  |     5|acc   |0.2648|±  |0.0078|




|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|
|-----|------:|----------------|-----:|-----------|-----:|---|-----:|
|gsm8k|      3|strict-match    |     5|exact_match|0.0000|±  | 0.000|
|     |       |flexible-extract|     5|exact_match|0.0053|±  | 0.002|
*** DONE LoRD [LoRD-VI]
CLOSED: [2024-06-20 Thu 16:30]

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |     25 | acc      | 0.2287 | ± | 0.0123 |
|               |         | none   |     25 | acc_norm | 0.2688 | ± | 0.0130 |


| Tasks     | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|-----------+---------+--------+--------+----------+--------+---+--------|
| hellaswag |       1 | none   |     10 | acc      | 0.2659 | ± | 0.0044 |
|           |         | none   |     10 | acc_norm | 0.2774 | ± | 0.0045 |

| Tasks            | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa       |     N/A | none   |      0 | rouge1_max  |  2.8980 | ± | 0.1143 |
|                  |         | none   |      0 | rouge1_acc  |  0.3293 | ± | 0.0165 |
|                  |         | none   |      0 | acc         |  0.3837 | ± | 0.0113 |
|                  |         | none   |      0 | rougeL_diff | -0.1608 | ± | 0.0812 |
|                  |         | none   |      0 | rouge2_diff | -0.0404 | ± | 0.0580 |
|                  |         | none   |      0 | rougeL_acc  |  0.3256 | ± | 0.0164 |
|                  |         | none   |      0 | rouge2_acc  |  0.1126 | ± | 0.0111 |
|                  |         | none   |      0 | rouge1_diff | -0.1514 | ± | 0.0859 |
|                  |         | none   |      0 | rougeL_max  |  2.6508 | ± | 0.1030 |
|                  |         | none   |      0 | bleu_acc    |  0.2521 | ± | 0.0152 |
|                  |         | none   |      0 | rouge2_max  |  0.6796 | ± | 0.0590 |
|                  |         | none   |      0 | bleu_diff   |  0.0059 | ± | 0.0313 |
|                  |         | none   |      0 | bleu_max    |  0.5920 | ± | 0.0383 |
| - truthfulqa_gen |       3 | none   |      0 | bleu_max    |  0.5920 | ± | 0.0383 |
|                  |         | none   |      0 | bleu_acc    |  0.2521 | ± | 0.0152 |
|                  |         | none   |      0 | bleu_diff   |  0.0059 | ± | 0.0313 |
|                  |         | none   |      0 | rouge1_max  |  2.8980 | ± | 0.1143 |
|                  |         | none   |      0 | rouge1_acc  |  0.3293 | ± | 0.0165 |
|                  |         | none   |      0 | rouge1_diff | -0.1514 | ± | 0.0859 |
|                  |         | none   |      0 | rouge2_max  |  0.6796 | ± | 0.0590 |
|                  |         | none   |      0 | rouge2_acc  |  0.1126 | ± | 0.0111 |
|                  |         | none   |      0 | rouge2_diff | -0.0404 | ± | 0.0580 |
|                  |         | none   |      0 | rougeL_max  |  2.6508 | ± | 0.1030 |
|                  |         | none   |      0 | rougeL_acc  |  0.3256 | ± | 0.0164 |
|                  |         | none   |      0 | rougeL_diff | -0.1608 | ± | 0.0812 |
| - truthfulqa_mc1 |       2 | none   |      0 | acc         |  0.2693 | ± | 0.0155 |
| - truthfulqa_mc2 |       2 | none   |      0 | acc         |  0.4981 | ± | 0.0164 |
                                                                                                                                                                                                                                    
| Groups     | Version | Filter | n-shot | Metric      |   Value |   | Stderr |
|------------+---------+--------+--------+-------------+---------+---+--------|
| truthfulqa | N/A     | none   |      0 | rouge1_max  |  2.8980 | ± | 0.1143 |
|            |         | none   |      0 | rouge1_acc  |  0.3293 | ± | 0.0165 |
|            |         | none   |      0 | acc         |  0.3837 | ± | 0.0113 |
|            |         | none   |      0 | rougeL_diff | -0.1608 | ± | 0.0812 |
|            |         | none   |      0 | rouge2_diff | -0.0404 | ± | 0.0580 |
|            |         | none   |      0 | rougeL_acc  |  0.3256 | ± | 0.0164 |
|            |         | none   |      0 | rouge2_acc  |  0.1126 | ± | 0.0111 |
|            |         | none   |      0 | rouge1_diff | -0.1514 | ± | 0.0859 |
|            |         | none   |      0 | rougeL_max  |  2.6508 | ± | 0.1030 |
|            |         | none   |      0 | bleu_acc    |  0.2521 | ± | 0.0152 |
|            |         | none   |      0 | rouge2_max  |  0.6796 | ± | 0.0590 |
|            |         | none   |      0 | bleu_diff   |  0.0059 | ± | 0.0313 |
|            |         | none   |      0 | bleu_max    |  0.5920 | ± | 0.0383 |


| Tasks                                 | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|---------------------------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu                                  |     N/A | none   |      0 | acc    | 0.2528 | ± | 0.0036 |
| - abstract_algebra                    |       0 | none   |      5 | acc    | 0.2700 | ± | 0.0446 |
| - anatomy                             |       0 | none   |      5 | acc    | 0.2593 | ± | 0.0379 |
| - astronomy                           |       0 | none   |      5 | acc    | 0.1842 | ± | 0.0315 |
| - business_ethics                     |       0 | none   |      5 | acc    | 0.2600 | ± | 0.0441 |
| - clinical_knowledge                  |       0 | none   |      5 | acc    | 0.2679 | ± | 0.0273 |
| - college_biology                     |       0 | none   |      5 | acc    | 0.2222 | ± | 0.0348 |
| - college_chemistry                   |       0 | none   |      5 | acc    | 0.2500 | ± | 0.0435 |
| - college_computer_science            |       0 | none   |      5 | acc    | 0.1500 | ± | 0.0359 |
| - college_mathematics                 |       0 | none   |      5 | acc    | 0.2300 | ± | 0.0423 |
| - college_medicine                    |       0 | none   |      5 | acc    | 0.1965 | ± | 0.0303 |
| - college_physics                     |       0 | none   |      5 | acc    | 0.1863 | ± | 0.0387 |
| - computer_security                   |       0 | none   |      5 | acc    | 0.1800 | ± | 0.0386 |
| - conceptual_physics                  |       0 | none   |      5 | acc    | 0.3191 | ± | 0.0305 |
| - econometrics                        |       0 | none   |      5 | acc    | 0.2807 | ± | 0.0423 |
| - electrical_engineering              |       0 | none   |      5 | acc    | 0.2276 | ± | 0.0349 |
| - elementary_mathematics              |       0 | none   |      5 | acc    | 0.2566 | ± | 0.0225 |
| - formal_logic                        |       0 | none   |      5 | acc    | 0.1905 | ± | 0.0351 |
| - global_facts                        |       0 | none   |      5 | acc    | 0.2700 | ± | 0.0446 |
| - high_school_biology                 |       0 | none   |      5 | acc    | 0.3097 | ± | 0.0263 |
| - high_school_chemistry               |       0 | none   |      5 | acc    | 0.3103 | ± | 0.0326 |
| - high_school_computer_science        |       0 | none   |      5 | acc    | 0.2100 | ± | 0.0409 |
| - high_school_european_history        |       0 | none   |      5 | acc    | 0.2061 | ± | 0.0316 |
| - high_school_geography               |       0 | none   |      5 | acc    | 0.2172 | ± | 0.0294 |
| - high_school_government_and_politics |       0 | none   |      5 | acc    | 0.1917 | ± | 0.0284 |
| - high_school_macroeconomics          |       0 | none   |      5 | acc    | 0.3000 | ± | 0.0232 |
| - high_school_mathematics             |       0 | none   |      5 | acc    | 0.2259 | ± | 0.0255 |
| - high_school_microeconomics          |       0 | none   |      5 | acc    | 0.2311 | ± | 0.0274 |
| - high_school_physics                 |       0 | none   |      5 | acc    | 0.1987 | ± | 0.0326 |
| - high_school_psychology              |       0 | none   |      5 | acc    | 0.2422 | ± | 0.0184 |
| - high_school_statistics              |       0 | none   |      5 | acc    | 0.4722 | ± | 0.0340 |
| - high_school_us_history              |       0 | none   |      5 | acc    | 0.2353 | ± | 0.0298 |
| - high_school_world_history           |       0 | none   |      5 | acc    | 0.2616 | ± | 0.0286 |
| - human_aging                         |       0 | none   |      5 | acc    | 0.3677 | ± | 0.0324 |
| - human_sexuality                     |       0 | none   |      5 | acc    | 0.2061 | ± | 0.0355 |
| - humanities                          |     N/A | none   |      5 | acc    | 0.2404 | ± | 0.0062 |
| - international_law                   |       0 | none   |      5 | acc    | 0.2479 | ± | 0.0394 |
| - jurisprudence                       |       0 | none   |      5 | acc    | 0.2222 | ± | 0.0402 |
| - logical_fallacies                   |       0 | none   |      5 | acc    | 0.2454 | ± | 0.0338 |
| - machine_learning                    |       0 | none   |      5 | acc    | 0.2054 | ± | 0.0383 |
| - management                          |       0 | none   |      5 | acc    | 0.1748 | ± | 0.0376 |
| - marketing                           |       0 | none   |      5 | acc    | 0.2350 | ± | 0.0278 |
| - medical_genetics                    |       0 | none   |      5 | acc    | 0.3000 | ± | 0.0461 |
| - miscellaneous                       |       0 | none   |      5 | acc    | 0.2784 | ± | 0.0160 |
| - moral_disputes                      |       0 | none   |      5 | acc    | 0.2486 | ± | 0.0233 |
| - moral_scenarios                     |       0 | none   |      5 | acc    | 0.2436 | ± | 0.0144 |
| - nutrition                           |       0 | none   |      5 | acc    | 0.2222 | ± | 0.0238 |
| - other                               |     N/A | none   |      5 | acc    | 0.2807 | ± | 0.0080 |
| - philosophy                          |       0 | none   |      5 | acc    | 0.2765 | ± | 0.0254 |
| - prehistory                          |       0 | none   |      5 | acc    | 0.2469 | ± | 0.0240 |
| - professional_accounting             |       0 | none   |      5 | acc    | 0.2553 | ± | 0.0260 |
| - professional_law                    |       0 | none   |      5 | acc    | 0.2366 | ± | 0.0109 |
| - professional_medicine               |       0 | none   |      5 | acc    | 0.4375 | ± | 0.0301 |
| - professional_psychology             |       0 | none   |      5 | acc    | 0.2255 | ± | 0.0169 |
| - public_relations                    |       0 | none   |      5 | acc    | 0.2455 | ± | 0.0412 |
| - security_studies                    |       0 | none   |      5 | acc    | 0.1714 | ± | 0.0241 |
| - social_sciences                     |     N/A | none   |      5 | acc    | 0.2353 | ± | 0.0076 |
| - sociology                           |       0 | none   |      5 | acc    | 0.2388 | ± | 0.0301 |
| - stem                                |     N/A | none   |      5 | acc    | 0.2610 | ± | 0.0077 |
| - us_foreign_policy                   |       0 | none   |      5 | acc    | 0.2600 | ± | 0.0441 |
| - virology                            |       0 | none   |      5 | acc    | 0.3133 | ± | 0.0361 |
| - world_religions                     |       0 | none   |      5 | acc    | 0.2105 | ± | 0.0313 |
                                                                                                                                                                                                                                    
| Groups            | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|-------------------+---------+--------+--------+--------+--------+---+--------|
| mmlu              | N/A     | none   |      0 | acc    | 0.2528 | ± | 0.0036 |
| - humanities      | N/A     | none   |      5 | acc    | 0.2404 | ± | 0.0062 |
| - other           | N/A     | none   |      5 | acc    | 0.2807 | ± | 0.0080 |
| - social_sciences | N/A     | none   |      5 | acc    | 0.2353 | ± | 0.0076 |
| - stem            | N/A     | none   |      5 | acc    | 0.2610 | ± | 0.0077 |


| Tasks      | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|------------+---------+--------+--------+--------+--------+---+--------|
| winogrande |       1 | none   |      5 | acc    | 0.5138 | ± |  0.014 |


| Tasks | Version | Filter           | n-shot | Metric      |  Value |   | Stderr |
|-------+---------+------------------+--------+-------------+--------+---+--------|
| gsm8k |       3 | strict-match     |      5 | exact_match | 0.0000 | ± | 0.0000 |
|       |         | flexible-extract |      5 | exact_match | 0.0038 | ± | 0.0017 |

*** DONE LoRD-IX
CLOSED: [2024-06-23 Sun 15:42]
|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |    25|acc     |0.2218|±  |0.0121|
|             |       |none  |    25|acc_norm|0.2611|±  |0.0128|

|  Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|
|---------|------:|------|-----:|--------|-----:|---|-----:|
|hellaswag|      1|none  |    10|acc     |0.2668|±  |0.0044|
|         |       |none  |    10|acc_norm|0.2754|±  |0.0045|

|      Tasks      |Version|Filter|n-shot|  Metric   | Value |   |Stderr|
|-----------------|-------|------|-----:|-----------|------:|---|-----:|
|truthfulqa       |N/A    |none  |     0|bleu_diff  | 0.0176|±  |0.0162|
|                 |       |none  |     0|rougeL_acc | 0.2925|±  |0.0159|
|                 |       |none  |     0|rouge1_acc | 0.3097|±  |0.0162|
|                 |       |none  |     0|rouge2_diff| 0.0765|±  |0.0395|
|                 |       |none  |     0|rougeL_diff|-0.0372|±  |0.0655|
|                 |       |none  |     0|rouge2_acc | 0.1200|±  |0.0114|
|                 |       |none  |     0|bleu_acc   | 0.2362|±  |0.0149|
|                 |       |none  |     0|rouge1_max | 2.1261|±  |0.1086|
|                 |       |none  |     0|rouge2_max | 0.4939|±  |0.0428|
|                 |       |none  |     0|rouge1_diff|-0.0475|±  |0.0684|
|                 |       |none  |     0|acc        | 0.3831|±  |0.0113|
|                 |       |none  |     0|rougeL_max | 1.9603|±  |0.0837|
|                 |       |none  |     0|bleu_max   | 0.3391|±  |0.0271|
| - truthfulqa_gen|      3|none  |     0|bleu_max   | 0.3391|±  |0.0271|
|                 |       |none  |     0|bleu_acc   | 0.2362|±  |0.0149|
|                 |       |none  |     0|bleu_diff  | 0.0176|±  |0.0162|
|                 |       |none  |     0|rouge1_max | 2.1261|±  |0.1086|
|                 |       |none  |     0|rouge1_acc | 0.3097|±  |0.0162|
|                 |       |none  |     0|rouge1_diff|-0.0475|±  |0.0684|
|                 |       |none  |     0|rouge2_max | 0.4939|±  |0.0428|
|                 |       |none  |     0|rouge2_acc | 0.1200|±  |0.0114|
|                 |       |none  |     0|rouge2_diff| 0.0765|±  |0.0395|
|                 |       |none  |     0|rougeL_max | 1.9603|±  |0.0837|
|                 |       |none  |     0|rougeL_acc | 0.2925|±  |0.0159|
|                 |       |none  |     0|rougeL_diff|-0.0372|±  |0.0655|
| - truthfulqa_mc1|      2|none  |     0|acc        | 0.2693|±  |0.0155|
| - truthfulqa_mc2|      2|none  |     0|acc        | 0.4970|±  |0.0164|

|  Groups  |Version|Filter|n-shot|  Metric   | Value |   |Stderr|
|----------|-------|------|-----:|-----------|------:|---|-----:|
|truthfulqa|N/A    |none  |     0|bleu_diff  | 0.0176|±  |0.0162|
|          |       |none  |     0|rougeL_acc | 0.2925|±  |0.0159|
|          |       |none  |     0|rouge1_acc | 0.3097|±  |0.0162|
|          |       |none  |     0|rouge2_diff| 0.0765|±  |0.0395|
|          |       |none  |     0|rougeL_diff|-0.0372|±  |0.0655|
|          |       |none  |     0|rouge2_acc | 0.1200|±  |0.0114|
|          |       |none  |     0|bleu_acc   | 0.2362|±  |0.0149|
|          |       |none  |     0|rouge1_max | 2.1261|±  |0.1086|
|          |       |none  |     0|rouge2_max | 0.4939|±  |0.0428|
|          |       |none  |     0|rouge1_diff|-0.0475|±  |0.0684|
|          |       |none  |     0|acc        | 0.3831|±  |0.0113|
|          |       |none  |     0|rougeL_max | 1.9603|±  |0.0837|
|          |       |none  |     0|bleu_max   | 0.3391|±  |0.0271|

|                 Tasks                 |Version|Filter|n-shot|Metric|Value |   |Stderr|
|---------------------------------------|-------|------|-----:|------|-----:|---|-----:|
|mmlu                                   |N/A    |none  |     0|acc   |0.2558|±  |0.0037|
|  - abstract_algebra                   |      0|none  |     5|acc   |0.2700|±  |0.0446|
|  - anatomy                            |      0|none  |     5|acc   |0.2444|±  |0.0371|
|  - astronomy                          |      0|none  |     5|acc   |0.1776|±  |0.0311|
|  - business_ethics                    |      0|none  |     5|acc   |0.2400|±  |0.0429|
|  - clinical_knowledge                 |      0|none  |     5|acc   |0.2679|±  |0.0273|
|  - college_biology                    |      0|none  |     5|acc   |0.2222|±  |0.0348|
|  - college_chemistry                  |      0|none  |     5|acc   |0.3000|±  |0.0461|
|  - college_computer_science           |      0|none  |     5|acc   |0.1500|±  |0.0359|
|  - college_mathematics                |      0|none  |     5|acc   |0.2300|±  |0.0423|
|  - college_medicine                   |      0|none  |     5|acc   |0.2023|±  |0.0306|
|  - college_physics                    |      0|none  |     5|acc   |0.2157|±  |0.0409|
|  - computer_security                  |      0|none  |     5|acc   |0.2000|±  |0.0402|
|  - conceptual_physics                 |      0|none  |     5|acc   |0.2851|±  |0.0295|
|  - econometrics                       |      0|none  |     5|acc   |0.2719|±  |0.0419|
|  - electrical_engineering             |      0|none  |     5|acc   |0.2207|±  |0.0346|
|  - elementary_mathematics             |      0|none  |     5|acc   |0.2566|±  |0.0225|
|  - formal_logic                       |      0|none  |     5|acc   |0.1905|±  |0.0351|
|  - global_facts                       |      0|none  |     5|acc   |0.2700|±  |0.0446|
|  - high_school_biology                |      0|none  |     5|acc   |0.3161|±  |0.0265|
|  - high_school_chemistry              |      0|none  |     5|acc   |0.2906|±  |0.0319|
|  - high_school_computer_science       |      0|none  |     5|acc   |0.2300|±  |0.0423|
|  - high_school_european_history       |      0|none  |     5|acc   |0.1758|±  |0.0297|
|  - high_school_geography              |      0|none  |     5|acc   |0.2121|±  |0.0291|
|  - high_school_government_and_politics|      0|none  |     5|acc   |0.3109|±  |0.0334|
|  - high_school_macroeconomics         |      0|none  |     5|acc   |0.3231|±  |0.0237|
|  - high_school_mathematics            |      0|none  |     5|acc   |0.2333|±  |0.0258|
|  - high_school_microeconomics         |      0|none  |     5|acc   |0.2311|±  |0.0274|
|  - high_school_physics                |      0|none  |     5|acc   |0.1987|±  |0.0326|
|  - high_school_psychology             |      0|none  |     5|acc   |0.2385|±  |0.0183|
|  - high_school_statistics             |      0|none  |     5|acc   |0.4722|±  |0.0340|
|  - high_school_us_history             |      0|none  |     5|acc   |0.2353|±  |0.0298|
|  - high_school_world_history          |      0|none  |     5|acc   |0.2616|±  |0.0286|
|  - human_aging                        |      0|none  |     5|acc   |0.3677|±  |0.0324|
|  - human_sexuality                    |      0|none  |     5|acc   |0.2672|±  |0.0388|
| - humanities                          |N/A    |none  |     5|acc   |0.2410|±  |0.0062|
|  - international_law                  |      0|none  |     5|acc   |0.2479|±  |0.0394|
|  - jurisprudence                      |      0|none  |     5|acc   |0.2130|±  |0.0396|
|  - logical_fallacies                  |      0|none  |     5|acc   |0.2454|±  |0.0338|
|  - machine_learning                   |      0|none  |     5|acc   |0.1875|±  |0.0370|
|  - management                         |      0|none  |     5|acc   |0.1748|±  |0.0376|
|  - marketing                          |      0|none  |     5|acc   |0.1923|±  |0.0258|
|  - medical_genetics                   |      0|none  |     5|acc   |0.3000|±  |0.0461|
|  - miscellaneous                      |      0|none  |     5|acc   |0.2848|±  |0.0161|
|  - moral_disputes                     |      0|none  |     5|acc   |0.2543|±  |0.0234|
|  - moral_scenarios                    |      0|none  |     5|acc   |0.2436|±  |0.0144|
|  - nutrition                          |      0|none  |     5|acc   |0.2320|±  |0.0242|
| - other                               |N/A    |none  |     5|acc   |0.2816|±  |0.0080|
|  - philosophy                         |      0|none  |     5|acc   |0.2605|±  |0.0249|
|  - prehistory                         |      0|none  |     5|acc   |0.2500|±  |0.0241|
|  - professional_accounting            |      0|none  |     5|acc   |0.2553|±  |0.0260|
|  - professional_law                   |      0|none  |     5|acc   |0.2438|±  |0.0110|
|  - professional_medicine              |      0|none  |     5|acc   |0.4485|±  |0.0302|
|  - professional_psychology            |      0|none  |     5|acc   |0.2353|±  |0.0172|
|  - public_relations                   |      0|none  |     5|acc   |0.2091|±  |0.0390|
|  - security_studies                   |      0|none  |     5|acc   |0.1714|±  |0.0241|
| - social_sciences                     |N/A    |none  |     5|acc   |0.2476|±  |0.0078|
|  - sociology                          |      0|none  |     5|acc   |0.2388|±  |0.0301|
| - stem                                |N/A    |none  |     5|acc   |0.2604|±  |0.0077|
|  - us_foreign_policy                  |      0|none  |     5|acc   |0.2600|±  |0.0441|
|  - virology                           |      0|none  |     5|acc   |0.3313|±  |0.0366|
|  - world_religions                    |      0|none  |     5|acc   |0.2105|±  |0.0313|

|      Groups      |Version|Filter|n-shot|Metric|Value |   |Stderr|
|------------------|-------|------|-----:|------|-----:|---|-----:|
|mmlu              |N/A    |none  |     0|acc   |0.2558|±  |0.0037|
| - humanities     |N/A    |none  |     5|acc   |0.2410|±  |0.0062|
| - other          |N/A    |none  |     5|acc   |0.2816|±  |0.0080|
| - social_sciences|N/A    |none  |     5|acc   |0.2476|±  |0.0078|
| - stem           |N/A    |none  |     5|acc   |0.2604|±  |0.0077|

|  Tasks   |Version|Filter|n-shot|Metric|Value |   |Stderr|
|----------|------:|------|-----:|------|-----:|---|-----:|
|winogrande|      1|none  |     5|acc   |0.5185|±  | 0.014|

|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|
|-----|------:|----------------|-----:|-----------|-----:|---|-----:|
|gsm8k|      3|strict-match    |     5|exact_match|0.0000|±  |0.0000|
|     |       |flexible-extract|     5|exact_match|0.0015|±  |0.0011|


*** DONE vanilla
CLOSED: [2024-06-20 Thu 16:24]
|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |    25|acc     |0.2201|±  |0.0121|
|             |       |none  |    25|acc_norm|0.2483|±  |0.0126|

|  Tasks  |Version|Filter|n-shot| Metric |Value |   |Stderr|
|---------|------:|------|-----:|--------|-----:|---|-----:|
|hellaswag|      1|none  |    10|acc     |0.2654|±  |0.0044|
|         |       |none  |    10|acc_norm|0.2811|±  |0.0045|

|      Tasks      |Version|Filter|n-shot|  Metric   | Value |   |Stderr|
|-----------------|-------|------|-----:|-----------|------:|---|-----:|
|truthfulqa       |N/A    |none  |     0|rouge2_max | 0.3086|±  |0.0345|
|                 |       |none  |     0|acc        | 0.3870|±  |0.0113|
|                 |       |none  |     0|rougeL_max | 1.3179|±  |0.0547|
|                 |       |none  |     0|rougeL_acc | 0.2436|±  |0.0150|
|                 |       |none  |     0|rouge1_diff|-0.0230|±  |0.0510|
|                 |       |none  |     0|rouge2_acc | 0.0747|±  |0.0092|
|                 |       |none  |     0|bleu_diff  | 0.0149|±  |0.0100|
|                 |       |none  |     0|bleu_acc   | 0.1603|±  |0.0128|
|                 |       |none  |     0|bleu_max   | 0.1622|±  |0.0118|
|                 |       |none  |     0|rougeL_diff|-0.0001|±  |0.0475|
|                 |       |none  |     0|rouge2_diff| 0.0645|±  |0.0320|
|                 |       |none  |     0|rouge1_acc | 0.2558|±  |0.0153|
|                 |       |none  |     0|rouge1_max | 1.4619|±  |0.0608|
| - truthfulqa_gen|      3|none  |     0|bleu_max   | 0.1622|±  |0.0118|
|                 |       |none  |     0|bleu_acc   | 0.1603|±  |0.0128|
|                 |       |none  |     0|bleu_diff  | 0.0149|±  |0.0100|
|                 |       |none  |     0|rouge1_max | 1.4619|±  |0.0608|
|                 |       |none  |     0|rouge1_acc | 0.2558|±  |0.0153|
|                 |       |none  |     0|rouge1_diff|-0.0230|±  |0.0510|
|                 |       |none  |     0|rouge2_max | 0.3086|±  |0.0345|
|                 |       |none  |     0|rouge2_acc | 0.0747|±  |0.0092|
|                 |       |none  |     0|rouge2_diff| 0.0645|±  |0.0320|
|                 |       |none  |     0|rougeL_max | 1.3179|±  |0.0547|
|                 |       |none  |     0|rougeL_acc | 0.2436|±  |0.0150|
|                 |       |none  |     0|rougeL_diff|-0.0001|±  |0.0475|
| - truthfulqa_mc1|      2|none  |     0|acc        | 0.2791|±  |0.0157|
| - truthfulqa_mc2|      2|none  |     0|acc        | 0.4948|±  |0.0164|

|  Groups  |Version|Filter|n-shot|  Metric   | Value |   |Stderr|
|----------|-------|------|-----:|-----------|------:|---|-----:|
|truthfulqa|N/A    |none  |     0|rouge2_max | 0.3086|±  |0.0345|
|          |       |none  |     0|acc        | 0.3870|±  |0.0113|
|          |       |none  |     0|rougeL_max | 1.3179|±  |0.0547|
|          |       |none  |     0|rougeL_acc | 0.2436|±  |0.0150|
|          |       |none  |     0|rouge1_diff|-0.0230|±  |0.0510|
|          |       |none  |     0|rouge2_acc | 0.0747|±  |0.0092|
|          |       |none  |     0|bleu_diff  | 0.0149|±  |0.0100|
|          |       |none  |     0|bleu_acc   | 0.1603|±  |0.0128|
|          |       |none  |     0|bleu_max   | 0.1622|±  |0.0118|
|          |       |none  |     0|rougeL_diff|-0.0001|±  |0.0475|
|          |       |none  |     0|rouge2_diff| 0.0645|±  |0.0320|
|          |       |none  |     0|rouge1_acc | 0.2558|±  |0.0153|
|          |       |none  |     0|rouge1_max | 1.4619|±  |0.0608|

|                 Tasks                 |Version|Filter|n-shot|Metric|Value |   |Stderr|
|---------------------------------------|-------|------|-----:|------|-----:|---|-----:|
|mmlu                                   |N/A    |none  |     0|acc   |0.2507|±  |0.0036|
|  - abstract_algebra                   |      0|none  |     5|acc   |0.2900|±  |0.0456|
|  - anatomy                            |      0|none  |     5|acc   |0.2519|±  |0.0375|
|  - astronomy                          |      0|none  |     5|acc   |0.1776|±  |0.0311|
|  - business_ethics                    |      0|none  |     5|acc   |0.1900|±  |0.0394|
|  - clinical_knowledge                 |      0|none  |     5|acc   |0.2642|±  |0.0271|
|  - college_biology                    |      0|none  |     5|acc   |0.2222|±  |0.0348|
|  - college_chemistry                  |      0|none  |     5|acc   |0.2600|±  |0.0441|
|  - college_computer_science           |      0|none  |     5|acc   |0.1600|±  |0.0368|
|  - college_mathematics                |      0|none  |     5|acc   |0.2300|±  |0.0423|
|  - college_medicine                   |      0|none  |     5|acc   |0.1908|±  |0.0300|
|  - college_physics                    |      0|none  |     5|acc   |0.2157|±  |0.0409|
|  - computer_security                  |      0|none  |     5|acc   |0.1500|±  |0.0359|
|  - conceptual_physics                 |      0|none  |     5|acc   |0.2511|±  |0.0283|
|  - econometrics                       |      0|none  |     5|acc   |0.2456|±  |0.0405|
|  - electrical_engineering             |      0|none  |     5|acc   |0.2276|±  |0.0349|
|  - elementary_mathematics             |      0|none  |     5|acc   |0.2566|±  |0.0225|
|  - formal_logic                       |      0|none  |     5|acc   |0.1984|±  |0.0357|
|  - global_facts                       |      0|none  |     5|acc   |0.2300|±  |0.0423|
|  - high_school_biology                |      0|none  |     5|acc   |0.3161|±  |0.0265|
|  - high_school_chemistry              |      0|none  |     5|acc   |0.2562|±  |0.0307|
|  - high_school_computer_science       |      0|none  |     5|acc   |0.2400|±  |0.0429|
|  - high_school_european_history       |      0|none  |     5|acc   |0.2000|±  |0.0312|
|  - high_school_geography              |      0|none  |     5|acc   |0.2172|±  |0.0294|
|  - high_school_government_and_politics|      0|none  |     5|acc   |0.2073|±  |0.0293|
|  - high_school_macroeconomics         |      0|none  |     5|acc   |0.2333|±  |0.0214|
|  - high_school_mathematics            |      0|none  |     5|acc   |0.2630|±  |0.0268|
|  - high_school_microeconomics         |      0|none  |     5|acc   |0.2353|±  |0.0276|
|  - high_school_physics                |      0|none  |     5|acc   |0.1987|±  |0.0326|
|  - high_school_psychology             |      0|none  |     5|acc   |0.2440|±  |0.0184|
|  - high_school_statistics             |      0|none  |     5|acc   |0.4722|±  |0.0340|
|  - high_school_us_history             |      0|none  |     5|acc   |0.2353|±  |0.0298|
|  - high_school_world_history          |      0|none  |     5|acc   |0.2616|±  |0.0286|
|  - human_aging                        |      0|none  |     5|acc   |0.3453|±  |0.0319|
|  - human_sexuality                    |      0|none  |     5|acc   |0.2595|±  |0.0384|
| - humanities                          |N/A    |none  |     5|acc   |0.2489|±  |0.0063|
|  - international_law                  |      0|none  |     5|acc   |0.2479|±  |0.0394|
|  - jurisprudence                      |      0|none  |     5|acc   |0.2130|±  |0.0396|
|  - logical_fallacies                  |      0|none  |     5|acc   |0.2393|±  |0.0335|
|  - machine_learning                   |      0|none  |     5|acc   |0.2143|±  |0.0389|
|  - management                         |      0|none  |     5|acc   |0.1748|±  |0.0376|
|  - marketing                          |      0|none  |     5|acc   |0.2009|±  |0.0262|
|  - medical_genetics                   |      0|none  |     5|acc   |0.3000|±  |0.0461|
|  - miscellaneous                      |      0|none  |     5|acc   |0.2759|±  |0.0160|
|  - moral_disputes                     |      0|none  |     5|acc   |0.2543|±  |0.0234|
|  - moral_scenarios                    |      0|none  |     5|acc   |0.2413|±  |0.0143|
|  - nutrition                          |      0|none  |     5|acc   |0.2288|±  |0.0241|
| - other                               |N/A    |none  |     5|acc   |0.2720|±  |0.0079|
|  - philosophy                         |      0|none  |     5|acc   |0.3087|±  |0.0262|
|  - prehistory                         |      0|none  |     5|acc   |0.2716|±  |0.0247|
|  - professional_accounting            |      0|none  |     5|acc   |0.2553|±  |0.0260|
|  - professional_law                   |      0|none  |     5|acc   |0.2523|±  |0.0111|
|  - professional_medicine              |      0|none  |     5|acc   |0.4485|±  |0.0302|
|  - professional_psychology            |      0|none  |     5|acc   |0.2010|±  |0.0162|
|  - public_relations                   |      0|none  |     5|acc   |0.2182|±  |0.0396|
|  - security_studies                   |      0|none  |     5|acc   |0.1796|±  |0.0246|
| - social_sciences                     |N/A    |none  |     5|acc   |0.2246|±  |0.0075|
|  - sociology                          |      0|none  |     5|acc   |0.2388|±  |0.0301|
| - stem                                |N/A    |none  |     5|acc   |0.2582|±  |0.0077|
|  - us_foreign_policy                  |      0|none  |     5|acc   |0.2700|±  |0.0446|
|  - virology                           |      0|none  |     5|acc   |0.2892|±  |0.0353|
|  - world_religions                    |      0|none  |     5|acc   |0.2105|±  |0.0313|

|      Groups      |Version|Filter|n-shot|Metric|Value |   |Stderr|
|------------------|-------|------|-----:|------|-----:|---|-----:|
|mmlu              |N/A    |none  |     0|acc   |0.2507|±  |0.0036|
| - humanities     |N/A    |none  |     5|acc   |0.2489|±  |0.0063|
| - other          |N/A    |none  |     5|acc   |0.2720|±  |0.0079|
| - social_sciences|N/A    |none  |     5|acc   |0.2246|±  |0.0075|
| - stem           |N/A    |none  |     5|acc   |0.2582|±  |0.0077|


|  Tasks   |Version|Filter|n-shot|Metric|Value|   |Stderr|
|----------|------:|------|-----:|------|----:|---|-----:|
|winogrande|      1|none  |     5|acc   |0.498|±  |0.0141|


|Tasks|Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|
|-----|------:|----------------|-----:|-----------|-----:|---|-----:|
|gsm8k|      3|strict-match    |     5|exact_match|0.0000|±  |0.0000|
|     |       |flexible-extract|     5|exact_match|0.0091|±  |0.0026|

** TODO Longtext
** GPT-4 cleaned
*** TODO With 256 samples
*** TODO With 1000 samples
** Watermark: data-to-text
We expect in the watermark experiments:
+ P-value: LoRD> vanilla
+ Green-word fraction: LoRD < vanilla
+ Z-score: LoRD<vanilla
*** DONE Now the results: with 64 times of querys
CLOSED: [2024-06-11 Tue 09:49]
#+begin_src python

{'allenai/common_gen-----__watermark__d2t_ckpts__D2TTTallenai__common_gen@wrmk641LoRD-VI___period512_____allenai__common_gen@wrmk_d2t_infer_resjson': {'bertscore': {'f1': 0.8722925782203674,
                                                                                                                                                                     'p': 0.8345624208450317,
                                                                                                                                                                     'r': 0.9137428998947144},
                                                                                                                                                       'bleu': {'1': 0.35043698900479286,
                                                                                                                                                                '2': 0.2032080966682189,
                                                                                                                                                                '3': 0.12093166283138004,
                                                                                                                                                                '4': 0.07363683945946936},
                                                                                                                                                       'green_fraction': 0.2647058823529412,
                                                                                                                                                       'num_green_tokens': 9,
                                                                                                                                                       'num_tokens_scored': 34,
                                                                                                                                                       'p_value': 0.42151098818486243,
                                                                                                                                                       'prediction': False,
                                                                                                                                                       'rouge-l': {'f1': 0.3796643044590478,
                                                                                                                                                                   'p': 0.35598089286897266,
                                                                                                                                                                   'r': 0.4237586268588298},
                                                                                                                                                       'z_score': 0.19802950859533489},
 'allenai/common_gen-----__watermark__d2t_ckpts__D2TTTallenai__common_gen@wrmk641vanilla___finally_____allenai__common_gen@wrmk_d2t_infer_resjson': {'bertscore': {'f1': 0.8726513385772705,
                                                                                                                                                                   'p': 0.8354629278182983,
                                                                                                                                                                   'r': 0.9134514927864075},
                                                                                                                                                     'bleu': {'1': 0.3178174295319369,
                                                                                                                                                              '2': 0.17969801208376904,
                                                                                                                                                              '3': 0.10361766613745924,
                                                                                                                                                              '4': 0.06146900055516714},
                                                                                                                                                     'green_fraction': 0.38235294117647056,
                                                                                                                                                     'num_green_tokens': 13,
                                                                                                                                                     'num_tokens_scored': 34,
                                                                                                                                                     'p_value': 0.03735296665606522,
                                                                                                                                                     'prediction': False,
                                                                                                                                                     'rouge-l': {'f1': 0.35694429477240597,
                                                                                                                                                                 'p': 0.323578978766058,
                                                                                                                                                                 'r': 0.41506365007215607},
                                                                                                                                                     'z_score': 1.7822655773580138},
 'e2e_nlg-----__watermark__d2t_ckpts__D2TTTe2e_nlg@wrmk641LoRD-VI___period512_____e2e_nlg@wrmk_d2t_infer_resjson': {'bertscore': {'f1': 0.9073704481124878,
                                                                                                                                  'p': 0.8802347779273987,
                                                                                                                                  'r': 0.9364007115364075},
                                                                                                                    'bleu': {'1': 0.49008658053436366,
                                                                                                                             '2': 0.33756781232336586,
                                                                                                                             '3': 0.23743468812796212,
                                                                                                                             '4': 0.1619372156200498},
                                                                                                                    'green_fraction': 0.27906976744186046,
                                                                                                                    'num_green_tokens': 12,
                                                                                                                    'num_tokens_scored': 43,
                                                                                                                    'p_value': 0.3298869132560869,
                                                                                                                    'prediction': False,
                                                                                                                    'rouge-l': {'f1': 0.4443390372515331,
                                                                                                                                'p': 0.44010526350393564,
                                                                                                                                'r': 0.459521135438056},
                                                                                                                    'z_score': 0.4402254531628119},
 'e2e_nlg-----__watermark__d2t_ckpts__D2TTTe2e_nlg@wrmk641vanilla___finally_____e2e_nlg@wrmk_d2t_infer_resjson': {'bertscore': {'f1': 0.909862220287323,
                                                                                                                                'p': 0.8831789493560791,
                                                                                                                                'r': 0.9383627772331238},
                                                                                                                  'bleu': {'1': 0.504872797454937,
                                                                                                                           '2': 0.34240284009039,
                                                                                                                           '3': 0.23611685741884592,
                                                                                                                           '4': 0.1581794014781304},
                                                                                                                  'green_fraction': 0.3023255813953488,
                                                                                                                  'num_green_tokens': 13,
                                                                                                                  'num_tokens_scored': 43,
                                                                                                                  'p_value': 0.21406204473308316,
                                                                                                                  'prediction': False,
                                                                                                                  'rouge-l': {'f1': 0.4512897178346773,
                                                                                                                              'p': 0.4456838476446302,
                                                                                                                              'r': 0.4678684280080462},
                                                                                                                  'z_score': 0.7924058156930615}}


#+end_src
*** TODO Now the results: varying sequence length
** Watermark: Machine translation
*** TODO Now the results: varying sequence length

