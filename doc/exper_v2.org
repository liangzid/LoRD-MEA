#+title: Experiments Results V2
#+date: Mon May 13 11:29:29 2024
#+author: Zi Liang
#+email: zi1415926.liang@connect.polyu.hk
#+latex_class: elegantpaper
#+filetags: ::



* QAs

SETTINGS: P_\theta_t  [1 times] NSD

|-------------------------------------------------------------------------------+-------+-------+-------+-------|
| fname                                                                         |   acc |     p |     r |    f1 |
|-------------------------------------------------------------------------------+-------+-------+-------+-------|
| piqa-----./qa_ckpts/QAAAnewpiqa5121LoRD-VI___period512/                       | 0.766 | 0.797 | 0.765 | 0.759 |
| piqa-----./qa_ckpts/QAAAnewpiqa5121vanilla___finally/                         | 0.796 | 0.811 | 0.796 | 0.793 |
| piqa-----./qa_ckpts/QAAAnewpiqa2561LoRD-VI___period256/                       | 0.784 | 0.788 | 0.784 | 0.783 |
| piqa-----./qa_ckpts/QAAAnewpiqa2561vanilla___finally/                         | 0.776 | 0.788 | 0.776 | 0.774 |
| piqa-----./qa_ckpts/QAAAnewpiqa1281LoRD-VI___period128/                       | 0.768 | 0.791 | 0.767 | 0.763 |
| piqa-----./qa_ckpts/QAAAnewpiqa1281vanilla___finally/                         | 0.788 | 0.807 | 0.788 | 0.784 |
| piqa-----./qa_ckpts/QAAAnewpiqa641LoRD-VI___period64/                         |  0.67 | 0.756 | 0.671 | 0.641 |
| piqa-----./qa_ckpts/QAAAnewpiqa641vanilla___finally/                          | 0.728 | 0.755 | 0.727 |  0.72 |
|-------------------------------------------------------------------------------+-------+-------+-------+-------|
| truthful_qa-----./qa_ckpts/QAAAnewtruthful_qa5121LoRD-VI___period512/         | 0.324 |   0.5 | 0.162 | 0.245 |
| truthful_qa-----./qa_ckpts/QAAAnewtruthful_qa5121vanilla___finally/           | 0.472 |   0.5 | 0.236 | 0.321 |
| truthful_qa-----./qa_ckpts/QAAAnewtruthful_qa2561LoRD-VI___period256/         | 0.198 |   0.5 | 0.099 | 0.165 |
| truthful_qa-----./qa_ckpts/QAAAnewtruthful_qa2561vanilla___finally/           | 0.365 |   0.5 | 0.182 | 0.267 |
| truthful_qa-----./qa_ckpts/QAAAnewtruthful_qa1281LoRD-VI___period128/         | 0.569 |   0.5 | 0.285 | 0.363 |
| truthful_qa-----./qa_ckpts/QAAAnewtruthful_qa1281vanilla___finally/           |  0.45 |   0.5 | 0.225 | 0.311 |
| truthful_qa-----./qa_ckpts/QAAAnewtruthful_qa641LoRD-VI___period64/           |  0.25 |   0.5 | 0.125 |   0.2 |
| truthful_qa-----./qa_ckpts/QAAAnewtruthful_qa641vanilla___finally/            | 0.436 |   0.5 | 0.218 | 0.303 |
|-------------------------------------------------------------------------------+-------+-------+-------+-------|
| allenai/ai2_arc-----./qa_ckpts/QAAAnewallenai/ai2_arc5121LoRD-VI___period512/ | 0.288 | 0.456 | 0.209 | 0.105 |
| allenai/ai2_arc-----./qa_ckpts/QAAAnewallenai/ai2_arc5121vanilla___finally/   | 0.278 | 0.238 | 0.202 | 0.102 |
| allenai/ai2_arc-----./qa_ckpts/QAAAnewallenai/ai2_arc2561LoRD-VI___period256/ | 0.281 | 0.322 | 0.205 | 0.108 |
| allenai/ai2_arc-----./qa_ckpts/QAAAnewallenai/ai2_arc2561vanilla___finally/   | 0.288 | 0.389 | 0.209 |  0.11 |
| allenai/ai2_arc-----./qa_ckpts/QAAAnewallenai/ai2_arc1281LoRD-VI___period128/ | 0.284 | 0.289 | 0.207 | 0.104 |
| allenai/ai2_arc-----./qa_ckpts/QAAAnewallenai/ai2_arc1281vanilla___finally/   | 0.278 | 0.288 | 0.202 | 0.107 |
| allenai/ai2_arc-----./qa_ckpts/QAAAnewallenai/ai2_arc641LoRD-VI___period64/   | 0.284 | 0.156 | 0.206 |   0.1 |
| allenai/ai2_arc-----./qa_ckpts/QAAAnewallenai/ai2_arc641vanilla___finally/    | 0.278 | 0.254 | 0.202 | 0.107 |
|-------------------------------------------------------------------------------+-------+-------+-------+-------|


SETTINGS: DELTA [5 times] hard to say

|-----------------------------------------+-------------+-------------+-------------+-------------|
| fname                                   |         acc |           p |           r |          f1 |
|-----------------------------------------+-------------+-------------+-------------+-------------|
| piqa--64LoRD-VI___period512             | 0.769-0.011 | 0.779-0.014 | 0.769-0.011 | 0.767-0.011 |
| piqa--64vanilla___finally               |  0.76-0.021 | 0.771-0.007 |  0.76-0.021 | 0.757-0.025 |
| piqa--128LoRD-VI___period512            | 0.749-0.032 | 0.764-0.014 | 0.748-0.032 | 0.744-0.039 |
| piqa--128vanilla___finally              | 0.774-0.022 |  0.784-0.02 | 0.774-0.022 | 0.771-0.023 |
| piqa--256LoRD-VI___period512            | 0.789-0.021 | 0.791-0.022 | 0.789-0.021 | 0.788-0.021 |
| piqa--256vanilla___finally              | 0.782-0.005 | 0.788-0.004 | 0.782-0.006 | 0.781-0.007 |
| piqa--512LoRD-VI___period512            | 0.772-0.044 | 0.799-0.014 | 0.771-0.045 | 0.765-0.054 |
| piqa--512vanilla___finally              | 0.788-0.023 | 0.798-0.017 | 0.787-0.023 | 0.785-0.024 |
|-----------------------------------------+-------------+-------------+-------------+-------------|
| truthful_qa--64LoRD-VI___period512      | 0.451-0.055 |     0.5-0.0 | 0.226-0.027 |  0.31-0.026 |
| truthful_qa--64vanilla___finally        | 0.381-0.173 |     0.5-0.0 |  0.19-0.087 | 0.266-0.093 |
| truthful_qa--128LoRD-VI___period512     | 0.432-0.052 |     0.5-0.0 | 0.216-0.026 | 0.301-0.026 |
| truthful_qa--128vanilla___finally       | 0.454-0.069 |     0.5-0.0 | 0.227-0.034 | 0.311-0.034 |
| truthful_qa--256LoRD-VI___period512     | 0.363-0.094 |     0.5-0.0 | 0.181-0.047 | 0.263-0.054 |
| truthful_qa--256vanilla___finally       | 0.446-0.053 |     0.5-0.0 | 0.223-0.026 | 0.308-0.026 |
| truthful_qa--512LoRD-VI___period512     | 0.357-0.132 |     0.5-0.0 | 0.179-0.066 | 0.257-0.076 |
| truthful_qa--512vanilla___finally       | 0.426-0.037 |     0.5-0.0 | 0.213-0.018 | 0.299-0.018 |
|-----------------------------------------+-------------+-------------+-------------+-------------|
| allenai/ai2_arc--64LoRD-VI___period512  | 0.288-0.003 |  0.39-0.116 |  0.21-0.002 | 0.106-0.005 |
| allenai/ai2_arc--64vanilla___finally    | 0.286-0.003 | 0.389-0.041 | 0.208-0.002 | 0.107-0.002 |
| allenai/ai2_arc--128LoRD-VI___period512 | 0.281-0.003 | 0.205-0.079 | 0.204-0.002 | 0.101-0.003 |
| allenai/ai2_arc--128vanilla___finally   | 0.282-0.004 | 0.305-0.056 | 0.204-0.003 | 0.103-0.005 |
| allenai/ai2_arc--256LoRD-VI___period512 |  0.28-0.006 | 0.285-0.103 | 0.204-0.005 | 0.106-0.003 |
| allenai/ai2_arc--256vanilla___finally   | 0.284-0.004 | 0.279-0.062 | 0.207-0.003 | 0.105-0.006 |
| allenai/ai2_arc--512LoRD-VI___period512 | 0.285-0.001 | 0.316-0.121 | 0.207-0.001 | 0.102-0.003 |
| allenai/ai2_arc--512vanilla___finally   | 0.283-0.003 | 0.242-0.076 | 0.206-0.002 | 0.103-0.006 |
|-----------------------------------------+-------------+-------------+-------------+-------------|


SETTINGS: DELTA (DONOT USE 512 STEP'S CHECKPOINTS) [5 times]

|-----------------------------------------------------------------------------+-------------+-------------+-------------+-------------|
| fname                                                                       |         acc |           p |           r |          f1 |
|-----------------------------------------------------------------------------+-------------+-------------+-------------+-------------|
| piqa--__qa_ckpts__QAAAnewpiqa645LoRD-VI___period64                          | 0.705-0.072 |  0.76-0.027 | 0.704-0.073 | 0.683-0.096 |
| piqa--__qa_ckpts__QAAAnewpiqa645vanilla___finally                           |  0.76-0.021 | 0.771-0.007 |  0.76-0.021 | 0.757-0.025 |
| piqa--__qa_ckpts__QAAAnewpiqa1285LoRD-VI___period128                        | 0.753-0.038 | 0.775-0.012 | 0.753-0.039 | 0.747-0.048 |
| piqa--__qa_ckpts__QAAAnewpiqa1285vanilla___finally                          | 0.774-0.022 |  0.784-0.02 | 0.774-0.022 | 0.771-0.023 |
| piqa--__qa_ckpts__QAAAnewpiqa2565LoRD-VI___period256                        | 0.785-0.014 | 0.792-0.012 | 0.785-0.014 | 0.783-0.014 |
| piqa--__qa_ckpts__QAAAnewpiqa2565vanilla___finally                          | 0.782-0.005 | 0.788-0.004 | 0.782-0.006 | 0.781-0.007 |
| piqa--__qa_ckpts__QAAAnewpiqa5125LoRD-VI___period512                        | 0.772-0.044 | 0.799-0.014 | 0.771-0.045 | 0.765-0.054 |
| piqa--__qa_ckpts__QAAAnewpiqa5125vanilla___finally                          | 0.788-0.023 | 0.798-0.017 | 0.787-0.023 | 0.785-0.024 |
|-----------------------------------------------------------------------------+-------------+-------------+-------------+-------------|
| truthful_qa--__qa_ckpts__QAAAnewtruthful_qa645LoRD-VI___period64            | 0.345-0.075 |     0.5-0.0 | 0.173-0.037 | 0.255-0.042 |
| truthful_qa--__qa_ckpts__QAAAnewtruthful_qa645vanilla___finally             | 0.381-0.173 |     0.5-0.0 |  0.19-0.087 | 0.266-0.093 |
| truthful_qa--__qa_ckpts__QAAAnewtruthful_qa1285LoRD-VI___period128          | 0.397-0.231 |     0.5-0.0 | 0.199-0.115 |  0.264-0.15 |
| truthful_qa--__qa_ckpts__QAAAnewtruthful_qa1285vanilla___finally            | 0.454-0.069 |     0.5-0.0 | 0.227-0.034 | 0.311-0.034 |
| truthful_qa--__qa_ckpts__QAAAnewtruthful_qa2565LoRD-VI___period256          | 0.292-0.204 |     0.5-0.0 | 0.146-0.102 | 0.209-0.131 |
| truthful_qa--__qa_ckpts__QAAAnewtruthful_qa2565vanilla___finally            | 0.446-0.053 |     0.5-0.0 | 0.223-0.026 | 0.308-0.026 |
| truthful_qa--__qa_ckpts__QAAAnewtruthful_qa5125LoRD-VI___period512          | 0.357-0.132 |     0.5-0.0 | 0.179-0.066 | 0.257-0.076 |
| truthful_qa--__qa_ckpts__QAAAnewtruthful_qa5125vanilla___finally            | 0.426-0.037 |     0.5-0.0 | 0.213-0.018 | 0.299-0.018 |
|-----------------------------------------------------------------------------+-------------+-------------+-------------+-------------|
| allenai/ai2_arc--__qa_ckpts__QAAAnewallenai__ai2_arc645LoRD-VI___period64   | 0.286-0.003 | 0.379-0.085 | 0.208-0.002 | 0.105-0.001 |
| allenai/ai2_arc--__qa_ckpts__QAAAnewallenai__ai2_arc645vanilla___finally    | 0.286-0.003 | 0.389-0.041 | 0.208-0.002 | 0.107-0.002 |
| allenai/ai2_arc--__qa_ckpts__QAAAnewallenai__ai2_arc1285LoRD-VI___period128 |  0.28-0.004 |  0.29-0.065 | 0.203-0.003 | 0.104-0.001 |
| allenai/ai2_arc--__qa_ckpts__QAAAnewallenai__ai2_arc1285vanilla___finally   | 0.282-0.004 | 0.305-0.056 | 0.204-0.003 | 0.103-0.005 |
| allenai/ai2_arc--__qa_ckpts__QAAAnewallenai__ai2_arc2565LoRD-VI___period256 | 0.278-0.004 |  0.244-0.05 | 0.203-0.003 | 0.106-0.003 |
| allenai/ai2_arc--__qa_ckpts__QAAAnewallenai__ai2_arc2565vanilla___finally   | 0.284-0.004 | 0.279-0.062 | 0.207-0.003 | 0.105-0.006 |
| allenai/ai2_arc--__qa_ckpts__QAAAnewallenai__ai2_arc5125LoRD-VI___period512 | 0.285-0.001 | 0.316-0.121 | 0.207-0.001 | 0.102-0.003 |
| allenai/ai2_arc--__qa_ckpts__QAAAnewallenai__ai2_arc5125vanilla___finally   | 0.283-0.003 | 0.242-0.076 | 0.206-0.002 | 0.103-0.006 |
|-----------------------------------------------------------------------------+-------------+-------------+-------------+-------------|



SETTINGS: P_\theta_t, FOR hyper-parameter SEARCH [5 times]


|---------------------------------------+-------------+-------------+-------------+-------------+------+------|
| fname                                 |         acc |           p |           r |          f1 | tau1 | tau2 |
|---------------------------------------+-------------+-------------+-------------+-------------+------+------|
| piqaTAU1070piqa645LoRD-VI___period512 | 0.752-0.027 | 0.766-0.014 | 0.752-0.028 | 0.748-0.032 | 0.70 |  1.0 |
| piqaTAU1075piqa645LoRD-VI___period512 | 0.766-0.009 |   0.77-0.01 | 0.766-0.009 | 0.765-0.009 | 0.75 |  1.0 |
| piqaTAU1080piqa645LoRD-VI___period512 |  0.767-0.01 | 0.777-0.009 |  0.767-0.01 | 0.765-0.011 | 0.80 |  1.0 |
| piqaTAU1085piqa645LoRD-VI___period512 | 0.757-0.009 | 0.766-0.006 | 0.757-0.009 |  0.755-0.01 | 0.85 |  1.0 |
| piqaTAU1090piqa645LoRD-VI___period512 |  0.75-0.009 | 0.763-0.013 | 0.749-0.009 | 0.746-0.009 | 0.90 |  1.0 |
| piqaTAU1095piqa645LoRD-VI___period512 | 0.743-0.017 |  0.77-0.013 | 0.742-0.017 | 0.736-0.021 | 0.95 |  1.0 |
| piqaTAU110piqa645LoRD-VI___period512  | 0.762-0.017 | 0.772-0.012 | 0.762-0.017 |  0.76-0.018 | 1.00 |  1.0 |
|---------------------------------------+-------------+-------------+-------------+-------------+------+------|
| piqaTAU1070piqa645LoRD-VI___period64  | 0.741-0.019 |  0.76-0.015 |  0.741-0.02 | 0.736-0.023 | 0.70 |  1.0 |
| piqaTAU1075piqa645LoRD-VI___period64  | 0.725-0.045 |  0.77-0.015 | 0.725-0.046 | 0.711-0.058 | 0.75 |  1.0 |
| piqaTAU1080piqa645LoRD-VI___period64  | 0.737-0.054 | 0.765-0.017 | 0.737-0.054 | 0.727-0.074 | 0.80 |  1.0 |
| piqaTAU1085piqa645LoRD-VI___period64  | 0.761-0.018 |  0.774-0.02 | 0.761-0.018 | 0.758-0.019 | 0.85 |  1.0 |
| piqaTAU1090piqa645LoRD-VI___period64  | 0.748-0.042 |  0.77-0.012 | 0.747-0.043 |  0.74-0.054 | 0.90 |  1.0 |
| piqaTAU1095piqa645LoRD-VI___period64  | 0.758-0.021 |  0.77-0.016 | 0.758-0.021 | 0.755-0.024 | 0.95 |  1.0 |
| piqaTAU110piqa645LoRD-VI___period64   |  0.77-0.011 |  0.772-0.01 |  0.77-0.011 |  0.77-0.011 | 1.00 |  1.0 |
|---------------------------------------+-------------+-------------+-------------+-------------+------+------|



|----------------------------------------------+-------------+-------------+-------------+-------------+------+------|
| fname                                        |         acc |           p |           r |          f1 | tau1 | tau2 |
|----------------------------------------------+-------------+-------------+-------------+-------------+------+------|
| piqaTAU1080TAU2080piqa645LoRD-VI___period512 | 0.774-0.019 | 0.785-0.024 | 0.774-0.019 | 0.772-0.018 | 0.80 | 0.80 |
| piqaTAU1080TAU2085piqa645LoRD-VI___period512 | 0.785-0.013 | 0.795-0.008 | 0.785-0.013 | 0.783-0.015 | 0.80 | 0.85 |
| piqaTAU1080TAU2090piqa645LoRD-VI___period512 | 0.776-0.014 | 0.779-0.012 | 0.776-0.014 | 0.776-0.014 | 0.80 | 0.90 |
| piqaTAU1080TAU2095piqa645LoRD-VI___period512 | 0.765-0.013 | 0.774-0.008 | 0.765-0.013 | 0.763-0.015 | 0.80 | 0.95 |
| piqaTAU1080TAU210piqa645LoRD-VI___period512  | 0.772-0.017 | 0.777-0.014 | 0.771-0.018 |  0.77-0.018 | 0.80 | 1.00 |
|----------------------------------------------+-------------+-------------+-------------+-------------+------+------|

Conclusion: BEST HYPER-PARAMETER: 0.80, 0.85


Summary of nowaday's results in QA datasets:


|---------------------------------------------------+-------------+-------------+-------------+-------------|
| fname                                             |         acc |           p |           r |          f1 |
|---------------------------------------------------+-------------+-------------+-------------+-------------|
| piqaTAU1080TAU2085piqa645LoRD-VI___period512      | 0.785-0.013 | 0.795-0.008 | 0.785-0.013 | 0.783-0.015 |
| piqa--__qa_ckpts__QAAAnewpiqa645vanilla___finally |  0.76-0.021 | 0.771-0.007 |  0.76-0.021 | 0.757-0.025 |
|---------------------------------------------------+-------------+-------------+-------------+-------------|
| truthful_qa--645LoRD-VI___period512               | 0.408-0.053 |     0.5-0.0 | 0.204-0.026 | 0.289-0.027 |
| truthful_qa--645vanilla___finally                 | 0.381-0.173 |     0.5-0.0 |  0.19-0.087 | 0.266-0.093 |
|---------------------------------------------------+-------------+-------------+-------------+-------------|
| allenai/ai2_arc--645LoRD-VI___period512           | 0.286-0.002 |  0.336-0.11 | 0.208-0.002 | 0.103-0.003 |
| allenai/ai2_arc--645vanilla___finally             | 0.286-0.003 | 0.389-0.041 | 0.208-0.002 | 0.107-0.002 |
|---------------------------------------------------+-------------+-------------+-------------+-------------|


*Varying sequence length* Experiments: *NO CHANGES* when sequence increases.

|----------------------------------------------+-------------+-------------+-------------+-------------|
| piqaTAU1080TAU2085piqa645LoRD-VI___period512 | 0.785-0.013 | 0.795-0.008 | 0.785-0.013 | 0.783-0.015 |
| piqa--1285LoRD-VI___period512                | 0.782-0.026 | 0.793-0.018 | 0.782-0.026 | 0.779-0.028 |
| piqa--2565LoRD-VI___period512                | 0.782-0.014 | 0.788-0.012 | 0.781-0.014 |  0.78-0.015 |
| piqa--5125LoRD-VI___period512                | 0.772-0.025 | 0.795-0.011 | 0.771-0.025 | 0.766-0.029 |
|----------------------------------------------+-------------+-------------+-------------+-------------|
| truthful_qa--645LoRD-VI___period512          | 0.408-0.053 |     0.5-0.0 | 0.204-0.026 | 0.289-0.027 |
| truthful_qa--1285LoRD-VI___period512         | 0.539-0.051 |     0.5-0.0 |  0.27-0.025 |  0.35-0.021 |
| truthful_qa--2565LoRD-VI___period512         | 0.379-0.118 |     0.5-0.0 | 0.189-0.059 |  0.27-0.064 |
| truthful_qa--5125LoRD-VI___period512         |  0.38-0.101 |     0.5-0.0 |   0.19-0.05 |  0.273-0.05 |
|----------------------------------------------+-------------+-------------+-------------+-------------|
| allenai/ai2_arc--645LoRD-VI___period512      | 0.286-0.002 |  0.336-0.11 | 0.208-0.002 | 0.103-0.003 |
| allenai/ai2_arc--1285LoRD-VI___period512     | 0.278-0.005 |  0.19-0.046 | 0.202-0.004 |   0.1-0.003 |
| allenai/ai2_arc--2565LoRD-VI___period512     | 0.283-0.006 |  0.31-0.095 | 0.206-0.004 |  0.11-0.003 |
| allenai/ai2_arc--5125LoRD-VI___period512     | 0.286-0.002 | 0.336-0.137 | 0.207-0.001 | 0.103-0.003 |
|----------------------------------------------+-------------+-------------+-------------+-------------|

* WMTs

SETTINGS: P_\theta_t [1 times] No significant difference

|--------------------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------|
| file name                      | bleu1 | bleu2 | bleu3 | bleu4 |  bs-p |  bs-r |  bs-f |  rl-p |  rl-r |  rl-f |
|--------------------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------|
| cs-en-5121LoRD-VI___period512/ | 0.566 | 0.432 |  0.34 | 0.271 | 0.875 | 0.944 | 0.908 | 0.541 | 0.573 | 0.554 |
| cs-en-5121vanilla___finally/   | 0.567 | 0.433 | 0.341 | 0.272 | 0.876 | 0.946 |  0.91 | 0.542 | 0.586 |  0.56 |
| cs-en-2561LoRD-VI___period256/ | 0.572 | 0.439 | 0.345 | 0.274 | 0.876 | 0.946 | 0.909 | 0.551 | 0.588 | 0.566 |
| cs-en-2561vanilla___finally/   | 0.572 | 0.439 | 0.346 | 0.277 | 0.876 | 0.946 | 0.909 |  0.55 | 0.588 | 0.565 |
| cs-en-1281LoRD-VI___period128/ | 0.562 | 0.429 | 0.337 | 0.269 | 0.875 | 0.944 | 0.908 | 0.543 | 0.579 | 0.557 |
| cs-en-1281vanilla___finally/   | 0.557 | 0.422 | 0.331 | 0.263 | 0.874 | 0.944 | 0.907 | 0.532 | 0.569 | 0.547 |
| cs-en-641LoRD-VI___period64/   | 0.552 | 0.417 | 0.327 |  0.26 | 0.874 | 0.942 | 0.907 | 0.531 | 0.565 | 0.544 |
| cs-en-641vanilla___finally/    | 0.553 | 0.418 | 0.326 | 0.258 | 0.874 | 0.943 | 0.907 | 0.527 | 0.566 | 0.543 |
|--------------------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------|
| de-en-5121LoRD-VI___period512/ | 0.611 | 0.487 | 0.397 | 0.328 | 0.886 | 0.953 | 0.918 | 0.589 | 0.628 | 0.604 |
| de-en-5121vanilla___finally/   | 0.607 | 0.482 | 0.392 | 0.324 | 0.886 | 0.954 | 0.919 | 0.585 | 0.628 | 0.603 |
| de-en-2561LoRD-VI___period256/ | 0.609 | 0.484 | 0.394 | 0.324 | 0.886 | 0.954 | 0.919 | 0.592 | 0.633 | 0.609 |
| de-en-2561vanilla___finally/   | 0.605 | 0.483 | 0.394 | 0.325 | 0.885 | 0.954 | 0.918 | 0.586 |  0.63 | 0.604 |
| de-en-1281LoRD-VI___period128/ | 0.613 | 0.489 | 0.398 | 0.328 | 0.885 | 0.953 | 0.918 | 0.591 | 0.627 | 0.606 |
| de-en-1281vanilla___finally/   | 0.602 |  0.48 | 0.392 | 0.325 | 0.886 | 0.954 | 0.919 | 0.586 | 0.636 | 0.607 |
| de-en-641LoRD-VI___period64/   | 0.609 | 0.486 | 0.397 | 0.328 | 0.885 | 0.953 | 0.918 | 0.586 | 0.627 | 0.603 |
| de-en-641vanilla___finally/    |   0.6 | 0.477 | 0.389 | 0.323 | 0.885 | 0.952 | 0.917 |  0.58 | 0.626 |   0.6 |
|--------------------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------|
| fi-en-5121LoRD-VI___period512/ | 0.521 | 0.376 | 0.283 | 0.217 | 0.875 | 0.941 | 0.907 | 0.504 | 0.516 | 0.507 |
| fi-en-5121vanilla___finally/   | 0.515 | 0.372 |  0.28 | 0.214 | 0.875 | 0.941 | 0.907 | 0.498 | 0.532 | 0.511 |
| fi-en-2561LoRD-VI___period256/ | 0.528 | 0.381 | 0.287 | 0.221 | 0.875 | 0.941 | 0.907 | 0.513 | 0.524 | 0.515 |
| fi-en-2561vanilla___finally/   | 0.515 |  0.37 | 0.276 | 0.209 | 0.875 | 0.942 | 0.907 | 0.495 |  0.53 | 0.509 |
| fi-en-1281LoRD-VI___period128/ | 0.515 | 0.369 | 0.275 | 0.208 | 0.874 |  0.94 | 0.906 | 0.504 |  0.53 | 0.514 |
| fi-en-1281vanilla___finally/   | 0.512 | 0.364 |  0.27 | 0.204 | 0.875 |  0.94 | 0.906 | 0.491 | 0.522 | 0.503 |
| fi-en-641LoRD-VI___period64/   | 0.515 | 0.368 | 0.275 | 0.208 | 0.874 | 0.939 | 0.905 | 0.497 | 0.521 | 0.506 |
| fi-en-641vanilla___finally/    | 0.507 | 0.362 |  0.27 | 0.205 | 0.874 | 0.941 | 0.906 | 0.488 | 0.523 | 0.502 |
|--------------------------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------|


SETTINGS: DELTA [5 times] WORSE

|------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| fname                        |          b1 |          b2 |          b3 |          b4 |         bsp |         bsr |         bsf |         rlp |         rlr |         rlf |
|------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| de-en645LoRD-VI___period64   | 0.604-0.003 | 0.481-0.002 | 0.391-0.002 | 0.322-0.002 | 0.884-0.001 | 0.951-0.001 | 0.916-0.001 | 0.582-0.004 | 0.606-0.014 | 0.591-0.008 |
| de-en645vanilla___finally    | 0.597-0.012 | 0.473-0.011 |  0.384-0.01 | 0.316-0.009 | 0.884-0.001 | 0.952-0.001 | 0.917-0.001 | 0.579-0.004 | 0.623-0.004 | 0.597-0.004 |
| de-en1285LoRD-VI___period128 | 0.612-0.002 |  0.49-0.002 |   0.4-0.003 | 0.331-0.003 |   0.886-0.0 |   0.954-0.0 |   0.918-0.0 | 0.589-0.001 | 0.629-0.005 | 0.606-0.003 |
| de-en1285vanilla___finally   | 0.608-0.003 | 0.485-0.004 | 0.396-0.005 | 0.328-0.005 |   0.886-0.0 |   0.954-0.0 |   0.918-0.0 | 0.586-0.004 | 0.631-0.004 | 0.605-0.004 |
| de-en2565LoRD-VI___period256 | 0.614-0.003 | 0.492-0.003 | 0.402-0.003 | 0.332-0.003 |   0.886-0.0 |   0.954-0.0 |   0.919-0.0 | 0.593-0.001 | 0.631-0.002 | 0.609-0.001 |
| de-en2565vanilla___finally   | 0.614-0.004 |  0.49-0.005 |   0.4-0.006 | 0.332-0.007 |   0.886-0.0 |   0.954-0.0 |   0.918-0.0 | 0.592-0.004 | 0.632-0.003 | 0.609-0.004 |
|------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| fi-en645LoRD-VI___period64   | 0.511-0.002 | 0.364-0.001 | 0.271-0.002 | 0.205-0.003 |   0.873-0.0 | 0.939-0.001 |   0.905-0.0 | 0.496-0.002 | 0.512-0.007 | 0.501-0.003 |
| fi-en645vanilla___finally    | 0.509-0.005 | 0.363-0.004 | 0.271-0.004 | 0.205-0.003 | 0.874-0.001 |  0.94-0.001 | 0.906-0.001 | 0.489-0.006 | 0.521-0.004 | 0.501-0.005 |
| fi-en1285LoRD-VI___period128 | 0.513-0.005 | 0.369-0.005 | 0.275-0.005 | 0.208-0.004 |   0.875-0.0 |  0.94-0.001 |   0.906-0.0 | 0.498-0.005 | 0.524-0.006 | 0.508-0.004 |
| fi-en1285vanilla___finally   | 0.514-0.005 | 0.369-0.005 | 0.275-0.006 | 0.208-0.006 | 0.875-0.001 | 0.941-0.001 | 0.906-0.001 | 0.496-0.005 | 0.529-0.004 | 0.508-0.005 |
| fi-en2565LoRD-VI___period256 | 0.519-0.003 | 0.373-0.004 | 0.279-0.004 | 0.211-0.005 |   0.875-0.0 | 0.941-0.001 |   0.907-0.0 | 0.502-0.003 |  0.522-0.01 | 0.508-0.005 |
| fi-en2565vanilla___finally   | 0.515-0.002 |  0.37-0.002 | 0.277-0.002 |  0.21-0.002 |   0.875-0.0 | 0.942-0.001 |   0.907-0.0 | 0.498-0.001 | 0.532-0.002 | 0.511-0.001 |
|------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|

Stable method now: No significant difference

|-------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| fname                         |          b1 |          b2 |          b3 |          b4 |         bsp |         bsr |         bsf |         rlp |         rlr |         rlf |
|-------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| cs-en--645LoRD-VI___period64  |  0.56-0.006 | 0.425-0.005 | 0.334-0.005 | 0.266-0.005 | 0.874-0.001 | 0.943-0.001 | 0.907-0.001 | 0.538-0.004 | 0.571-0.004 | 0.551-0.004 |
| cs-en--645LoRD-VI___period512 | 0.553-0.005 |  0.42-0.005 | 0.329-0.004 | 0.262-0.004 | 0.874-0.001 | 0.943-0.001 | 0.907-0.001 |  0.53-0.004 | 0.576-0.005 | 0.549-0.004 |
|-------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| de-en--645LoRD-VI___period64  | 0.601-0.013 | 0.477-0.012 |  0.387-0.01 | 0.319-0.009 | 0.882-0.003 | 0.949-0.002 | 0.914-0.003 | 0.582-0.006 | 0.595-0.006 | 0.584-0.003 |
| de-en--645LoRD-VI___period512 | 0.598-0.004 | 0.474-0.004 | 0.384-0.004 | 0.316-0.004 | 0.885-0.001 | 0.953-0.001 | 0.918-0.001 | 0.579-0.004 | 0.629-0.003 |   0.6-0.004 |
| de-en645vanilla___finally     | 0.597-0.012 | 0.473-0.011 |  0.384-0.01 | 0.316-0.009 | 0.884-0.001 | 0.952-0.001 | 0.917-0.001 | 0.579-0.004 | 0.623-0.004 | 0.597-0.004 |
|-------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| fi-en--645LoRD-VI___period64  |  0.51-0.005 | 0.364-0.006 | 0.271-0.006 | 0.204-0.006 | 0.873-0.001 | 0.939-0.001 | 0.905-0.001 | 0.495-0.006 | 0.516-0.009 | 0.502-0.008 |
| fi-en--645LoRD-VI___period512 | 0.504-0.008 | 0.359-0.006 | 0.268-0.006 | 0.203-0.005 | 0.874-0.001 | 0.941-0.001 | 0.906-0.001 | 0.485-0.005 | 0.522-0.004 |   0.5-0.004 |
| fi-en645vanilla___finally     | 0.509-0.005 | 0.363-0.004 | 0.271-0.004 | 0.205-0.003 | 0.874-0.001 |  0.94-0.001 | 0.906-0.001 | 0.489-0.006 | 0.521-0.004 | 0.501-0.005 |
|-------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|


|------------------------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| Hyper-para experiments                         |             |             |             |             |             |             |             |             |             |             |
|------------------------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| fname                                          | b1          | b2          | b3          | b4          | bsp         | bsr         | bsf         | rlp         | rlr         | rlf         |
|------------------------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
|------------------------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| cs-en--TAU104TAU210cs-en641LoRD-VI___period512 | 0.557-nan   | 0.423-nan   | 0.331-nan   | 0.263-nan   | 0.874-nan   | 0.944-nan   | 0.908-nan   | 0.535-nan   | 0.578-nan   | 0.552-nan   |
| cs-en--TAU105TAU210cs-en641LoRD-VI___period512 | 0.558-nan   | 0.425-nan   | 0.334-nan   | 0.266-nan   | 0.875-nan   | 0.944-nan   | 0.908-nan   | 0.537-nan   | 0.578-nan   | 0.554-nan   |
| cs-en--TAU106TAU210cs-en641LoRD-VI___period512 | 0.553-nan   | 0.42-nan    | 0.331-nan   | 0.264-nan   | 0.873-nan   | 0.943-nan   | 0.906-nan   | 0.529-nan   | 0.571-nan   | 0.546-nan   |
| cs-enTAU1070TAU210cs-en645LoRD-VI___period512  | 0.557-0.002 | 0.423-0.003 | 0.333-0.003 | 0.265-0.003 | 0.875-0.0   | 0.944-0.001 | 0.908-0.0   | 0.533-0.004 | 0.579-0.003 | 0.552-0.003 |
| cs-enTAU1075TAU210cs-en645LoRD-VI___period512  | 0.559-0.003 | 0.426-0.004 | 0.335-0.004 | 0.267-0.005 | 0.875-0.001 | 0.944-0.001 | 0.908-0.001 | 0.536-0.002 | 0.578-0.003 | 0.553-0.003 |
| cs-enTAU1080TAU210cs-en645LoRD-VI___period512  | 0.558-0.005 | 0.424-0.005 | 0.333-0.006 | 0.265-0.006 | 0.875-0.0   | 0.944-0.001 | 0.908-0.001 | 0.534-0.007 | 0.579-0.005 | 0.552-0.006 |
| cs-enTAU1085TAU210cs-en645LoRD-VI___period512  | 0.558-0.008 | 0.423-0.009 | 0.332-0.009 | 0.264-0.008 | 0.874-0.0   | 0.944-0.001 | 0.907-0.001 | 0.534-0.006 | 0.575-0.005 | 0.551-0.005 |
| cs-enTAU1090TAU210cs-en645LoRD-VI___period512  | 0.557-0.003 | 0.423-0.004 | 0.333-0.004 | 0.266-0.004 | 0.875-0.001 | 0.944-0.001 | 0.908-0.001 | 0.534-0.005 | 0.579-0.006 | 0.553-0.005 |
| cs-enTAU1095TAU210cs-en645LoRD-VI___period512  | 0.557-0.003 | 0.422-0.003 | 0.331-0.003 | 0.264-0.003 | 0.875-0.0   | 0.944-0.001 | 0.908-0.001 | 0.533-0.002 | 0.577-0.004 | 0.551-0.003 |
| cs-enTAU110TAU210cs-en645LoRD-VI___period512   | 0.557-0.003 | 0.423-0.005 | 0.332-0.005 | 0.264-0.005 | 0.875-0.001 | 0.944-0.001 | 0.908-0.001 | 0.534-0.004 | 0.576-0.005 | 0.551-0.004 |
|------------------------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| Tau1=0.75, tau2=0.8 to 0.95                    |             |             |             |             |             |             |             |             |             |             |
| TAU1075TAU2080cs-en641LoRD-VI___period512      | 0.551-nan   | 0.417-nan   | 0.327-nan   | 0.259-nan   | 0.874-nan   | 0.942-nan   | 0.906-nan   | 0.524-nan   | 0.568-nan   | 0.542-nan   |
| TAU1075TAU2085cs-en641LoRD-VI___period512      | 0.547-nan   | 0.415-nan   | 0.325-nan   | 0.258-nan   | 0.875-nan   | 0.944-nan   | 0.908-nan   | 0.526-nan   | 0.576-nan   | 0.547-nan   |
| TAU1075TAU2090cs-en641LoRD-VI___period512      | 0.556-nan   | 0.422-nan   | 0.331-nan   | 0.263-nan   | 0.875-nan   | 0.944-nan   | 0.908-nan   | 0.535-nan   | 0.58-nan    | 0.554-nan   |
| TAU1075TAU2095cs-en641LoRD-VI___period512      | 0.553-nan   | 0.42-nan    | 0.332-nan   | 0.266-nan   | 0.874-nan   | 0.943-nan   | 0.907-nan   | 0.531-nan   | 0.571-nan   | 0.547-nan   |
|------------------------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|


|-------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| fname                         |          b1 |          b2 |          b3 |          b4 |         bsp |         bsr |         bsf |         rlp |         rlr |         rlf |
|-------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
|-------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| cs-en--165LoRD-VI___period256 | 0.545-0.007 | 0.407-0.006 | 0.316-0.005 | 0.249-0.004 | 0.873-0.001 | 0.942-0.001 | 0.906-0.001 |  0.52-0.006 | 0.565-0.005 | 0.538-0.005 |
| cs-en--165vanilla___finally   | 0.535-0.009 |   0.4-0.008 | 0.311-0.007 | 0.245-0.006 | 0.865-0.005 | 0.936-0.004 | 0.899-0.004 | 0.517-0.008 |  0.55-0.015 | 0.526-0.013 |
|-------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| de-en--165LoRD-VI___period256 | 0.587-0.004 | 0.463-0.004 | 0.374-0.004 | 0.308-0.004 |   0.884-0.0 |   0.952-0.0 |   0.917-0.0 | 0.569-0.003 | 0.617-0.004 | 0.589-0.002 |
| de-en--165vanilla___finally   |  0.578-0.02 | 0.455-0.017 | 0.368-0.014 | 0.302-0.011 |  0.87-0.007 | 0.943-0.005 | 0.904-0.006 | 0.565-0.007 | 0.596-0.016 | 0.573-0.015 |
|-------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| fi-en--165LoRD-VI___period256 | 0.498-0.006 | 0.351-0.004 |  0.26-0.004 | 0.196-0.003 | 0.873-0.001 | 0.939-0.001 | 0.905-0.001 | 0.474-0.004 | 0.504-0.015 | 0.485-0.008 |
| fi-en--165vanilla___finally   | 0.444-0.034 |  0.31-0.027 | 0.229-0.021 | 0.173-0.016 | 0.849-0.012 | 0.923-0.007 |  0.884-0.01 | 0.455-0.019 | 0.466-0.026 | 0.449-0.027 |
|-------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|

* Text2SQL

|----------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| file name            |       bleu1 |       bleu2 |       bleu3 |       bleu4 |        bs-p |        bs-r |        bs-f |        rl-p |        rl-r |        rl-f |
|----------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| wikisql645pretrained | 0.202-0.002 | 0.145-0.002 | 0.109-0.001 | 0.081-0.001 |   0.825-0.0 | 0.924-0.001 |   0.871-0.0 | 0.226-0.003 | 0.664-0.004 | 0.332-0.003 |
| wikisql--645vanilla  |  0.54-0.016 |  0.375-0.02 |  0.264-0.02 | 0.188-0.018 | 0.831-0.002 | 0.929-0.002 | 0.877-0.002 | 0.562-0.015 | 0.561-0.009 | 0.558-0.012 |
| wikisql--645LoRD-VI  | 0.551-0.023 |  0.39-0.036 |   0.28-0.04 | 0.204-0.039 | 0.834-0.004 | 0.929-0.003 | 0.879-0.004 | 0.577-0.022 |  0.563-0.02 | 0.567-0.021 |
|----------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| spider645pretrained  | 0.064-0.002 | 0.021-0.001 | 0.009-0.001 |   0.005-0.0 |   0.8-0.001 | 0.826-0.001 | 0.812-0.001 |   0.1-0.003 | 0.215-0.006 | 0.127-0.004 |
| spider--645vanilla   | 0.062-0.011 | 0.013-0.005 | 0.006-0.003 | 0.002-0.002 | 0.764-0.007 | 0.818-0.004 | 0.789-0.006 | 0.127-0.016 | 0.183-0.016 | 0.143-0.016 |
| spider--645LoRD-VI   | 0.091-0.009 | 0.028-0.005 | 0.013-0.003 | 0.006-0.002 | 0.777-0.004 | 0.831-0.005 | 0.802-0.003 |  0.169-0.01 | 0.241-0.024 | 0.188-0.014 |
|----------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
#+begin_src python
{'spider-----gpt-35-turbo-1106___spider_t2s_infer_resjson': {'bertscore': {'f1': 0.8069506883621216,                                                                                                                                
                                                                           'p': 0.7777447700500488,                                                                                                                                 
                                                                           'r': 0.8416928648948669},                                                                                                                                
                                                             'bleu': {'1': 0.09456008666718256,                                                                                                                                     
                                                                      '2': 0.039253125367069416,                                                                                                                                    
                                                                      '3': 0.020650319816448832,                                                                                                                                    
                                                                      '4': 0.011418477148745212},                                                                                                                                   
                                                             'rouge-l': {'f1': 0.21856654321292776,                                                                                                                                 
                                                                         'p': 0.1714749288478877,                                                                                                                                   
                                                                         'r': 0.36394007894099367}},                                                                                                                                
 'wikisql-----gpt-35-turbo-1106___wikisql_t2s_infer_resjson': {'bertscore': {'f1': 0.9011110663414001,                                                                                                                              
                                                                             'p': 0.8691533207893372,                                                                                                                               
                                                                             'r': 0.9358997941017151},                                                                                                                              
                                                               'bleu': {'1': 0.5407538440604164,                                                                                                                                    
                                                                        '2': 0.41496747421574665,                                                                                                                                   
                                                                        '3': 0.32060209444505605,                                                                                                                                   
                                                                        '4': 0.24447062090035337},                                                                                                                                  
                                                               'rouge-l': {'f1': 0.5969118057807032,                                                                                                                                
                                                                           'p': 0.5898996103010546,                                                                                                                                 
                                                                           'r': 0.6207660902606007}}} 
#+end_src

* D2T

|------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| file name                    |       bleu1 |       bleu2 |       bleu3 |       bleu4 |        bs-p |        bs-r |        bs-f |        rl-p |        rl-r |        rl-f |
|------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| e2e-vanilla                  |  0.53-0.009 |  0.38-0.006 | 0.275-0.005 | 0.199-0.004 |   0.891-0.0 |   0.945-0.0 |   0.918-0.0 | 0.483-0.005 | 0.542-0.014 | 0.504-0.009 |
| e2e-lordvi-512               | 0.531-0.011 | 0.382-0.009 | 0.278-0.007 | 0.202-0.005 | 0.891-0.001 | 0.945-0.001 | 0.917-0.001 | 0.483-0.007 | 0.535-0.014 | 0.502-0.009 |
|------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| common_gen645vanilla         |  0.324-0.02 | 0.183-0.013 |  0.109-0.01 | 0.066-0.007 | 0.842-0.001 |   0.917-0.0 |   0.878-0.0 | 0.317-0.024 | 0.411-0.004 | 0.351-0.016 |
| common_gen645LoRD-VI         | 0.321-0.013 |  0.18-0.009 | 0.107-0.005 | 0.064-0.003 |   0.841-0.0 | 0.916-0.001 |   0.877-0.0 | 0.314-0.011 | 0.403-0.009 | 0.346-0.009 |
|------------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| e2e_nlgpretrained            | 0.311-0.001 | 0.201-0.002 | 0.135-0.002 | 0.089-0.003 | 0.861-0.001 | 0.924-0.001 | 0.891-0.001 |  0.29-0.003 | 0.494-0.004 | 0.359-0.003 |
| allenai/common_genpretrained |   0.122-0.0 | 0.065-0.001 |   0.038-0.0 |   0.023-0.0 |    0.83-0.0 |   0.897-0.0 |   0.862-0.0 | 0.146-0.001 | 0.462-0.002 |   0.216-0.0 |






|------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| file name              |       bleu1 |       bleu2 |       bleu3 |       bleu4 |        bs-p |        bs-r |        bs-f |        rl-p |        rl-r |        rl-f |
|------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| e2e_nlg--165vanilla    | 0.514-0.013 | 0.366-0.007 | 0.262-0.004 | 0.187-0.003 |   0.891-0.0 | 0.944-0.001 |   0.916-0.0 | 0.487-0.007 | 0.556-0.026 | 0.513-0.015 |
| e2e_nlg--165LoRD-VI    | 0.513-0.012 | 0.367-0.008 | 0.264-0.006 | 0.188-0.004 | 0.891-0.001 | 0.944-0.001 | 0.917-0.001 | 0.487-0.005 | 0.554-0.019 | 0.512-0.011 |
|------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| common_gen--165vanilla | 0.313-0.016 |  0.175-0.01 | 0.103-0.007 | 0.062-0.006 | 0.843-0.001 | 0.918-0.001 | 0.878-0.001 | 0.307-0.018 | 0.415-0.004 | 0.346-0.012 |
| common_gen--165LoRD-VI |  0.296-0.02 | 0.158-0.013 | 0.091-0.008 | 0.053-0.006 | 0.839-0.001 | 0.913-0.001 | 0.874-0.001 | 0.294-0.016 | 0.397-0.003 | 0.331-0.011 |
|------------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|


 | e2e_nlg--85LoRD-VI    |  0.499-0.01 | 0.353-0.007 | 0.249-0.006 | 0.174-0.005 |    0.89-0.0 | 0.944-0.001 | 0.916-0.001 |  0.47-0.011 | 0.541-0.019 | 0.497-0.013 |
 | e2e_nlg--85vanilla    | 0.508-0.007 | 0.363-0.006 | 0.261-0.005 | 0.186-0.005 | 0.887-0.003 | 0.942-0.002 | 0.913-0.002 | 0.483-0.006 | 0.554-0.011 |  0.51-0.008 |
 | common_gen--85LoRD-VI | 0.334-0.022 | 0.187-0.015 | 0.112-0.011 | 0.069-0.009 | 0.839-0.001 | 0.915-0.001 | 0.875-0.001 | 0.338-0.025 | 0.407-0.006 | 0.362-0.015 |
 | common_gen--85vanilla | 0.324-0.014 | 0.182-0.009 | 0.109-0.006 | 0.068-0.005 | 0.841-0.001 | 0.916-0.001 | 0.877-0.001 | 0.325-0.012 | 0.413-0.003 | 0.356-0.008 |


#+begin_src python
{'allenai/common_gen-----gpt-35-turbo-1106___allenai__common_gen_d2t_infer_resjson': {'bertscore': {'f1': 0.9176157116889954,                                                                                                       
                                                                                                    'p': 0.9139708280563354,                                                                                                        
                                                                                                    'r': 0.9214537739753723},                                                                                                       
                                                                                      'bleu': {'1': 0.33346198379004244,                                                                                                            
                                                                                               '2': 0.18568434552630944,                                                                                                            
                                                                                               '3': 0.11186667597622967,                                                                                                            
                                                                                               '4': 0.06907275386200885},                                                                                                           
                                                                                      'rouge-l': {'f1': 0.36159297504509147,                                                                                                        
                                                                                                  'p': 0.33680977888251995,                                                                                                         
                                                                                                  'r': 0.4077315590809321}},                                                                                                        
 'e2e_nlg-----gpt-35-turbo-1106___e2e_nlg_d2t_infer_resjson': {'bertscore': {'f1': 0.9428955912590027,                                                                                                                              
                                                                             'p': 0.939216673374176,                                                                                                                                
                                                                             'r': 0.9467652440071106},                                                                                                                              
                                                               'bleu': {'1': 0.5182123844214066,                                                                                                                                    
                                                                        '2': 0.37059388015340206,                                                                                                                                   
                                                                        '3': 0.2681585921852735,                                                                                                                                    
                                                                        '4': 0.1912288848223388},                                                                                                                                   
                                                               'rouge-l': {'f1': 0.5142239184978086,                                                                                                                                
                                                                           'p': 0.4969671905975535,                                                                                                                                 
                                                                           'r': 0.5468309022419587}}}
  
#+end_src


* Summarization

|----------------+-------------+-------------+-------------+-------------+-------------+-----------+-------------+-------------+-------------+-------------|
| file name      |       bleu1 |       bleu2 |       bleu3 |       bleu4 |        bs-p |      bs-r |        bs-f |        rl-p |        rl-r |        rl-f |
|----------------+-------------+-------------+-------------+-------------+-------------+-----------+-------------+-------------+-------------+-------------|
| e2e-lordvi-512 | 0.108-0.001 | 0.047-0.001 | 0.026-0.001 | 0.015-0.001 |   0.842-0.0 | 0.883-0.0 |   0.862-0.0 | 0.132-0.001 | 0.321-0.008 | 0.183-0.002 |
| e2e-vanilla    | 0.107-0.005 | 0.049-0.002 | 0.027-0.001 | 0.016-0.001 | 0.842-0.001 | 0.884-0.0 | 0.862-0.001 | 0.135-0.005 | 0.337-0.008 | 0.188-0.004 |
|----------------+-------------+-------------+-------------+-------------+-------------+-----------+-------------+-------------+-------------+-------------|


|-----------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| tldr165vanilla        | 0.106-0.005 | 0.048-0.002 | 0.026-0.001 | 0.016-0.001 | 0.836-0.007 | 0.884-0.002 | 0.859-0.005 | 0.143-0.005 | 0.327-0.011 | 0.189-0.004 |
| tldr165LoRD-VI512     | 0.102-0.003 | 0.045-0.001 | 0.024-0.001 |   0.014-0.0 | 0.841-0.001 | 0.883-0.001 | 0.862-0.001 | 0.128-0.003 | 0.332-0.009 |  0.18-0.002 |
|-----------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| cnn165vanilla         | 0.051-0.001 |   0.037-0.0 |   0.028-0.0 |   0.022-0.0 |   0.806-0.0 |   0.883-0.0 |   0.843-0.0 | 0.113-0.001 | 0.786-0.001 | 0.193-0.001 |
| cnn165LoRD-VI512      |   0.053-0.0 |   0.039-0.0 |   0.029-0.0 |   0.023-0.0 |   0.806-0.0 |   0.884-0.0 |   0.843-0.0 | 0.112-0.001 | 0.785-0.002 | 0.191-0.001 |
|-----------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|
| samsum--165vanilla    | 0.169-0.011 | 0.094-0.007 | 0.058-0.004 | 0.037-0.003 | 0.839-0.009 | 0.909-0.006 | 0.873-0.008 | 0.252-0.008 | 0.498-0.025 |  0.31-0.017 |
| samsum--165LoRD-VI512 | 0.184-0.007 | 0.101-0.003 |  0.06-0.002 | 0.037-0.001 | 0.849-0.001 | 0.915-0.001 | 0.881-0.001 | 0.232-0.008 | 0.497-0.015 | 0.302-0.006 |
|-----------------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------|


#+begin_src python
 {'UCL-DARK/openai-tldr-filtered-----gpt-35-turbo-1106___UCL-DARK__openai-tldr-filtered_sum_infer_resjson': {'bertscore': {'f1': 0.8717703819274902,                                                                                 
                                                                                                                          'p': 0.8595172762870789,                                                                                  
                                                                                                                          'r': 0.884503960609436},                                                                                  
                                                                                                            'bleu': {'1': 0.11956819824985666,                                                                                      
                                                                                                                     '2': 0.05080225576160951,                                                                                      
                                                                                                                     '3': 0.026870235192500813,                                                                                     
                                                                                                                     '4': 0.015799060410652297},                                                                                    
                                                                                                            'rouge-l': {'f1': 0.1847252549034446,                                                                                   
                                                                                                                        'p': 0.1345581830786843,                                                                                    
                                                                                                                        'r': 0.3091139297957122}},                                                                                  
 'cnn_dailymail-----gpt-35-turbo-1106___cnn_dailymail_sum_infer_resjson': {'bertscore': {'f1': 0.8711915016174316,                                                                                                                  
                                                                                         'p': 0.8640521168708801,                                                                                                                   
                                                                                         'r': 0.8786939382553101},                                                                                                                  
                                                                           'bleu': {'1': 0.2048864280101603,                                                                                                                        
                                                                                    '2': 0.10869980501623241,                                                                                                                       
                                                                                    '3': 0.06465712437831377,                                                                                                                       
                                                                                    '4': 0.04144589138763352},                                                                                                                      
                                                                           'rouge-l': {'f1': 0.28299002929377476,                                                                                                                   
                                                                                       'p': 0.22463030259331065,                                                                                                                    
                                                                                       'r': 0.4082815206282913}},                                                                                                                   
 'samsum-----gpt-35-turbo-1106___samsum_sum_infer_resjson': {'bertscore': {'f1': 0.8988019227981567,                                                                                                                                
                                                                           'p': 0.8810166716575623,                                                                                                                                 
                                                                           'r': 0.9175769686698914},                                                                                                                                
                                                             'bleu': {'1': 0.20793489318413022,                                                                                                                                     
                                                                      '2': 0.11487941518227282,                                                                                                                                     
                                                                      '3': 0.06985808470400273,                                                                                                                                     
                                                                      '4': 0.04485539847510228},                                                                                                                                    
                                                             'rouge-l': {'f1': 0.31637825020750776,                                                                                                                                 
                                                                         'p': 0.24230915195634306,                                                                                                                                  
                                                                         'r': 0.5056513873325041}}}   
#+end_src






* General Train
** Shorttext

|------------------------+---------+--------+--------|
| method                 | dataset |    acc | strerr |
|------------------------+---------+--------+--------|
| llama3-8B (init model) | piqa    | 0.7824 | 0.0096 |
|------------------------+---------+--------+--------|
| CE w. bug              | piqa    | 0.7949 | 0.0094 |
| CE w.o. bug            | piqa    | 0.7802 | 0.0097 |
|------------------------+---------+--------+--------|
| LoRD-VI                | piqa    | 0.7987 | 0.0094 |


Overall Results:

Vanilla: (This is the old version of the vanilla)

|-------------|------:|------|-----:|--------|-----:|---|-----:|
|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5290|±  |0.0146|
|arc_easy     |      1|none  |     0|acc     |0.8144|±  |0.0080|
|boolq        |      2|none  |     0|acc     |0.8312|±  |0.0066|
|hellaswag    |      1|none  |     0|acc     |0.5771|±  |0.0049|
|openbookqa   |      1|none  |     0|acc     |0.3380|±  |0.0212|
|piqa         |      1|none  |     0|acc     |0.7862|±  |0.0096|
|winogrande   |      1|none  |     0|acc     |0.7190|±  |0.0126|
|-------------|------:|------|-----:|--------|-----:|---|-----:|


LoRD-VI until now

|-------------|------:|------|-----:|--------|-----:|---|-----:|
|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5358|±  |0.0146|
|arc_easy     |      1|none  |     0|acc     |0.8274|±  |0.0078|
|boolq        |      2|none  |     0|acc     |0.8370|±  |0.0065|
|hellaswag    |      1|none  |     0|acc     |0.5929|±  |0.0049|
|openbookqa   |      1|none  |     0|acc     |0.3500|±  |0.0214|
|piqa         |      1|none  |     0|acc     |0.7949|±  |0.0094|
|winogrande   |      1|none  |     0|acc     |0.7301|±  |0.0125|
|-------------|------:|------|-----:|--------|-----:|---|-----:|

LoRD-VI new version with more accurate likelihood (tau1,tau2=0.4,0.5)

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5469|±  |0.0145|
|arc_easy     |      1|none  |     0|acc     |0.8295|±  |0.0077|
|boolq        |      2|none  |     0|acc     |0.8321|±  |0.0065|
|hellaswag    |      1|none  |     0|acc     |0.5966|±  |0.0049|
|openbookqa   |      1|none  |     0|acc     |0.3560|±  |0.0214|
|piqa         |      1|none  |     0|acc     |0.7943|±  |0.0094|
|winogrande   |      1|none  |     0|acc     |0.7285|±  |0.0125|

LoRD-VI with delta (tau1,tau2=-0.1,0.5)

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5461|±  |0.0145|
|arc_easy     |      1|none  |     0|acc     |0.8316|±  |0.0077|
|boolq        |      2|none  |     0|acc     |0.8376|±  |0.0065|
|hellaswag    |      1|none  |     0|acc     |0.5978|±  |0.0049|
|openbookqa   |      1|none  |     0|acc     |0.3420|±  |0.0212|
|piqa         |      1|none  |     0|acc     |0.7949|±  |0.0094|
|winogrande   |      1|none  |     0|acc     |0.7348|±  |0.0124|

LoRD-VI with delta (tau1,tau2=0.8,0.85)
|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5503|±  |0.0145|
|arc_easy     |      1|none  |     0|acc     |0.8148|±  |0.0080|
|boolq        |      2|none  |     0|acc     |0.8318|±  |0.0065|
|hellaswag    |      1|none  |     0|acc     |0.5977|±  |0.0049|
|openbookqa   |      1|none  |     0|acc     |0.3640|±  |0.0215|
|piqa         |      1|none  |     0|acc     |0.7900|±  |0.0095|
|winogrande   |      1|none  |     0|acc     |0.7214|±  |0.0126|

** SHORT-TEXT (VARY HYPER-PARAMETER)

baseline 1 Vanilla:

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5486|±  |0.0145|
|arc_easy     |      1|none  |     0|acc     |0.8228|±  |0.0078|
|boolq        |      2|none  |     0|acc     |0.8379|±  |0.0064|
|hellaswag    |      1|none  |     0|acc     |0.5998|±  |0.0049|
|openbookqa   |      1|none  |     0|acc     |0.3600|±  |0.0215|
|piqa         |      1|none  |     0|acc     |0.7927|±  |0.0095|
|winogrande   |      1|none  |     0|acc     |0.7340|±  |0.0124|

baseline 2 Pre-trained:

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5299|±  |0.0146|
|arc_easy     |      1|none  |     0|acc     |0.8152|±  |0.0080|
|boolq        |      2|none  |     0|acc     |0.8303|±  |0.0066|
|hellaswag    |      1|none  |     0|acc     |0.5775|±  |0.0049|
|openbookqa   |      1|none  |     0|acc     |0.3400|±  |0.0212|
|piqa         |      1|none  |     0|acc     |0.7867|±  |0.0096|
|winogrande   |      1|none  |     0|acc     |0.7206|±  |0.0126|


tau1=0.2

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5461 | ±  | 0.0145 |
| arc_easy      |       1 | none   |      0 | acc      | 0.8194 | ±  | 0.0079 |
| boolq         |       2 | none   |      0 | acc      | 0.8306 | ±  | 0.0066 |
| hellaswag     |       1 | none   |      0 | acc      | 0.5935 | ±  | 0.0049 |
| openbookqa    |       1 | none   |      0 | acc      | 0.3580 | ±  | 0.0215 |
| piqa          |       1 | none   |      0 | acc      | 0.7922 | ±  | 0.0095 |
| winogrande    |       1 | none   |      0 | acc      | 0.7167 | ±  | 0.0127 |


tau1=0.3

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5495 | ±  | 0.0145 |
| arc_easy      |       1 | none   |      0 | acc      | 0.8258 | ±  | 0.0078 |
| boolq         |       2 | none   |      0 | acc      | 0.8343 | ±  | 0.0065 |
| hellaswag     |       1 | none   |      0 | acc      | 0.5971 | ±  | 0.0049 |
| openbookqa    |       1 | none   |      0 | acc      | 0.3520 | ±  | 0.0214 |
| piqa          |       1 | none   |      0 | acc      | 0.7922 | ±  | 0.0095 |
| winogrande    |       1 | none   |      0 | acc      | 0.7214 | ±  | 0.0126 |

tau1=0.4

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5401 | ±  | 0.0146 |
| arc_easy      |       1 | none   |      0 | acc      | 0.8165 | ±  | 0.0079 |
| boolq         |       2 | none   |      0 | acc      | 0.8346 | ±  | 0.0065 |
| hellaswag     |       1 | none   |      0 | acc      | 0.5959 | ±  | 0.0049 |
| openbookqa    |       1 | none   |      0 | acc      | 0.3540 | ±  | 0.0214 |
| piqa          |       1 | none   |      0 | acc      | 0.7900 | ±  | 0.0095 |
| winogrande    |       1 | none   |      0 | acc      | 0.7293 | ±  | 0.0125 |


tau1=0.5

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5418 | ±  | 0.0146 |
| arc_easy      |       1 | none   |      0 | acc      | 0.8190 | ±  | 0.0079 |
| boolq         |       2 | none   |      0 | acc      | 0.8352 | ±  | 0.0065 |
| hellaswag     |       1 | none   |      0 | acc      | 0.5966 | ±  | 0.0049 |
| openbookqa    |       1 | none   |      0 | acc      | 0.3580 | ±  | 0.0215 |
| piqa          |       1 | none   |      0 | acc      | 0.7916 | ±  | 0.0095 |
| winogrande    |       1 | none   |      0 | acc      | 0.7285 | ±  | 0.0125 |


tau1=0.6

| Tasks         | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|---------------+---------+--------+--------+--------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc    | 0.5410 | ±  | 0.0146 |
| arc_easy      |       1 | none   |      0 | acc    | 0.8165 | ±  | 0.0079 |
| boolq         |       2 | none   |      0 | acc    | 0.8327 | ±  | 0.0065 |
| hellaswag     |       1 | none   |      0 | acc    | 0.5981 | ±  | 0.0049 |
| openbookqa    |       1 | none   |      0 | acc    | 0.3600 | ±  | 0.0215 |
| piqa          |       1 | none   |      0 | acc    | 0.7905 | ±  | 0.0095 |
| winogrande    |       1 | none   |      0 | acc    | 0.7238 | ±  | 0.0126 |


tau1=0.7

| Tasks         | Version | Filter | n-shot | Metric |  Value |   | Stderr |
|---------------+---------+--------+--------+--------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc    | 0.5503 | ±  | 0.0145 |
| arc_easy      |       1 | none   |      0 | acc    | 0.8157 | ±  | 0.0080 |
| boolq         |       2 | none   |      0 | acc    | 0.8358 | ±  | 0.0065 |
| hellaswag     |       1 | none   |      0 | acc    | 0.5963 | ±  | 0.0049 |
| openbookqa    |       1 | none   |      0 | acc    | 0.3540 | ±  | 0.0214 |
| piqa          |       1 | none   |      0 | acc    | 0.7889 | ±  | 0.0095 |
| winogrande    |       1 | none   |      0 | acc    | 0.7301 | ±  | 0.0125 |


tau1=0.8

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5444|±  |0.0146|
|arc_easy     |      1|none  |     0|acc     |0.8220|±  |0.0078|
|boolq        |      2|none  |     0|acc     |0.8361|±  |0.0065|
|hellaswag    |      1|none  |     0|acc     |0.5970|±  |0.0049|
|openbookqa   |      1|none  |     0|acc     |0.3620|±  |0.0215|
|piqa         |      1|none  |     0|acc     |0.7938|±  |0.0094|
|winogrande   |      1|none  |     0|acc     |0.7356|±  |0.0124|


tau1=0.9

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5512|±  |0.0145|
|arc_easy     |      1|none  |     0|acc     |0.8190|±  |0.0079|
|boolq        |      2|none  |     0|acc     |0.8355|±  |0.0065|
|hellaswag    |      1|none  |     0|acc     |0.5984|±  |0.0049|
|openbookqa   |      1|none  |     0|acc     |0.3600|±  |0.0215|
|piqa         |      1|none  |     0|acc     |0.7938|±  |0.0094|
|winogrande   |      1|none  |     0|acc     |0.7206|±  |0.0126|

tau1=1.0

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5410|±  |0.0146|
|arc_easy     |      1|none  |     0|acc     |0.8194|±  |0.0079|
|boolq        |      2|none  |     0|acc     |0.8343|±  |0.0065|
|hellaswag    |      1|none  |     0|acc     |0.5975|±  |0.0049|
|openbookqa   |      1|none  |     0|acc     |0.3540|±  |0.0214|
|piqa         |      1|none  |     0|acc     |0.7911|±  |0.0095|
|winogrande   |      1|none  |     0|acc     |0.7214|±  |0.0126|


RANGE 2: TAU1=0.8, TAU2=?

TAU2=0.8

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5478 | ±  | 0.0145 |
| arc_easy      |       1 | none   |      0 | acc      | 0.8173 | ±  | 0.0079 |
| boolq         |       2 | none   |      0 | acc      | 0.8367 | ±  | 0.0065 |
| hellaswag     |       1 | none   |      0 | acc      | 0.5971 | ±  | 0.0049 |
| openbookqa    |       1 | none   |      0 | acc      | 0.3500 | ±  | 0.0214 |
| piqa          |       1 | none   |      0 | acc      | 0.7911 | ±  | 0.0095 |
| winogrande    |       1 | none   |      0 | acc      | 0.7238 | ±  | 0.0126 |

tau2=0.85

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5435 | ±  | 0.0146 |
| arc_easy      |       1 | none   |      0 | acc      | 0.8186 | ±  | 0.0079 |
| boolq         |       2 | none   |      0 | acc      | 0.8339 | ±  | 0.0065 |
| hellaswag     |       1 | none   |      0 | acc      | 0.5940 | ±  | 0.0049 |
| openbookqa    |       1 | none   |      0 | acc      | 0.3480 | ±  | 0.0213 |
| piqa          |       1 | none   |      0 | acc      | 0.7933 | ±  | 0.0094 |
| winogrande    |       1 | none   |      0 | acc      | 0.7269 | ±  | 0.0125 |

tau2=0.9

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5461 | ±  | 0.0145 |
| arc_easy      |       1 | none   |      0 | acc      | 0.8173 | ±  | 0.0079 |
| boolq         |       2 | none   |      0 | acc      | 0.8324 | ±  | 0.0065 |
| hellaswag     |       1 | none   |      0 | acc      | 0.5959 | ±  | 0.0049 |
| openbookqa    |       1 | none   |      0 | acc      | 0.3500 | ±  | 0.0214 |
| piqa          |       1 | none   |      0 | acc      | 0.7916 | ±  | 0.0095 |
| winogrande    |       1 | none   |      0 | acc      | 0.7309 | ±  | 0.0125 |

tau2=0.95
| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5375 | ±  | 0.0146 |
| arc_easy      |       1 | none   |      0 | acc      | 0.8161 | ±  | 0.0079 |
| boolq         |       2 | none   |      0 | acc      | 0.8315 | ±  | 0.0065 |
| hellaswag     |       1 | none   |      0 | acc      | 0.5967 | ±  | 0.0049 |
| openbookqa    |       1 | none   |      0 | acc      | 0.3480 | ±  | 0.0213 |
| piqa          |       1 | none   |      0 | acc      | 0.7916 | ±  | 0.0095 |
| winogrande    |       1 | none   |      0 | acc      | 0.7230 | ±  | 0.0126 |


tau2=1.0

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5444|±  |0.0146|
|arc_easy     |      1|none  |     0|acc     |0.8220|±  |0.0078|
|boolq        |      2|none  |     0|acc     |0.8361|±  |0.0065|
|hellaswag    |      1|none  |     0|acc     |0.5970|±  |0.0049|
|openbookqa   |      1|none  |     0|acc     |0.3620|±  |0.0215|
|piqa         |      1|none  |     0|acc     |0.7938|±  |0.0094|
|winogrande   |      1|none  |     0|acc     |0.7356|±  |0.0124|


Conclusion of the above experiments:  *NO SIGNIFICANT DIFFERENCE* .

FOR Period 1000: TAU1=0.8, TAU2=0.9

TRAIN STEP 1000:
|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5196|±  |0.0146|
|arc_easy     |      1|none  |     0|acc     |0.7917|±  |0.0083|
|boolq        |      2|none  |     0|acc     |0.8248|±  |0.0066|
|hellaswag    |      1|none  |     0|acc     |0.5884|±  |0.0049|
|openbookqa   |      1|none  |     0|acc     |0.3660|±  |0.0216|
|piqa         |      1|none  |     0|acc     |0.7726|±  |0.0098|
|winogrande   |      1|none  |     0|acc     |0.7151|±  |0.0127|

TRAIN STEP 300:
|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5495|±  |0.0145|
|arc_easy     |      1|none  |     0|acc     |0.8237|±  |0.0078|
|boolq        |      2|none  |     0|acc     |0.8333|±  |0.0065|
|hellaswag    |      1|none  |     0|acc     |0.5995|±  |0.0049|
|openbookqa   |      1|none  |     0|acc     |0.3460|±  |0.0213|
|piqa         |      1|none  |     0|acc     |0.7933|±  |0.0094|
|winogrande   |      1|none  |     0|acc     |0.7356|±  |0.0124|


tau1=0.8, tau2=1.0 is acceptable.

** SHORT-TEXT: ON NEW DATASET
*** DONE pretrained
CLOSED: [2024-05-29 Wed 10:50]

| Tasks      | Version | Filter           | n-shot | Metric      |  Value |   | Stderr |
|------------+---------+------------------+--------+-------------+--------+---+--------|
| winogrande |       1 | none             |      0 | acc         | 0.7190 | ±  | 0.0126 |
| hellaswag  |       1 | none             |      0 | acc         | 0.5771 | ±  | 0.0049 |
|            |         | none             |      0 | acc_norm    | 0.7581 | ±  | 0.0043 |
| gsm8k      |       3 | strict-match     |      5 | exact_match | 0.7597 | ±  | 0.0118 |
|            |         | flexible-extract |      5 | exact_match | 0.7574 | ±  | 0.0118 |
| arc_easy   |       1 | none             |      0 | acc         | 0.8144 | ±  | 0.0080 |
|            |         | none             |      0 | acc_norm    | 0.7955 | ±  | 0.0083 |

*** DONE vanilla
CLOSED: [2024-05-29 Wed 10:50]

|    Tasks    |Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|
|-------------|------:|----------------|-----:|-----------|-----:|---|-----:|
|arc_challenge|      1|none            |     0|acc        |0.5486|±  |0.0145|
|             |       |none            |     0|acc_norm   |0.5887|±  |0.0144|
|gsm8k        |      3|strict-match    |     5|exact_match|0.7521|±  |0.0119|
|             |       |flexible-extract|     5|exact_match|0.7544|±  |0.0119|
|hellaswag    |      1|none            |     0|acc        |0.5993|±  |0.0049|
|             |       |none            |     0|acc_norm   |0.7970|±  |0.0040|
|winogrande   |      1|none            |     0|acc        |0.7332|±  |0.0124|

*** Random taking [CANCELED]

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5230|±  |0.0146|
|             |       |none  |     0|acc_norm|0.5623|±  |0.0145|
|hellaswag    |      1|none  |     0|acc     |0.5913|±  |0.0049|
|             |       |none  |     0|acc_norm|0.7878|±  |0.0041|
|winogrande   |      1|none  |     0|acc     |0.7301|±  |0.0125|

*** New loss function [CANCELED] 

loss = -1*log_clip(torch.mean(logits2_cons-logits12))\
    -1*log_clip(torch.mean(logits11-logits12))

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5316|±  |0.0146|
|             |       |none  |     0|acc_norm|0.5700|±  |0.0145|
|hellaswag    |      1|none  |     0|acc     |0.5760|±  |0.0049|
|             |       |none  |     0|acc_norm|0.7709|±  |0.0042|
|winogrande   |      1|none  |     0|acc     |0.7293|±  |0.0125|


*** With Sigmoid with the fraction <LOS2>

#+begin_src python
los2=-1*(torch.sum(logits2_cons*mask2[:,:-1])/torch.sum(mask2[:,:-1]))
loss = los2\
    -1*(torch.sum(logits11*mask11[:,:-1])/torch.sum(mask11[:,:-1]))\
    +2*(torch.sum(logits12*mask12[:,:-1])/torch.sum(mask12[:,:-1]))
if method=="LoRD-VII":
    loss=sigmoid(loss/los2)
#+end_src

500 step results:

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5375 | ±  | 0.0146 |
|               |         | none   |      0 | acc_norm | 0.5811 | ±  | 0.0144 |
| hellaswag     |       1 | none   |      0 | acc      | 0.6115 | ±  | 0.0049 |
|               |         | none   |      0 | acc_norm | 0.8258 | ±  | 0.0038 |
| winogrande    |       1 | none   |      0 | acc      | 0.7348 | ±  | 0.0124 |

1000 step results:

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5375 | ±  | 0.0146 |
|               |         | none   |      0 | acc_norm | 0.5734 | ±  | 0.0145 |
| hellaswag     |       1 | none   |      0 | acc      | 0.6255 | ±  | 0.0048 |
|               |         | none   |      0 | acc_norm | 0.8340 | ±  | 0.0037 |
| winogrande    |       1 | none   |      0 | acc      | 0.7356 | ±  | 0.0124 |

750 step results:

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5392 | ±  | 0.0146 |
|               |         | none   |      0 | acc_norm | 0.5819 | ±  | 0.0144 |
| hellaswag     |       1 | none   |      0 | acc      | 0.6151 | ±  | 0.0049 |
|               |         | none   |      0 | acc_norm | 0.8288 | ±  | 0.0038 |
| winogrande    |       1 | none   |      0 | acc      | 0.7451 | ±  | 0.0122 |

*** LoRD-VIII (similar results.): STRANGE. IT's generation is not ok.

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5333 | ±  | 0.0146 |
|               |         | none   |      0 | acc_norm | 0.5717 | ±  | 0.0145 |
| hellaswag     |       1 | none   |      0 | acc      | 0.6257 | ±  | 0.0048 |
|               |         | none   |      0 | acc_norm | 0.8345 | ±  | 0.0037 |
| winogrande    |       1 | none   |      0 | acc      | 0.7395 | ±  | 0.0123 |



| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5367 | ±  | 0.0146 |
|               |         | none   |      0 | acc_norm | 0.5768 | ±  | 0.0144 |
| hellaswag     |       1 | none   |      0 | acc      | 0.6048 | ±  | 0.0049 |
|               |         | none   |      0 | acc_norm | 0.8235 | ±  | 0.0038 |
| winogrande    |       1 | none   |      0 | acc      | 0.7403 | ±  | 0.0123 |



*** With Sigmoid with the fraction <LOS11>[canceled]

#+begin_src python
    los2=-1*(torch.sum(logits2_cons*mask2[:,:-1])/torch.sum(mask2[:,:-1]))
    loss11=-1*(torch.sum(logits11*mask11[:,:-1])/torch.sum(mask11[:,:-1]))
    loss12=+2*(torch.sum(logits12*mask12[:,:-1])/torch.sum(mask12[:,:-1]))

    loss = los2+loss11+loss12

    if method=="LoRD-VII":
        # loss=sigmoid(loss/los2)
        loss=sigmoid(loss/loss11)
    else:
        # loss=sigmoid(loss)
        loss=sigmoid(loss/loss12)
#+end_src


| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5375 | ±  | 0.0146 |
|               |         | none   |      0 | acc_norm | 0.5725 | ±  | 0.0145 |
| hellaswag     |       1 | none   |      0 | acc      | 0.6268 | ±  | 0.0048 |
|               |         | none   |      0 | acc_norm | 0.8351 | ±  | 0.0037 |
| winogrande    |       1 | none   |      0 | acc      | 0.7388 | ±  | 0.0123 |


*** With Sigmoid with the fraction <LOS12> [canceled]
#+begin_src python
    los2=-1*(torch.sum(logits2_cons*mask2[:,:-1])/torch.sum(mask2[:,:-1]))
    loss11=-1*(torch.sum(logits11*mask11[:,:-1])/torch.sum(mask11[:,:-1]))
    loss12=+2*(torch.sum(logits12*mask12[:,:-1])/torch.sum(mask12[:,:-1]))

    loss = los2+loss11+loss12

    if method=="LoRD-VII":
        # loss=sigmoid(loss/los2)
        loss=sigmoid(loss/loss11)
    else:
        # loss=sigmoid(loss)
        loss=sigmoid(loss/loss12)
#+end_src

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.4087 | ±  | 0.0144 |
|               |         | none   |      0 | acc_norm | 0.4147 | ±  | 0.0144 |
| hellaswag     |       1 | none   |      0 | acc      | 0.4933 | ±  | 0.0050 |
|               |         | none   |      0 | acc_norm | 0.6281 | ±  | 0.0048 |
| winogrande    |       1 | none   |      0 | acc      | 0.6906 | ±  | 0.0130 |



*** With Sigmoid with no fraction [CANCELED]

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5307 | ±  | 0.0146 |
|               |         | none   |      0 | acc_norm | 0.5768 | ±  | 0.0144 |
| hellaswag     |       1 | none   |      0 | acc      | 0.5976 | ±  | 0.0049 |
|               |         | none   |      0 | acc_norm | 0.8187 | ±  | 0.0038 |
| winogrande    |       1 | none   |      0 | acc      | 0.7411 | ±  | 0.0123 |


*** DONE [DEPERATED] OURS: old long result: LoRD-VI (train-num 3000, max_new_tokens=256, N=1024, tau1=0.75, tau2=0.8)
CLOSED: [2024-05-29 Wed 10:50]

| Tasks      | Version | Filter           | n-shot | Metric      |  Value |   | Stderr |
|------------+---------+------------------+--------+-------------+--------+---+--------|
| winogrande |       1 | none             |      0 | acc         | 0.7332 | ±  | 0.0124 |
| hellaswag  |       1 | none             |      0 | acc         | 0.5970 | ±  | 0.0049 |
|            |         | none             |      0 | acc_norm    | 0.7966 | ±  | 0.0040 |
| gsm8k      |       3 | strict-match     |      5 | exact_match | 0.7415 | ±  | 0.0121 |
|            |         | flexible-extract |      5 | exact_match | 0.7415 | ±  | 0.0121 |
| arc_easy   |       1 | none             |      0 | acc         | 0.8152 | ±  | 0.0080 |
|            |         | none             |      0 | acc_norm    | 0.7925 | ±  | 0.0083 |


*** INPROGRESS LoRD-VI \tau_1=0.8,\tau_2=0.9

|    Tasks    |Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|
|-------------|------:|----------------|-----:|-----------|-----:|---|-----:|
|arc_challenge|      1|none            |     0|acc        |0.5392|±  |0.0146|
|             |       |none            |     0|acc_norm   |0.5759|±  |0.0144|
|gsm8k        |      3|strict-match    |     5|exact_match|0.7392|±  |0.0121|
|             |       |flexible-extract|     5|exact_match|0.7392|±  |0.0121|
|hellaswag    |      1|none            |     0|acc        |0.5953|±  |0.0049|
|             |       |none            |     0|acc_norm   |0.7935|±  |0.0040|
|winogrande   |      1|none            |     0|acc        |0.7309|±  |0.0125|


longer sequence length: No influence.

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5427|±  |0.0146|
|             |       |none  |     0|acc_norm|0.5819|±  |0.0144|
|hellaswag    |      1|none  |     0|acc     |0.5981|±  |0.0049|
|             |       |none  |     0|acc_norm|0.7961|±  |0.0040|
|winogrande   |      1|none  |     0|acc     |0.7277|±  |0.0125|

|    Tasks    |Version|     Filter     |n-shot|  Metric   |Value |   |Stderr|
|-------------|------:|----------------|-----:|-----------|-----:|---|-----:|
|arc_challenge|      1|none            |     0|acc        |0.5384|±  |0.0146|
|             |       |none            |     0|acc_norm   |0.5819|±  |0.0144|
|gsm8k        |      3|strict-match    |     5|exact_match|0.7498|±  |0.0119|
|             |       |flexible-extract|     5|exact_match|0.7513|±  |0.0119|
|hellaswag    |      1|none            |     0|acc        |0.5874|±  |0.0049|
|             |       |none            |     0|acc_norm   |0.7876|±  |0.0041|
|winogrande   |      1|none            |     0|acc        |0.7316|±  |0.0125|

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5418|±  |0.0146|
|             |       |none  |     0|acc_norm|0.5717|±  |0.0145|
|hellaswag    |      1|none  |     0|acc     |0.5980|±  |0.0049|
|             |       |none  |     0|acc_norm|0.7962|±  |0.0040|
|winogrande   |      1|none  |     0|acc     |0.7245|±  |0.0126|

** Short text

vanilla

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.2278|±  |0.0123|
|             |       |none  |     0|acc_norm|0.2543|±  |0.0127|
|hellaswag    |      1|none  |     0|acc     |0.2852|±  |0.0045|
|             |       |none  |     0|acc_norm|0.3135|±  |0.0046|
|winogrande   |      1|none  |     0|acc     |0.5170|±  |0.0140|


** Longtext
*** Unknown old results.
|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5435|±  |0.0146|
|             |       |none  |     0|acc_norm|0.5879|±  |0.0144|
|arc_easy     |      1|none  |     0|acc     |0.8287|±  |0.0077|
|             |       |none  |     0|acc_norm|0.8106|±  |0.0080|
|boolq        |      2|none  |     0|acc     |0.8379|±  |0.0064|
|hellaswag    |      1|none  |     0|acc     |0.5961|±  |0.0049|
|             |       |none  |     0|acc_norm|0.7966|±  |0.0040|
|openbookqa   |      1|none  |     0|acc     |0.3480|±  |0.0213|
|             |       |none  |     0|acc_norm|0.4480|±  |0.0223|
|piqa         |      1|none  |     0|acc     |0.8003|±  |0.0093|
|             |       |none  |     0|acc_norm|0.7992|±  |0.0093|
|winogrande   |      1|none  |     0|acc     |0.7348|±  |0.0124|
*** Now the results

pretrained-llama3 8B
|                 Tasks                 |Version|     Filter     |n-shot|  Metric   | Value |   |Stderr|
|---------------------------------------|-------|----------------|-----:|-----------|------:|---|-----:|
|winogrande                             |      1|none            |     0|acc        | 0.7190|±  |0.0126|
|truthfulqa                             |N/A    |none            |     0|bleu_diff  |-0.3697|±  |0.6322|
|                                       |       |none            |     0|rouge1_diff|-0.3104|±  |0.8621|
|                                       |       |none            |     0|rouge1_acc | 0.4859|±  |0.0175|
|                                       |       |none            |     0|rouge2_max |27.2295|±  |0.9549|
|                                       |       |none            |     0|acc        | 0.4398|±  |0.0113|
|                                       |       |none            |     0|rouge2_acc | 0.3562|±  |0.0168|
|                                       |       |none            |     0|rougeL_max |40.3938|±  |0.8669|
|                                       |       |none            |     0|rouge1_max |43.2437|±  |0.8677|
|                                       |       |none            |     0|bleu_acc   | 0.4639|±  |0.0175|
|                                       |       |none            |     0|rougeL_acc | 0.4737|±  |0.0175|
|                                       |       |none            |     0|rougeL_diff|-0.7585|±  |0.8675|
|                                       |       |none            |     0|bleu_max   |20.1353|±  |0.7243|
|                                       |       |none            |     0|rouge2_diff|-1.7143|±  |0.9135|
| - truthfulqa_gen                      |      3|none            |     0|bleu_max   |20.1353|±  |0.7243|
|                                       |       |none            |     0|bleu_acc   | 0.4639|±  |0.0175|
|                                       |       |none            |     0|bleu_diff  |-0.3697|±  |0.6322|
|                                       |       |none            |     0|rouge1_max |43.2437|±  |0.8677|
|                                       |       |none            |     0|rouge1_acc | 0.4859|±  |0.0175|
|                                       |       |none            |     0|rouge1_diff|-0.3104|±  |0.8621|
|                                       |       |none            |     0|rouge2_max |27.2295|±  |0.9549|
|                                       |       |none            |     0|rouge2_acc | 0.3562|±  |0.0168|
|                                       |       |none            |     0|rouge2_diff|-1.7143|±  |0.9135|
|                                       |       |none            |     0|rougeL_max |40.3938|±  |0.8669|
|                                       |       |none            |     0|rougeL_acc | 0.4737|±  |0.0175|
|                                       |       |none            |     0|rougeL_diff|-0.7585|±  |0.8675|
| - truthfulqa_mc1                      |      2|none            |     0|acc        | 0.3623|±  |0.0168|
| - truthfulqa_mc2                      |      2|none            |     0|acc        | 0.5172|±  |0.0152|
|mmlu                                   |N/A    |none            |     0|acc        | 0.6390|±  |0.0038|
| - humanities                          |N/A    |none            |     0|acc        | 0.5817|±  |0.0068|
|  - formal_logic                       |      0|none            |     0|acc        | 0.4921|±  |0.0447|
|  - high_school_european_history       |      0|none            |     0|acc        | 0.7394|±  |0.0343|
|  - high_school_us_history             |      0|none            |     0|acc        | 0.8382|±  |0.0258|
|  - high_school_world_history          |      0|none            |     0|acc        | 0.8397|±  |0.0239|
|  - international_law                  |      0|none            |     0|acc        | 0.7686|±  |0.0385|
|  - jurisprudence                      |      0|none            |     0|acc        | 0.7500|±  |0.0419|
|  - logical_fallacies                  |      0|none            |     0|acc        | 0.7546|±  |0.0338|
|  - moral_disputes                     |      0|none            |     0|acc        | 0.6908|±  |0.0249|
|  - moral_scenarios                    |      0|none            |     0|acc        | 0.3352|±  |0.0158|
|  - philosophy                         |      0|none            |     0|acc        | 0.7138|±  |0.0257|
|  - prehistory                         |      0|none            |     0|acc        | 0.7315|±  |0.0247|
|  - professional_law                   |      0|none            |     0|acc        | 0.4922|±  |0.0128|
|  - world_religions                    |      0|none            |     0|acc        | 0.7778|±  |0.0319|
| - other                               |N/A    |none            |     0|acc        | 0.7187|±  |0.0078|
|  - business_ethics                    |      0|none            |     0|acc        | 0.6600|±  |0.0476|
|  - clinical_knowledge                 |      0|none            |     0|acc        | 0.7170|±  |0.0277|
|  - college_medicine                   |      0|none            |     0|acc        | 0.6532|±  |0.0363|
|  - global_facts                       |      0|none            |     0|acc        | 0.3800|±  |0.0488|
|  - human_aging                        |      0|none            |     0|acc        | 0.6906|±  |0.0310|
|  - management                         |      0|none            |     0|acc        | 0.8252|±  |0.0376|
|  - marketing                          |      0|none            |     0|acc        | 0.8889|±  |0.0206|
|  - medical_genetics                   |      0|none            |     0|acc        | 0.8200|±  |0.0386|
|  - miscellaneous                      |      0|none            |     0|acc        | 0.8097|±  |0.0140|
|  - nutrition                          |      0|none            |     0|acc        | 0.7386|±  |0.0252|
|  - professional_accounting            |      0|none            |     0|acc        | 0.5319|±  |0.0298|
|  - professional_medicine              |      0|none            |     0|acc        | 0.7426|±  |0.0266|
|  - virology                           |      0|none            |     0|acc        | 0.5120|±  |0.0389|
| - social_sciences                     |N/A    |none            |     0|acc        | 0.7416|±  |0.0077|
|  - econometrics                       |      0|none            |     0|acc        | 0.5175|±  |0.0470|
|  - high_school_geography              |      0|none            |     0|acc        | 0.7828|±  |0.0294|
|  - high_school_government_and_politics|      0|none            |     0|acc        | 0.8705|±  |0.0242|
|  - high_school_macroeconomics         |      0|none            |     0|acc        | 0.6436|±  |0.0243|
|  - high_school_microeconomics         |      0|none            |     0|acc        | 0.7227|±  |0.0291|
|  - high_school_psychology             |      0|none            |     0|acc        | 0.8257|±  |0.0163|
|  - human_sexuality                    |      0|none            |     0|acc        | 0.7710|±  |0.0369|
|  - professional_psychology            |      0|none            |     0|acc        | 0.6748|±  |0.0190|
|  - public_relations                   |      0|none            |     0|acc        | 0.6636|±  |0.0453|
|  - security_studies                   |      0|none            |     0|acc        | 0.7388|±  |0.0281|
|  - sociology                          |      0|none            |     0|acc        | 0.8607|±  |0.0245|
|  - us_foreign_policy                  |      0|none            |     0|acc        | 0.8600|±  |0.0349|
| - stem                                |N/A    |none            |     0|acc        | 0.5458|±  |0.0086|
|  - abstract_algebra                   |      0|none            |     0|acc        | 0.3500|±  |0.0479|
|  - anatomy                            |      0|none            |     0|acc        | 0.6296|±  |0.0417|
|  - astronomy                          |      0|none            |     0|acc        | 0.7039|±  |0.0372|
|  - college_biology                    |      0|none            |     0|acc        | 0.7361|±  |0.0369|
|  - college_chemistry                  |      0|none            |     0|acc        | 0.4400|±  |0.0499|
|  - college_computer_science           |      0|none            |     0|acc        | 0.5300|±  |0.0502|
|  - college_mathematics                |      0|none            |     0|acc        | 0.3500|±  |0.0479|
|  - college_physics                    |      0|none            |     0|acc        | 0.4902|±  |0.0497|
|  - computer_security                  |      0|none            |     0|acc        | 0.7300|±  |0.0446|
|  - conceptual_physics                 |      0|none            |     0|acc        | 0.5447|±  |0.0326|
|  - electrical_engineering             |      0|none            |     0|acc        | 0.6414|±  |0.0400|
|  - elementary_mathematics             |      0|none            |     0|acc        | 0.4683|±  |0.0257|
|  - high_school_biology                |      0|none            |     0|acc        | 0.7613|±  |0.0243|
|  - high_school_chemistry              |      0|none            |     0|acc        | 0.4581|±  |0.0351|
|  - high_school_computer_science       |      0|none            |     0|acc        | 0.6900|±  |0.0465|
|  - high_school_mathematics            |      0|none            |     0|acc        | 0.3963|±  |0.0298|
|  - high_school_physics                |      0|none            |     0|acc        | 0.4636|±  |0.0407|
|  - high_school_statistics             |      0|none            |     0|acc        | 0.4954|±  |0.0341|
|  - machine_learning                   |      0|none            |     0|acc        | 0.4732|±  |0.0474|
|hellaswag                              |      1|none            |     0|acc        | 0.5771|±  |0.0049|
|                                       |       |none            |     0|acc_norm   | 0.7581|±  |0.0043|
|gsm8k                                  |      3|strict-match    |     5|exact_match| 0.7597|±  |0.0118|
|                                       |       |flexible-extract|     5|exact_match| 0.7574|±  |0.0118|
|arc_easy                               |      1|none            |     0|acc        | 0.8144|±  |0.0080|
|                                       |       |none            |     0|acc_norm   | 0.7955|±  |0.0083|

|      Groups      |Version|Filter|n-shot|  Metric   | Value |   |Stderr|
|------------------|-------|------|-----:|-----------|------:|---|-----:|
|truthfulqa        |N/A    |none  |     0|bleu_diff  |-0.3697|±  |0.6322|
|                  |       |none  |     0|rouge1_diff|-0.3104|±  |0.8621|
|                  |       |none  |     0|rouge1_acc | 0.4859|±  |0.0175|
|                  |       |none  |     0|rouge2_max |27.2295|±  |0.9549|
|                  |       |none  |     0|acc        | 0.4398|±  |0.0113|
|                  |       |none  |     0|rouge2_acc | 0.3562|±  |0.0168|
|                  |       |none  |     0|rougeL_max |40.3938|±  |0.8669|
|                  |       |none  |     0|rouge1_max |43.2437|±  |0.8677|
|                  |       |none  |     0|bleu_acc   | 0.4639|±  |0.0175|
|                  |       |none  |     0|rougeL_acc | 0.4737|±  |0.0175|
|                  |       |none  |     0|rougeL_diff|-0.7585|±  |0.8675|
|                  |       |none  |     0|bleu_max   |20.1353|±  |0.7243|
|                  |       |none  |     0|rouge2_diff|-1.7143|±  |0.9135|
|mmlu              |N/A    |none  |     0|acc        | 0.6390|±  |0.0038|
| - humanities     |N/A    |none  |     0|acc        | 0.5817|±  |0.0068|
| - other          |N/A    |none  |     0|acc        | 0.7187|±  |0.0078|
| - social_sciences|N/A    |none  |     0|acc        | 0.7416|±  |0.0077|
| - stem           |N/A    |none  |     0|acc        | 0.5458|±  |0.0086|

Vanilla:

|                 Tasks                 |Version|     Filter     |n-shot|  Metric   | Value |   |Stderr|
|---------------------------------------|-------|----------------|-----:|-----------|------:|---|-----:|
|winogrande                             |      1|none            |     0|acc        | 0.7316|±  |0.0125|
|truthfulqa                             |N/A    |none            |     0|rouge1_diff| 2.9861|±  |1.0727|
|                                       |       |none            |     0|bleu_max   |22.0719|±  |0.7379|
|                                       |       |none            |     0|rouge2_acc | 0.4357|±  |0.0174|
|                                       |       |none            |     0|bleu_acc   | 0.4920|±  |0.0175|
|                                       |       |none            |     0|rouge2_max |31.5368|±  |0.9948|
|                                       |       |none            |     0|rouge2_diff| 2.4606|±  |1.1452|
|                                       |       |none            |     0|rouge1_max |46.9718|±  |0.8716|
|                                       |       |none            |     0|rougeL_acc | 0.5018|±  |0.0175|
|                                       |       |none            |     0|acc        | 0.4241|±  |0.0111|
|                                       |       |none            |     0|bleu_diff  | 1.8143|±  |0.7426|
|                                       |       |none            |     0|rougeL_diff| 2.8182|±  |1.0854|
|                                       |       |none            |     0|rougeL_max |44.0425|±  |0.8854|
|                                       |       |none            |     0|rouge1_acc | 0.5092|±  |0.0175|
| - truthfulqa_gen                      |      3|none            |     0|bleu_max   |22.0719|±  |0.7379|
|                                       |       |none            |     0|bleu_acc   | 0.4920|±  |0.0175|
|                                       |       |none            |     0|bleu_diff  | 1.8143|±  |0.7426|
|                                       |       |none            |     0|rouge1_max |46.9718|±  |0.8716|
|                                       |       |none            |     0|rouge1_acc | 0.5092|±  |0.0175|
|                                       |       |none            |     0|rouge1_diff| 2.9861|±  |1.0727|
|                                       |       |none            |     0|rouge2_max |31.5368|±  |0.9948|
|                                       |       |none            |     0|rouge2_acc | 0.4357|±  |0.0174|
|                                       |       |none            |     0|rouge2_diff| 2.4606|±  |1.1452|
|                                       |       |none            |     0|rougeL_max |44.0425|±  |0.8854|
|                                       |       |none            |     0|rougeL_acc | 0.5018|±  |0.0175|
|                                       |       |none            |     0|rougeL_diff| 2.8182|±  |1.0854|
| - truthfulqa_mc1                      |      2|none            |     0|acc        | 0.3452|±  |0.0166|
| - truthfulqa_mc2                      |      2|none            |     0|acc        | 0.5030|±  |0.0147|
|mmlu                                   |N/A    |none            |     0|acc        | 0.6285|±  |0.0038|
| - humanities                          |N/A    |none            |     0|acc        | 0.5532|±  |0.0067|
|  - formal_logic                       |      0|none            |     0|acc        | 0.4762|±  |0.0447|
|  - high_school_european_history       |      0|none            |     0|acc        | 0.7152|±  |0.0352|
|  - high_school_us_history             |      0|none            |     0|acc        | 0.8431|±  |0.0255|
|  - high_school_world_history          |      0|none            |     0|acc        | 0.8228|±  |0.0249|
|  - international_law                  |      0|none            |     0|acc        | 0.7603|±  |0.0390|
|  - jurisprudence                      |      0|none            |     0|acc        | 0.7778|±  |0.0402|
|  - logical_fallacies                  |      0|none            |     0|acc        | 0.7669|±  |0.0332|
|  - moral_disputes                     |      0|none            |     0|acc        | 0.6936|±  |0.0248|
|  - moral_scenarios                    |      0|none            |     0|acc        | 0.2492|±  |0.0145|
|  - philosophy                         |      0|none            |     0|acc        | 0.7010|±  |0.0260|
|  - prehistory                         |      0|none            |     0|acc        | 0.7191|±  |0.0250|
|  - professional_law                   |      0|none            |     0|acc        | 0.4622|±  |0.0127|
|  - world_religions                    |      0|none            |     0|acc        | 0.7836|±  |0.0316|
| - other                               |N/A    |none            |     0|acc        | 0.7219|±  |0.0078|
|  - business_ethics                    |      0|none            |     0|acc        | 0.6700|±  |0.0473|
|  - clinical_knowledge                 |      0|none            |     0|acc        | 0.7321|±  |0.0273|
|  - college_medicine                   |      0|none            |     0|acc        | 0.6243|±  |0.0369|
|  - global_facts                       |      0|none            |     0|acc        | 0.4400|±  |0.0499|
|  - human_aging                        |      0|none            |     0|acc        | 0.7130|±  |0.0304|
|  - management                         |      0|none            |     0|acc        | 0.8350|±  |0.0368|
|  - marketing                          |      0|none            |     0|acc        | 0.8974|±  |0.0199|
|  - medical_genetics                   |      0|none            |     0|acc        | 0.8000|±  |0.0402|
|  - miscellaneous                      |      0|none            |     0|acc        | 0.8135|±  |0.0139|
|  - nutrition                          |      0|none            |     0|acc        | 0.7516|±  |0.0247|
|  - professional_accounting            |      0|none            |     0|acc        | 0.5035|±  |0.0298|
|  - professional_medicine              |      0|none            |     0|acc        | 0.7206|±  |0.0273|
|  - virology                           |      0|none            |     0|acc        | 0.5422|±  |0.0388|
| - social_sciences                     |N/A    |none            |     0|acc        | 0.7377|±  |0.0077|
|  - econometrics                       |      0|none            |     0|acc        | 0.4386|±  |0.0467|
|  - high_school_geography              |      0|none            |     0|acc        | 0.7828|±  |0.0294|
|  - high_school_government_and_politics|      0|none            |     0|acc        | 0.8756|±  |0.0238|
|  - high_school_macroeconomics         |      0|none            |     0|acc        | 0.6333|±  |0.0244|
|  - high_school_microeconomics         |      0|none            |     0|acc        | 0.7521|±  |0.0280|
|  - high_school_psychology             |      0|none            |     0|acc        | 0.8257|±  |0.0163|
|  - human_sexuality                    |      0|none            |     0|acc        | 0.7557|±  |0.0377|
|  - professional_psychology            |      0|none            |     0|acc        | 0.6716|±  |0.0190|
|  - public_relations                   |      0|none            |     0|acc        | 0.6636|±  |0.0453|
|  - security_studies                   |      0|none            |     0|acc        | 0.7388|±  |0.0281|
|  - sociology                          |      0|none            |     0|acc        | 0.8507|±  |0.0252|
|  - us_foreign_policy                  |      0|none            |     0|acc        | 0.8500|±  |0.0359|
| - stem                                |N/A    |none            |     0|acc        | 0.5420|±  |0.0086|
|  - abstract_algebra                   |      0|none            |     0|acc        | 0.3500|±  |0.0479|
|  - anatomy                            |      0|none            |     0|acc        | 0.6074|±  |0.0422|
|  - astronomy                          |      0|none            |     0|acc        | 0.6776|±  |0.0380|
|  - college_biology                    |      0|none            |     0|acc        | 0.7708|±  |0.0351|
|  - college_chemistry                  |      0|none            |     0|acc        | 0.4700|±  |0.0502|
|  - college_computer_science           |      0|none            |     0|acc        | 0.4700|±  |0.0502|
|  - college_mathematics                |      0|none            |     0|acc        | 0.3800|±  |0.0488|
|  - college_physics                    |      0|none            |     0|acc        | 0.4902|±  |0.0497|
|  - computer_security                  |      0|none            |     0|acc        | 0.7500|±  |0.0435|
|  - conceptual_physics                 |      0|none            |     0|acc        | 0.5532|±  |0.0325|
|  - electrical_engineering             |      0|none            |     0|acc        | 0.6207|±  |0.0404|
|  - elementary_mathematics             |      0|none            |     0|acc        | 0.4550|±  |0.0256|
|  - high_school_biology                |      0|none            |     0|acc        | 0.7645|±  |0.0241|
|  - high_school_chemistry              |      0|none            |     0|acc        | 0.4581|±  |0.0351|
|  - high_school_computer_science       |      0|none            |     0|acc        | 0.6800|±  |0.0469|
|  - high_school_mathematics            |      0|none            |     0|acc        | 0.3889|±  |0.0297|
|  - high_school_physics                |      0|none            |     0|acc        | 0.4437|±  |0.0406|
|  - high_school_statistics             |      0|none            |     0|acc        | 0.4676|±  |0.0340|
|  - machine_learning                   |      0|none            |     0|acc        | 0.5179|±  |0.0474|
|hellaswag                              |      1|none            |     0|acc        | 0.5990|±  |0.0049|
|                                       |       |none            |     0|acc_norm   | 0.7963|±  |0.0040|
|gsm8k                                  |      3|strict-match    |     5|exact_match| 0.7324|±  |0.0122|
|                                       |       |flexible-extract|     5|exact_match| 0.7377|±  |0.0121|
|arc_easy                               |      1|none            |     0|acc        | 0.8199|±  |0.0079|
|                                       |       |none            |     0|acc_norm   | 0.7980|±  |0.0082|

|      Groups      |Version|Filter|n-shot|  Metric   | Value |   |Stderr|
|------------------|-------|------|-----:|-----------|------:|---|-----:|
|truthfulqa        |N/A    |none  |     0|rouge1_diff| 2.9861|±  |1.0727|
|                  |       |none  |     0|bleu_max   |22.0719|±  |0.7379|
|                  |       |none  |     0|rouge2_acc | 0.4357|±  |0.0174|
|                  |       |none  |     0|bleu_acc   | 0.4920|±  |0.0175|
|                  |       |none  |     0|rouge2_max |31.5368|±  |0.9948|
|                  |       |none  |     0|rouge2_diff| 2.4606|±  |1.1452|
|                  |       |none  |     0|rouge1_max |46.9718|±  |0.8716|
|                  |       |none  |     0|rougeL_acc | 0.5018|±  |0.0175|
|                  |       |none  |     0|acc        | 0.4241|±  |0.0111|
|                  |       |none  |     0|bleu_diff  | 1.8143|±  |0.7426|
|                  |       |none  |     0|rougeL_diff| 2.8182|±  |1.0854|
|                  |       |none  |     0|rougeL_max |44.0425|±  |0.8854|
|                  |       |none  |     0|rouge1_acc | 0.5092|±  |0.0175|
|mmlu              |N/A    |none  |     0|acc        | 0.6285|±  |0.0038|
| - humanities     |N/A    |none  |     0|acc        | 0.5532|±  |0.0067|
| - other          |N/A    |none  |     0|acc        | 0.7219|±  |0.0078|
| - social_sciences|N/A    |none  |     0|acc        | 0.7377|±  |0.0077|
| - stem           |N/A    |none  |     0|acc        | 0.5420|±  |0.0086|


LoRD-VI (train-num 3000, max_new_tokens=256, N=1024, tau1=0.75, tau2=0.8)

|                 Tasks                 |Version|     Filter     |n-shot|  Metric   | Value |   |Stderr|
|---------------------------------------|-------|----------------|-----:|-----------|------:|---|-----:|
|winogrande                             |      1|none            |     0|acc        | 0.7332|±  |0.0124|
|truthfulqa                             |N/A    |none            |     0|rouge2_diff| 0.1799|±  |1.0977|
|                                       |       |none            |     0|rouge1_diff| 0.6080|±  |1.0146|
|                                       |       |none            |     0|rouge1_max |47.1916|±  |0.8782|
|                                       |       |none            |     0|bleu_max   |22.4816|±  |0.7451|
|                                       |       |none            |     0|acc        | 0.4378|±  |0.0112|
|                                       |       |none            |     0|rouge2_max |31.3573|±  |0.9968|
|                                       |       |none            |     0|rouge2_acc | 0.4027|±  |0.0172|
|                                       |       |none            |     0|bleu_acc   | 0.4774|±  |0.0175|
|                                       |       |none            |     0|rougeL_diff| 0.7869|±  |1.0207|
|                                       |       |none            |     0|bleu_diff  | 0.9814|±  |0.7336|
|                                       |       |none            |     0|rougeL_acc | 0.4786|±  |0.0175|
|                                       |       |none            |     0|rougeL_max |44.2243|±  |0.8891|
|                                       |       |none            |     0|rouge1_acc | 0.4798|±  |0.0175|
| - truthfulqa_gen                      |      3|none            |     0|bleu_max   |22.4816|±  |0.7451|
|                                       |       |none            |     0|bleu_acc   | 0.4774|±  |0.0175|
|                                       |       |none            |     0|bleu_diff  | 0.9814|±  |0.7336|
|                                       |       |none            |     0|rouge1_max |47.1916|±  |0.8782|
|                                       |       |none            |     0|rouge1_acc | 0.4798|±  |0.0175|
|                                       |       |none            |     0|rouge1_diff| 0.6080|±  |1.0146|
|                                       |       |none            |     0|rouge2_max |31.3573|±  |0.9968|
|                                       |       |none            |     0|rouge2_acc | 0.4027|±  |0.0172|
|                                       |       |none            |     0|rouge2_diff| 0.1799|±  |1.0977|
|                                       |       |none            |     0|rougeL_max |44.2243|±  |0.8891|
|                                       |       |none            |     0|rougeL_acc | 0.4786|±  |0.0175|
|                                       |       |none            |     0|rougeL_diff| 0.7869|±  |1.0207|
| - truthfulqa_mc1                      |      2|none            |     0|acc        | 0.3599|±  |0.0168|
| - truthfulqa_mc2                      |      2|none            |     0|acc        | 0.5157|±  |0.0147|
|mmlu                                   |N/A    |none            |     0|acc        | 0.6337|±  |0.0038|
| - humanities                          |N/A    |none            |     0|acc        | 0.5681|±  |0.0067|
|  - formal_logic                       |      0|none            |     0|acc        | 0.4841|±  |0.0447|
|  - high_school_european_history       |      0|none            |     0|acc        | 0.7212|±  |0.0350|
|  - high_school_us_history             |      0|none            |     0|acc        | 0.8431|±  |0.0255|
|  - high_school_world_history          |      0|none            |     0|acc        | 0.8523|±  |0.0231|
|  - international_law                  |      0|none            |     0|acc        | 0.7686|±  |0.0385|
|  - jurisprudence                      |      0|none            |     0|acc        | 0.7685|±  |0.0408|
|  - logical_fallacies                  |      0|none            |     0|acc        | 0.7423|±  |0.0344|
|  - moral_disputes                     |      0|none            |     0|acc        | 0.7023|±  |0.0246|
|  - moral_scenarios                    |      0|none            |     0|acc        | 0.2961|±  |0.0153|
|  - philosophy                         |      0|none            |     0|acc        | 0.7106|±  |0.0258|
|  - prehistory                         |      0|none            |     0|acc        | 0.7191|±  |0.0250|
|  - professional_law                   |      0|none            |     0|acc        | 0.4746|±  |0.0128|
|  - world_religions                    |      0|none            |     0|acc        | 0.7719|±  |0.0322|
| - other                               |N/A    |none            |     0|acc        | 0.7203|±  |0.0078|
|  - business_ethics                    |      0|none            |     0|acc        | 0.6200|±  |0.0488|
|  - clinical_knowledge                 |      0|none            |     0|acc        | 0.7208|±  |0.0276|
|  - college_medicine                   |      0|none            |     0|acc        | 0.6069|±  |0.0372|
|  - global_facts                       |      0|none            |     0|acc        | 0.4300|±  |0.0498|
|  - human_aging                        |      0|none            |     0|acc        | 0.7085|±  |0.0305|
|  - management                         |      0|none            |     0|acc        | 0.8544|±  |0.0349|
|  - marketing                          |      0|none            |     0|acc        | 0.8932|±  |0.0202|
|  - medical_genetics                   |      0|none            |     0|acc        | 0.8000|±  |0.0402|
|  - miscellaneous                      |      0|none            |     0|acc        | 0.8212|±  |0.0137|
|  - nutrition                          |      0|none            |     0|acc        | 0.7549|±  |0.0246|
|  - professional_accounting            |      0|none            |     0|acc        | 0.5177|±  |0.0298|
|  - professional_medicine              |      0|none            |     0|acc        | 0.7096|±  |0.0276|
|  - virology                           |      0|none            |     0|acc        | 0.5361|±  |0.0388|
| - social_sciences                     |N/A    |none            |     0|acc        | 0.7361|±  |0.0078|
|  - econometrics                       |      0|none            |     0|acc        | 0.4386|±  |0.0467|
|  - high_school_geography              |      0|none            |     0|acc        | 0.8030|±  |0.0283|
|  - high_school_government_and_politics|      0|none            |     0|acc        | 0.8808|±  |0.0234|
|  - high_school_macroeconomics         |      0|none            |     0|acc        | 0.6564|±  |0.0241|
|  - high_school_microeconomics         |      0|none            |     0|acc        | 0.7437|±  |0.0284|
|  - high_school_psychology             |      0|none            |     0|acc        | 0.8220|±  |0.0164|
|  - human_sexuality                    |      0|none            |     0|acc        | 0.7557|±  |0.0377|
|  - professional_psychology            |      0|none            |     0|acc        | 0.6454|±  |0.0194|
|  - public_relations                   |      0|none            |     0|acc        | 0.6727|±  |0.0449|
|  - security_studies                   |      0|none            |     0|acc        | 0.7347|±  |0.0283|
|  - sociology                          |      0|none            |     0|acc        | 0.8458|±  |0.0255|
|  - us_foreign_policy                  |      0|none            |     0|acc        | 0.8700|±  |0.0338|
| - stem                                |N/A    |none            |     0|acc        | 0.5461|±  |0.0086|
|  - abstract_algebra                   |      0|none            |     0|acc        | 0.3500|±  |0.0479|
|  - anatomy                            |      0|none            |     0|acc        | 0.6074|±  |0.0422|
|  - astronomy                          |      0|none            |     0|acc        | 0.6842|±  |0.0378|
|  - college_biology                    |      0|none            |     0|acc        | 0.7569|±  |0.0359|
|  - college_chemistry                  |      0|none            |     0|acc        | 0.4800|±  |0.0502|
|  - college_computer_science           |      0|none            |     0|acc        | 0.4700|±  |0.0502|
|  - college_mathematics                |      0|none            |     0|acc        | 0.3700|±  |0.0485|
|  - college_physics                    |      0|none            |     0|acc        | 0.4804|±  |0.0497|
|  - computer_security                  |      0|none            |     0|acc        | 0.7600|±  |0.0429|
|  - conceptual_physics                 |      0|none            |     0|acc        | 0.5617|±  |0.0324|
|  - electrical_engineering             |      0|none            |     0|acc        | 0.6138|±  |0.0406|
|  - elementary_mathematics             |      0|none            |     0|acc        | 0.4630|±  |0.0257|
|  - high_school_biology                |      0|none            |     0|acc        | 0.7613|±  |0.0243|
|  - high_school_chemistry              |      0|none            |     0|acc        | 0.5172|±  |0.0352|
|  - high_school_computer_science       |      0|none            |     0|acc        | 0.7000|±  |0.0461|
|  - high_school_mathematics            |      0|none            |     0|acc        | 0.3667|±  |0.0294|
|  - high_school_physics                |      0|none            |     0|acc        | 0.4437|±  |0.0406|
|  - high_school_statistics             |      0|none            |     0|acc        | 0.4861|±  |0.0341|
|  - machine_learning                   |      0|none            |     0|acc        | 0.5089|±  |0.0475|
|hellaswag                              |      1|none            |     0|acc        | 0.5970|±  |0.0049|
|                                       |       |none            |     0|acc_norm   | 0.7966|±  |0.0040|
|gsm8k                                  |      3|strict-match    |     5|exact_match| 0.7415|±  |0.0121|
|                                       |       |flexible-extract|     5|exact_match| 0.7415|±  |0.0121|
|arc_easy                               |      1|none            |     0|acc        | 0.8152|±  |0.0080|
|                                       |       |none            |     0|acc_norm   | 0.7925|±  |0.0083|

|      Groups      |Version|Filter|n-shot|  Metric   | Value |   |Stderr|
|------------------|-------|------|-----:|-----------|------:|---|-----:|
|truthfulqa        |N/A    |none  |     0|rouge2_diff| 0.1799|±  |1.0977|
|                  |       |none  |     0|rouge1_diff| 0.6080|±  |1.0146|
|                  |       |none  |     0|rouge1_max |47.1916|±  |0.8782|
|                  |       |none  |     0|bleu_max   |22.4816|±  |0.7451|
|                  |       |none  |     0|acc        | 0.4378|±  |0.0112|
|                  |       |none  |     0|rouge2_max |31.3573|±  |0.9968|
|                  |       |none  |     0|rouge2_acc | 0.4027|±  |0.0172|
|                  |       |none  |     0|bleu_acc   | 0.4774|±  |0.0175|
|                  |       |none  |     0|rougeL_diff| 0.7869|±  |1.0207|
|                  |       |none  |     0|bleu_diff  | 0.9814|±  |0.7336|
|                  |       |none  |     0|rougeL_acc | 0.4786|±  |0.0175|
|                  |       |none  |     0|rougeL_max |44.2243|±  |0.8891|
|                  |       |none  |     0|rouge1_acc | 0.4798|±  |0.0175|
|mmlu              |N/A    |none  |     0|acc        | 0.6337|±  |0.0038|
| - humanities     |N/A    |none  |     0|acc        | 0.5681|±  |0.0067|
| - other          |N/A    |none  |     0|acc        | 0.7203|±  |0.0078|
| - social_sciences|N/A    |none  |     0|acc        | 0.7361|±  |0.0078|
| - stem           |N/A    |none  |     0|acc        | 0.5461|±  |0.0086|
*** LoRD-VII results

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5401 | ±  | 0.0146 |
|               |         | none   |      0 | acc_norm | 0.5794 | ±  | 0.0144 |
| hellaswag     |       1 | none   |      0 | acc      | 0.6060 | ±  | 0.0049 |
|               |         | none   |      0 | acc_norm | 0.8102 | ±  | 0.0039 |
| winogrande    |       1 | none   |      0 | acc      | 0.7427 | ±  | 0.0123 |


| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.5452 | ±  | 0.0146 |
|               |         | none   |      0 | acc_norm | 0.5862 | ±  | 0.0144 |
| hellaswag     |       1 | none   |      0 | acc      | 0.6100 | ±  | 0.0049 |
|               |         | none   |      0 | acc_norm | 0.8148 | ±  | 0.0039 |
| winogrande    |       1 | none   |      0 | acc      | 0.7395 | ±  | 0.0123 |

** GPT-4 cleaned
*** With 256 samples
lord-vi

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5452|±  |0.0146|
|             |       |none  |     0|acc_norm|0.5666|±  |0.0145|
|hellaswag    |      1|none  |     0|acc     |0.5833|±  |0.0049|
|             |       |none  |     0|acc_norm|0.7690|±  |0.0042|
|winogrande   |      1|none  |     0|acc     |0.7309|±  |0.0125|

vanilla

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5401|±  |0.0146|
|             |       |none  |     0|acc_norm|0.5691|±  |0.0145|
|hellaswag    |      1|none  |     0|acc     |0.5818|±  |0.0049|
|             |       |none  |     0|acc_norm|0.7719|±  |0.0042|
|winogrande   |      1|none  |     0|acc     |0.7364|±  |0.0124|

lord-vi (only contrastive loss)
|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5256|±  |0.0146|
|             |       |none  |     0|acc_norm|0.5589|±  |0.0145|
|hellaswag    |      1|none  |     0|acc     |0.5755|±  |0.0049|
|             |       |none  |     0|acc_norm|0.7679|±  |0.0042|
|winogrande   |      1|none  |     0|acc     |0.7253|±  |0.0125|



Prof. HU: not the same method, but the same *core idea*.

Are there any other core idea to solve this improvements???

*** With 1000 samples

vanilla

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5674|±  |0.0145|
|             |       |none  |     0|acc_norm|0.5819|±  |0.0144|
|hellaswag    |      1|none  |     0|acc     |0.5907|±  |0.0049|
|             |       |none  |     0|acc_norm|0.7768|±  |0.0042|


lord-vi (2000 steps)

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5486|±  |0.0145|
|             |       |none  |     0|acc_norm|0.5691|±  |0.0145|
|hellaswag    |      1|none  |     0|acc     |0.5898|±  |0.0049|
|             |       |none  |     0|acc_norm|0.7727|±  |0.0042|
|winogrande   |      1|none  |     0|acc     |0.7293|±  |0.0125|

lord-vi (3000 steps)

|    Tasks    |Version|Filter|n-shot| Metric |Value |   |Stderr|
|-------------|------:|------|-----:|--------|-----:|---|-----:|
|arc_challenge|      1|none  |     0|acc     |0.5213|±  |0.0146|
|             |       |none  |     0|acc_norm|0.5452|±  |0.0146|
|hellaswag    |      1|none  |     0|acc     |0.5822|±  |0.0049|
|             |       |none  |     0|acc_norm|0.7574|±  |0.0043|
|winogrande   |      1|none  |     0|acc     |0.7182|±  |0.0126|


lord-vi (4000 steps)

| Tasks         | Version | Filter | n-shot | Metric   |  Value |   | Stderr |
|---------------+---------+--------+--------+----------+--------+---+--------|
| arc_challenge |       1 | none   |      0 | acc      | 0.4821 | ±  | 0.0146 |
|               |         | none   |      0 | acc_norm | 0.5068 | ±  | 0.0146 |
| hellaswag     |       1 | none   |      0 | acc      | 0.5756 | ±  | 0.0049 |
|               |         | none   |      0 | acc_norm | 0.7487 | ±  | 0.0043 |
| winogrande    |       1 | none   |      0 | acc      | 0.7056 | ±  | 0.0128 |

** watermark

We expect in the watermark experiments:
+ P-value: LoRD> vanilla
+ Green-word fraction: LoRD < vanilla
+ Z-score: LoRD<vanilla

with 64 times of querys

#+begin_src python

{'allenai/common_gen-----__watermark__d2t_ckpts__D2TTTallenai__common_gen@wrmk641LoRD-VI___period512_____allenai__common_gen@wrmk_d2t_infer_resjson': {'bertscore': {'f1': 0.8722925782203674,
                                                                                                                                                                     'p': 0.8345624208450317,
                                                                                                                                                                     'r': 0.9137428998947144},
                                                                                                                                                       'bleu': {'1': 0.35043698900479286,
                                                                                                                                                                '2': 0.2032080966682189,
                                                                                                                                                                '3': 0.12093166283138004,
                                                                                                                                                                '4': 0.07363683945946936},
                                                                                                                                                       'green_fraction': 0.2647058823529412,
                                                                                                                                                       'num_green_tokens': 9,
                                                                                                                                                       'num_tokens_scored': 34,
                                                                                                                                                       'p_value': 0.42151098818486243,
                                                                                                                                                       'prediction': False,
                                                                                                                                                       'rouge-l': {'f1': 0.3796643044590478,
                                                                                                                                                                   'p': 0.35598089286897266,
                                                                                                                                                                   'r': 0.4237586268588298},
                                                                                                                                                       'z_score': 0.19802950859533489},
 'allenai/common_gen-----__watermark__d2t_ckpts__D2TTTallenai__common_gen@wrmk641vanilla___finally_____allenai__common_gen@wrmk_d2t_infer_resjson': {'bertscore': {'f1': 0.8726513385772705,
                                                                                                                                                                   'p': 0.8354629278182983,
                                                                                                                                                                   'r': 0.9134514927864075},
                                                                                                                                                     'bleu': {'1': 0.3178174295319369,
                                                                                                                                                              '2': 0.17969801208376904,
                                                                                                                                                              '3': 0.10361766613745924,
                                                                                                                                                              '4': 0.06146900055516714},
                                                                                                                                                     'green_fraction': 0.38235294117647056,
                                                                                                                                                     'num_green_tokens': 13,
                                                                                                                                                     'num_tokens_scored': 34,
                                                                                                                                                     'p_value': 0.03735296665606522,
                                                                                                                                                     'prediction': False,
                                                                                                                                                     'rouge-l': {'f1': 0.35694429477240597,
                                                                                                                                                                 'p': 0.323578978766058,
                                                                                                                                                                 'r': 0.41506365007215607},
                                                                                                                                                     'z_score': 1.7822655773580138},
 'e2e_nlg-----__watermark__d2t_ckpts__D2TTTe2e_nlg@wrmk641LoRD-VI___period512_____e2e_nlg@wrmk_d2t_infer_resjson': {'bertscore': {'f1': 0.9073704481124878,
                                                                                                                                  'p': 0.8802347779273987,
                                                                                                                                  'r': 0.9364007115364075},
                                                                                                                    'bleu': {'1': 0.49008658053436366,
                                                                                                                             '2': 0.33756781232336586,
                                                                                                                             '3': 0.23743468812796212,
                                                                                                                             '4': 0.1619372156200498},
                                                                                                                    'green_fraction': 0.27906976744186046,
                                                                                                                    'num_green_tokens': 12,
                                                                                                                    'num_tokens_scored': 43,
                                                                                                                    'p_value': 0.3298869132560869,
                                                                                                                    'prediction': False,
                                                                                                                    'rouge-l': {'f1': 0.4443390372515331,
                                                                                                                                'p': 0.44010526350393564,
                                                                                                                                'r': 0.459521135438056},
                                                                                                                    'z_score': 0.4402254531628119},
 'e2e_nlg-----__watermark__d2t_ckpts__D2TTTe2e_nlg@wrmk641vanilla___finally_____e2e_nlg@wrmk_d2t_infer_resjson': {'bertscore': {'f1': 0.909862220287323,
                                                                                                                                'p': 0.8831789493560791,
                                                                                                                                'r': 0.9383627772331238},
                                                                                                                  'bleu': {'1': 0.504872797454937,
                                                                                                                           '2': 0.34240284009039,
                                                                                                                           '3': 0.23611685741884592,
                                                                                                                           '4': 0.1581794014781304},
                                                                                                                  'green_fraction': 0.3023255813953488,
                                                                                                                  'num_green_tokens': 13,
                                                                                                                  'num_tokens_scored': 43,
                                                                                                                  'p_value': 0.21406204473308316,
                                                                                                                  'prediction': False,
                                                                                                                  'rouge-l': {'f1': 0.4512897178346773,
                                                                                                                              'p': 0.4456838476446302,
                                                                                                                              'r': 0.4678684280080462},
                                                                                                                  'z_score': 0.7924058156930615}}


#+end_src

LoRD-VII: Experiment Results.


#+begin_src python
{'allenai/common_gen-----__watermark__d2t_ckpts__D2TTTallenai__common_gen@wrmk641LoRD-VI___period512_____allenai__common_gen@wrmk_d2t_infer_resjson': {'bertscore': {'f1': 0.8722925782203674,                                  
                                                                                                                                                                     'p': 0.8345624208450317,                                   
                                                                                                                                                                     'r': 0.9137428998947144},                                  
                                                                                                                                                       'bleu': {'1': 0.35043698900479286,                                       
                                                                                                                                                                '2': 0.2032080966682189,                                        
                                                                                                                                                                '3': 0.12093166283138004,                                       
                                                                                                                                                                '4': 0.07363683945946936},                                      
                                                                                                                                                       'green_fraction': 0.2647058823529412,                                    
                                                                                                                                                       'num_green_tokens': 9,                                                   
                                                                                                                                                       'num_tokens_scored': 34,                                                 
                                                                                                                                                       'p_value': 0.42151098818486243,                                          
                                                                                                                                                       'prediction': False,                                                     
                                                                                                                                                       'rouge-l': {'f1': 0.3796643044590478,                                    
                                                                                                                                                                   'p': 0.35598089286897266,                                    
                                                                                                                                                                   'r': 0.4237586268588298},                                    
                                                                                                                                                       'z_score': 0.19802950859533489},                                         
 'e2e_nlg-----__watermark__d2t_ckpts__D2TTTe2e_nlg@wrmk641LoRD-VI___period512_____e2e_nlg@wrmk_d2t_infer_resjson': {'bertscore': {'f1': 0.9073704481124878,                                                                     
                                                                                                                                  'p': 0.8802347779273987,                                                                      
                                                                                                                                  'r': 0.9364007115364075},                                                                     
                                                                                                                    'bleu': {'1': 0.49008658053436366,                                                                          
                                                                                                                             '2': 0.33756781232336586,                                                                          
                                                                                                                             '3': 0.23743468812796212,                                                                          
                                                                                                                             '4': 0.1619372156200498},                                                                          
                                                                                                                    'green_fraction': 0.27906976744186046,                                                                      
                                                                                                                    'num_green_tokens': 12,                                                                                     
                                                                                                                    'num_tokens_scored': 43,                                                                                    
                                                                                                                    'p_value': 0.3298869132560869,                                                                              
                                                                                                                    'prediction': False,                                                                                        
                                                                                                                    'rouge-l': {'f1': 0.4443390372515331,                                                                       
                                                                                                                                'p': 0.44010526350393564,                                                                       
                                                                                                                                'r': 0.459521135438056},                                                                        
                                                                                                                    'z_score': 0.4402254531628119}}
  
#+end_src




with 1000 times of query

#+begin_src python
{'allenai/common_gen-----__watermark__d2t_ckpts__D2TTTallenai__common_gen@wrmk641LoRD-VII___period512_____allenai__common_gen@wrmk_d2t_infer_resjson': {'bertscore': {'f1': 0.872545599937439,
                                                                                                                                                                      'p': 0.8355032205581665,
                                                                                                                                                                      'r': 0.9131741523742676},
                                                                                                                                                        'bleu': {'1': 0.2834863400066364,
                                                                                                                                                                 '2': 0.16162147789653342,
                                                                                                                                                                 '3': 0.09241974934221728,
                                                                                                                                                                 '4': 0.05445480109461286},
                                                                                                                                                        'green_fraction': 0.2571428571428571,
                                                                                                                                                        'num_green_tokens': 9,
                                                                                                                                                        'num_tokens_scored': 35,
                                                                                                                                                        'p_value': 0.46112892982603093,
                                                                                                                                                        'prediction': False,
                                                                                                                                                        'rouge-l': {'f1': 0.33908837872722647,
                                                                                                                                                                    'p': 0.2964621600133307,
                                                                                                                                                                    'r': 0.41869109142728533},
                                                                                                                                                        'z_score': 0.09759000729485333},
 'allenai/common_gen-----__watermark__d2t_ckpts__D2TTTallenai__common_gen@wrmk641vanilla___finally_____allenai__common_gen@wrmk_d2t_infer_resjson': {'bertscore': {'f1': 0.8721603751182556,
                                                                                                                                                                   'p': 0.835795521736145,
                                                                                                                                                                   'r': 0.911978542804718},
                                                                                                                                                     'bleu': {'1': 0.26168618761211354,
                                                                                                                                                              '2': 0.14016989105002184,
                                                                                                                                                              '3': 0.07764965895676125,
                                                                                                                                                              '4': 0.04436400548684131},
                                                                                                                                                     'green_fraction': 0.18421052631578946,
                                                                                                                                                     'num_green_tokens': 7,
                                                                                                                                                     'num_tokens_scored': 38,
                                                                                                                                                     'p_value': 0.825514173425129,
                                                                                                                                                     'prediction': False,
                                                                                                                                                     'rouge-l': {'f1': 0.31178666512894554,
                                                                                                                                                                 'p': 0.26642968023135616,
                                                                                                                                                                 'r': 0.39496569908656803},
                                                                                                                                                     'z_score': -0.936585811581694}}
#+end_src

** Continuing Pre-training
