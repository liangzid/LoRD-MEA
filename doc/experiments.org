#+title: Complex Experiments in Model Extraction
#+date: Thu Apr 25 13:45:37 2024
#+author: Zi Liang
#+email: zi1415926.liang@connect.polyu.hk
#+latex_class: elegantpaper
#+filetags: :lord:


* 226 domain-specific training (gemma-7b new version)

|---------------+---------+-------+-------+-------+-------|
| method        | dataset |   acc |   pre |   rec |    f1 |
|---------------+---------+-------+-------+-------+-------|
| vanilla       | piqa    | 0.742 | 0.784 | 0.742 | 0.732 |
| kd            | piqa    | 0.722 | 0.739 | 0.722 | 0.717 |
| lordii        | piqa    | 0.742 | 0.784 | 0.742 | 0.732 |
| lordiv        | piqa    | 0.540 | 0.540 | 0.539 | 0.538 |
| lordvi        | piqa    | ?     |  ?     | ?      |  ?     |
| Complex-lord  | piqa    | 0.648 | 0.778 | 0.647 | 0.636 |
|---------------+---------+-------+-------+-------+-------|
| gemma2-7b     | piqa    | 0.498 | 0.490 | 0.496 | 0.401 |
| gpt-3.5-turbo | piqa    | 0.828 | 0.827 | 0.828 | 0.827 |
|---------------+---------+-------+-------+-------+-------|


Given up Gemma2-7b and use Llama3-8B


+ New Settings
  + train num: 256
  + max new token: 32
  + ...


QA related experiments.

|---------------+---------+-------+-------+-------+-------|
| method        | dataset |   acc |   pre |   rec |    f1 |
|---------------+---------+-------+-------+-------+-------|
| gpt-3.5-turbo | piqa    | 0.828 | 0.827 | 0.828 | 0.827 |
| llama3-8b     | piqa    | 0.666 | 0.684 | 0.665 | 0.656 |
|---------------+---------+-------+-------+-------+-------|
| vanilla       | piqa    | 0.774 | 0.776 | 0.774 | 0.773 |
|---------------+---------+-------+-------+-------+-------|
| lordvi e1     | piqa    | 0.69 | 0.71 | 0.68 | 0.67 |
| lordvi e2     | piqa    | 0. | 0. | 0. | 0. |
| lordvi e3     | piqa    | 0. | 0. | 0. | 0. |
| lordvi e4     | piqa    | 0. | 0. | 0. | 0. |
| lordvi e5     | piqa    | 0. | 0. | 0. | 0. |
| lordvi e6     | piqa    | 0. | 0. | 0. | 0. |
| lordvi e7     | piqa    | 0. | 0. | 0. | 0. |
| lordvi e8     | piqa    | 0. | 0. | 0. | 0. |
|---------------+---------+-------+-------+-------+-------|


* 226: =WMT16= domain-specific training (llama3-8B new version)

+ Settings
  + train num: 256
  + ...

** CS-EN

|------------+-------+-------+-------+--------+-------+-------+-------|
| method     |  BS-P |  BS-R |  BS-F | BLEU-4 |   R-P |   R-R |   R-F |
|------------+-------+-------+-------+--------+-------+-------+-------|
| 3.5        | 0.958 | 0.957 | 0.957 |  0.313 | 0.600 | 0.614 | 0.604 |
| llama3-8B  | 0.831 | 0.911 | 0.868 |  0.105 | 0.297 | 0.562 | 0.348 |
|------------+-------+-------+-------+--------+-------+-------+-------|
| vanilla    | 0.859 | 0.940 | 0.897 |  0.256 | 0.523 | 0.565 | 0.539 |
|------------+-------+-------+-------+--------+-------+-------+-------|
| lord-vi e1 | 0.865 | 0.936 | 0.899 |  0.244 | 0.511 | 0.555 | 0.527 |
| lord-vi e2 | 0.871 | 0.939 | 0.903 |  0.226 | 0.496 | 0.552 | 0.516 |
| lord-vi e3 | 0.873 | 0.942 | 0.906 |  0.254 | 0.523 | 0.565 | 0.540 |
| lord-vi e4 | 0.870 | 0.940 | 0.904 |  0.255 | 0.536 | 0.574 | 0.551 |
| lord-vi e5 | 0.873 | 0.942 | 0.906 |  0.265 | 0.541 | 0.576 | 0.546 |
| lord-vi e6 | 0.871 | 0.941 | 0.905 |  0.261 | 0.533 | 0.565 | 0.546 |
| lord-vi e7 | 0.874 | 0.943 | 0.907 |  0.252 | 0.533 | 0.570 | 0.548 |
| lord-vi e8 | 0.874 | 0.944 | 0.907 |  0.261 | 0.536 | 0.575 | 0.552 |
| lord-vi e9 | 0.875 | 0.945 | 0.909 |  0.270 | 0.542 | 0.583 | 0.559 |
|            |       |       |       |        |       |       |       |
|------------+-------+-------+-------+--------+-------+-------+-------|







* Changes varing training number: No significant difference on Gemma2B
** Results on PiQA  [231 SERVER]

|-----------+----------+---------+-------+-------+-------+-------|
| train-num | try-time | method  |   acc |    f1 |   pre |   rec |
|-----------+----------+---------+-------+-------+-------+-------|
|       512 |        3 | vanilla | 0.528 | 0.408 |  0.58 | 0.518 |
|       512 |        3 | kd      |  0.52 | 0.394 | 0.549 |  0.51 |
|       512 |        3 | LoRD-II | 0.514 | 0.455 | 0.512 | 0.507 |
|       512 |        1 | vanilla | 0.513 | 0.471 |  0.53 | 0.519 |
|       512 |        3 | LoRD-IV | 0.504 | 0.443 | 0.522 | 0.512 |
|       512 |        2 | vanilla |   0.5 | 0.426 | 0.518 | 0.508 |
|       512 |        2 | LoRD-II | 0.494 | 0.489 | 0.496 | 0.496 |
|       512 |        1 | LoRD-II | 0.492 | 0.473 | 0.496 | 0.496 |
|       512 |        2 | LoRD-IV | 0.485 | 0.354 | 0.467 | 0.495 |
|       512 |        1 | kd      | 0.483 | 0.368 | 0.468 | 0.493 |
|       512 |        2 | kd      | 0.477 | 0.449 | 0.477 | 0.482 |
|       512 |        1 | LoRD-IV | 0.473 |  0.45 | 0.473 | 0.478 |
|-----------+----------+---------+-------+-------+-------+-------|
|       256 |        1 | kd      | 0.511 |  0.34 | 0.506 |   0.5 |
|       256 |        1 | vanilla |  0.51 | 0.473 | 0.523 | 0.516 |
|       256 |        1 | LoRD-II | 0.508 | 0.337 | 0.255 | 0.497 |
|       256 |        3 | kd      | 0.506 | 0.496 | 0.503 | 0.503 |
|       256 |        3 | LoRD-II | 0.507 | 0.491 | 0.504 | 0.503 |
|       256 |        3 | vanilla | 0.504 | 0.485 |   0.5 |   0.5 |
|       256 |        2 | vanilla | 0.493 | 0.414 | 0.503 | 0.501 |
|       256 |        2 | LoRD-IV |  0.49 | 0.422 | 0.496 | 0.498 |
|       256 |        3 | LoRD-IV | 0.488 |  0.44 | 0.473 | 0.482 |
|       256 |        1 | LoRD-IV | 0.487 | 0.477 | 0.489 |  0.49 |
|       256 |        2 | kd      | 0.486 | 0.369 | 0.481 | 0.496 |
|       256 |        2 | LoRD-II | 0.483 | 0.369 | 0.469 | 0.493 |
|-----------+----------+---------+-------+-------+-------+-------|
|       100 |        2 | LoRD-II | 0.531 | 0.518 |  0.53 | 0.528 |
|       100 |        3 | vanilla | 0.518 | 0.398 | 0.535 | 0.508 |
|       100 |        2 | LoRD-IV | 0.517 | 0.458 | 0.517 |  0.51 |
|       100 |        2 | kd      | 0.512 | 0.364 | 0.515 | 0.502 |
|       100 |        3 | kd      | 0.511 | 0.338 | 0.256 |   0.5 |
|       100 |        1 | kd      | 0.505 | 0.348 | 0.429 | 0.494 |
|       100 |        1 | LoRD-IV | 0.504 | 0.444 | 0.521 | 0.511 |
|       100 |        3 | LoRD-II | 0.494 | 0.461 |   0.5 |   0.5 |
|       100 |        3 | LoRD-IV | 0.489 | 0.328 | 0.244 |   0.5 |
|       100 |        1 | LoRD-II | 0.489 | 0.383 | 0.494 | 0.498 |
|       100 |        2 | vanilla | 0.489 | 0.396 | 0.494 | 0.498 |
|       100 |        1 | vanilla | 0.485 |  0.48 | 0.483 | 0.483 |
|-----------+----------+---------+-------+-------+-------+-------|
|        64 |        3 | kd      | 0.498 | 0.385 | 0.463 | 0.489 |
|        64 |        3 | vanilla | 0.492 | 0.481 | 0.495 | 0.495 |
|        64 |        1 | kd      | 0.492 | 0.342 | 0.563 | 0.503 |
|        64 |        1 | LoRD-IV | 0.489 | 0.328 | 0.244 |   0.5 |
|        64 |        2 | LoRD-IV | 0.489 | 0.328 | 0.244 |   0.5 |
|        64 |        2 | kd      | 0.487 |  0.42 |  0.49 | 0.495 |
|        64 |        2 | vanilla | 0.486 | 0.375 | 0.482 | 0.496 |
|        64 |        1 | vanilla | 0.485 | 0.373 | 0.478 | 0.495 |
|        64 |        3 | LoRD-II | 0.485 | 0.375 | 0.479 | 0.494 |
|        64 |        1 | LoRD-II | 0.484 | 0.371 | 0.474 | 0.494 |
|        64 |        2 | LoRD-II |  0.48 | 0.393 | 0.471 | 0.489 |
|        64 |        3 | LoRD-IV | 0.476 | 0.476 | 0.476 | 0.476 |
|-----------+----------+---------+-------+-------+-------+-------|
|        32 |        3 | kd      | 0.516 | 0.388 | 0.531 | 0.506 |
|        32 |        2 | kd      | 0.511 | 0.342 | 0.506 |   0.5 |
|        32 |        2 | LoRD-II | 0.511 | 0.354 | 0.506 |   0.5 |
|        32 |        1 | LoRD-IV | 0.511 | 0.338 | 0.256 |   0.5 |
|        32 |        2 | LoRD-IV |  0.51 |  0.34 | 0.422 | 0.499 |
|        32 |        1 | kd      |  0.51 | 0.338 | 0.255 | 0.499 |
|        32 |        1 | LoRD-II | 0.507 | 0.501 |  0.51 |  0.51 |
|        32 |        2 | vanilla | 0.504 | 0.496 | 0.508 | 0.507 |
|        32 |        3 | LoRD-IV | 0.501 | 0.447 | 0.491 | 0.494 |
|        32 |        3 | LoRD-II | 0.487 | 0.339 | 0.463 | 0.498 |
|        32 |        3 | vanilla | 0.485 | 0.464 | 0.487 |  0.49 |
|        32 |        1 | vanilla | 0.484 | 0.372 | 0.474 | 0.494 |
|-----------+----------+---------+-------+-------+-------+-------|
|        16 |        1 | vanilla |  0.52 | 0.504 | 0.528 | 0.524 |
|        16 |        2 | vanilla | 0.513 | 0.471 |  0.51 | 0.507 |
|        16 |        3 | kd      | 0.509 | 0.337 | 0.255 | 0.498 |
|        16 |        1 | kd      | 0.496 |  0.43 | 0.507 | 0.504 |
|        16 |        2 | LoRD-IV | 0.492 | 0.363 | 0.514 | 0.502 |
|        16 |        3 | LoRD-IV |  0.49 | 0.368 |   0.5 |   0.5 |
|        16 |        1 | LoRD-II | 0.489 | 0.366 | 0.494 | 0.499 |
|        16 |        1 | LoRD-IV | 0.488 |  0.42 | 0.492 | 0.496 |
|        16 |        2 | kd      | 0.488 |  0.38 |  0.49 | 0.497 |
|        16 |        2 | LoRD-II | 0.488 | 0.406 | 0.458 |  0.48 |
|        16 |        3 | vanilla | 0.486 | 0.483 | 0.487 | 0.488 |
|        16 |        3 | LoRD-II | 0.485 | 0.374 | 0.478 | 0.495 |
|-----------+----------+---------+-------+-------+-------+-------|
|         8 |        2 | LoRD-IV | 0.526 | 0.525 | 0.525 | 0.525 |
|         8 |        2 | LoRD-II | 0.524 | 0.522 | 0.523 | 0.523 |
|         8 |        1 | kd      | 0.517 | 0.515 | 0.516 | 0.516 |
|         8 |        2 | kd      | 0.512 |  0.34 | 0.756 | 0.501 |
|         8 |        3 | LoRD-IV | 0.485 | 0.464 | 0.478 | 0.481 |
|         8 |        3 | kd      | 0.509 | 0.339 |  0.38 | 0.498 |
|         8 |        3 | vanilla | 0.506 | 0.361 | 0.469 | 0.496 |
|         8 |        2 | vanilla | 0.491 | 0.412 | 0.498 | 0.499 |
|         8 |        1 | LoRD-IV | 0.489 | 0.328 | 0.244 |   0.5 |
|         8 |        1 | vanilla | 0.488 | 0.477 | 0.484 | 0.485 |
|         8 |        3 | LoRD-II |  0.48 | 0.379 | 0.465 | 0.489 |
|         8 |        1 | LoRD-II | 0.462 | 0.398 | 0.444 | 0.469 |
|-----------+----------+---------+-------+-------+-------+-------|
|         4 |        2 | LoRD-IV | 0.512 |  0.51 | 0.514 | 0.514 |
|         4 |        1 | LoRD-IV | 0.498 | 0.497 | 0.499 | 0.499 |
|         4 |        1 | LoRD-II | 0.495 | 0.363 |  0.54 | 0.505 |
|         4 |        2 | LoRD-II | 0.493 | 0.352 | 0.541 | 0.504 |
|         4 |        1 | vanilla | 0.491 |  0.36 | 0.509 | 0.501 |
|         4 |        3 | vanilla | 0.491 | 0.386 | 0.501 |   0.5 |
|         4 |        1 | kd      | 0.491 | 0.475 | 0.494 | 0.495 |
|         4 |        2 | kd      | 0.489 | 0.368 | 0.494 | 0.499 |
|         4 |        3 | LoRD-IV | 0.485 | 0.426 | 0.486 | 0.492 |
|         4 |        3 | kd      | 0.484 |  0.36 | 0.467 | 0.494 |
|         4 |        3 | LoRD-II | 0.482 | 0.341 | 0.428 | 0.492 |
|         4 |        2 | vanilla | 0.476 | 0.474 | 0.477 | 0.478 |
|-----------+----------+---------+-------+-------+-------+-------|
|           |        1 | gemma2b | 0.498 | 0.497 | 0.498 | 0.497 |
|           |          | gpt3.5  | 0.828 | 0.827 | 0.828 | 0.827 |
|-----------+----------+---------+-------+-------+-------+-------|

** Truthful QA [231 SERVER]

|-----------+----------+-----------+-------+-------+-------+-------|
| train-num | rep-time | method    |   acc |    f1 |   pre |   rec |
|-----------+----------+-----------+-------+-------+-------+-------|
|       512 |        2 | LoRD-II   | 0.995 | 0.499 |   0.5 | 0.498 |
|       512 |        1 | kd        |  0.49 | 0.329 |   0.5 | 0.245 |
|       512 |        1 | vanilla   | 0.316 |  0.24 |   0.5 | 0.158 |
|       512 |        3 | LoRD-IV   | 0.104 | 0.094 |   0.5 | 0.052 |
|       512 |        3 | kd        | 0.018 | 0.018 |   0.5 | 0.009 |
|       512 |        3 | vanilla   |  0.06 | 0.057 |   0.5 |  0.03 |
|       512 |        2 | vanilla   | 0.022 | 0.022 |   0.5 | 0.011 |
|       512 |        3 | LoRD-II   | 0.021 |  0.02 |   0.5 |  0.01 |
|       512 |        2 | kd        | 0.018 | 0.018 |   0.5 | 0.009 |
|       512 |        1 | LoRD-II   | 0.016 | 0.016 |   0.5 | 0.008 |
|       512 |        2 | LoRD-IV   |  0.01 |  0.01 |   0.5 | 0.005 |
|       512 |        1 | LoRD-IV   |   0.0 |   0.0 |   0.0 |   0.0 |
|-----------+----------+-----------+-------+-------+-------+-------|
|       256 |        3 | vanilla   | 0.929 | 0.482 |   0.5 | 0.465 |
|       256 |        2 | kd        | 0.651 | 0.394 |   0.5 | 0.326 |
|       256 |        3 | LoRD-IV   | 0.638 | 0.389 |   0.5 | 0.319 |
|       256 |        2 | vanilla   | 0.487 | 0.328 |   0.5 | 0.244 |
|       256 |        1 | vanilla   | 0.148 | 0.129 |   0.5 | 0.074 |
|       256 |        3 | LoRD-II   | 0.135 | 0.119 |   0.5 | 0.067 |
|       256 |        2 | LoRD-II   | 0.023 | 0.023 |   0.5 | 0.012 |
|       256 |        2 | LoRD-IV   | 0.021 |  0.02 |   0.5 |  0.01 |
|       256 |        1 | LoRD-IV   | 0.021 |  0.02 |   0.5 |  0.01 |
|       256 |        1 | kd        | 0.021 |  0.02 |   0.5 |  0.01 |
|       256 |        3 | kd        | 0.015 | 0.014 |   0.5 | 0.007 |
|       256 |        1 | LoRD-II   | 0.009 | 0.008 |   0.5 | 0.004 |
|-----------+----------+-----------+-------+-------+-------+-------|
|       100 |        1 | LoRD-II   |   1.0 |   1.0 |   1.0 |   1.0 |
|       100 |        1 | vanilla   | 0.824 | 0.452 |   0.5 | 0.412 |
|       100 |        3 | vanilla   | 0.507 | 0.336 |   0.5 | 0.253 |
|       100 |        1 | LoRD-IV   | 0.349 | 0.259 |   0.5 | 0.174 |
|       100 |        3 | LoRD-II   | 0.259 | 0.206 |   0.5 |  0.13 |
|       100 |        3 | kd        | 0.031 |  0.03 |   0.5 | 0.015 |
|       100 |        1 | kd        |  0.02 | 0.019 |   0.5 |  0.01 |
|       100 |        2 | kd        | 0.031 |  0.03 |   0.5 | 0.015 |
|       100 |        2 | vanilla   | 0.021 |  0.02 |   0.5 |  0.01 |
|       100 |        2 | LoRD-II   | 0.016 | 0.016 |   0.5 | 0.008 |
|       100 |        3 | LoRD-IV   |   0.0 |   0.0 |   0.0 |   0.0 |
|       100 |        2 | LoRD-IV   | 0.026 | 0.025 |   0.5 | 0.013 |
|-----------+----------+-----------+-------+-------+-------+-------|
|        64 |        3 | vanilla   |   1.0 |   1.0 |   1.0 |   1.0 |
|        64 |        2 | kd        | 0.998 | 0.499 |   0.5 | 0.499 |
|        64 |        2 | LoRD-II   | 0.994 | 0.498 |   0.5 | 0.497 |
|        64 |        1 | LoRD-IV   | 0.908 | 0.476 |   0.5 | 0.454 |
|        64 |        1 | vanilla   | 0.849 | 0.459 |   0.5 | 0.425 |
|        64 |        2 | vanilla   | 0.603 | 0.376 |   0.5 | 0.302 |
|        64 |        2 | LoRD-IV   |   0.0 |   0.0 |   0.0 |   0.0 |
|        64 |        1 | LoRD-II   | 0.006 | 0.006 |   0.5 | 0.003 |
|        64 |        3 | kd        | 0.055 | 0.052 |   0.5 | 0.028 |
|        64 |        3 | LoRD-IV   | 0.011 | 0.011 |   0.5 | 0.006 |
|        64 |        1 | kd        | 0.021 |  0.02 |   0.5 |  0.01 |
|        64 |        3 | LoRD-II   |  0.02 | 0.019 |   0.5 |  0.01 |
|-----------+----------+-----------+-------+-------+-------+-------|
|        32 |        1 | LoRD-II   | 0.999 |   0.5 |   0.5 | 0.499 |
|        32 |        3 | vanilla   | 0.999 |   0.5 |   0.5 | 0.499 |
|        32 |        3 | kd        | 0.967 | 0.492 |   0.5 | 0.483 |
|        32 |        2 | vanilla   | 0.996 | 0.499 |   0.5 | 0.498 |
|        32 |        2 | kd        | 0.733 | 0.423 |   0.5 | 0.367 |
|        32 |        1 | kd        | 0.073 | 0.068 |   0.5 | 0.037 |
|        32 |        1 | vanilla   | 0.436 | 0.303 |   0.5 | 0.218 |
|        32 |        3 | LoRD-IV   | 0.084 | 0.078 |   0.5 | 0.042 |
|        32 |        1 | LoRD-IV   | 0.002 | 0.002 |   0.5 | 0.001 |
|        32 |        3 | LoRD-II   | 0.016 | 0.016 |   0.5 | 0.008 |
|        32 |        2 | LoRD-II   | 0.017 | 0.017 |   0.5 | 0.009 |
|        32 |        2 | LoRD-IV   |   0.0 |   0.0 |   0.0 |   0.0 |
|-----------+----------+-----------+-------+-------+-------+-------|
|        16 |        1 | LoRD-II   |   1.0 |   1.0 |   1.0 |   1.0 |
|        16 |        3 | LoRD-II   | 0.995 | 0.499 |   0.5 | 0.498 |
|        16 |        2 | LoRD-IV   | 0.987 | 0.497 |   0.5 | 0.493 |
|        16 |        2 | vanilla   | 0.599 | 0.374 |   0.5 | 0.299 |
|        16 |        1 | LoRD-IV   | 0.201 | 0.167 |   0.5 |   0.1 |
|        16 |        1 | vanilla   | 0.026 | 0.025 |   0.5 | 0.013 |
|        16 |        1 | kd        |  0.02 | 0.019 |   0.5 |  0.01 |
|        16 |        2 | LoRD-II   | 0.028 | 0.027 |   0.5 | 0.014 |
|        16 |        2 | kd        | 0.028 | 0.027 |   0.5 | 0.014 |
|        16 |        3 | kd        | 0.015 | 0.014 |   0.5 | 0.007 |
|        16 |        3 | LoRD-IV   | 0.037 | 0.035 |   0.5 | 0.018 |
|        16 |        3 | vanilla   |  0.02 | 0.019 |   0.5 |  0.01 |
|-----------+----------+-----------+-------+-------+-------+-------|
|         8 |        1 | kd        | 0.384 | 0.278 |   0.5 | 0.192 |
|         8 |        3 | kd        | 0.252 | 0.201 |   0.5 | 0.126 |
|         8 |        2 | LoRD-II   | 0.251 | 0.201 |   0.5 | 0.125 |
|         8 |        3 | LoRD-IV   | 0.234 | 0.189 |   0.5 | 0.117 |
|         8 |        2 | vanilla   | 0.029 | 0.029 |   0.5 | 0.015 |
|         8 |        1 | LoRD-II   | 0.018 | 0.018 |   0.5 | 0.009 |
|         8 |        2 | kd        | 0.017 | 0.017 |   0.5 | 0.009 |
|         8 |        3 | vanilla   | 0.016 | 0.016 |   0.5 | 0.008 |
|         8 |        1 | vanilla   | 0.016 | 0.016 |   0.5 | 0.008 |
|         8 |        2 | LoRD-IV   | 0.011 | 0.011 |   0.5 | 0.006 |
|         8 |        3 | LoRD-II   | 0.011 | 0.011 |   0.5 | 0.006 |
|         8 |        1 | LoRD-IV   |  0.06 | 0.057 |   0.5 |  0.03 |
|-----------+----------+-----------+-------+-------+-------+-------|
|         4 |        2 | LoRD-II   | 0.987 | 0.497 |   0.5 | 0.493 |
|         4 |        1 | LoRD-II   |  0.98 | 0.495 |   0.5 |  0.49 |
|         4 |        2 | kd        |  0.86 | 0.463 |   0.5 |  0.43 |
|         4 |        1 | vanilla   | 0.777 | 0.437 |   0.5 | 0.389 |
|         4 |        2 | LoRD-IV   | 0.771 | 0.435 |   0.5 | 0.386 |
|         4 |        3 | LoRD-IV   |  0.63 | 0.387 |   0.5 | 0.315 |
|         4 |        1 | kd        |   0.6 | 0.375 |   0.5 |   0.3 |
|         4 |        2 | vanilla   | 0.302 | 0.232 |   0.5 | 0.151 |
|         4 |        1 | LoRD-IV   | 0.006 | 0.006 |   0.5 | 0.003 |
|         4 |        3 | vanilla   | 0.086 | 0.079 |   0.5 | 0.043 |
|         4 |        3 | kd        |   0.8 | 0.445 |   0.5 |   0.4 |
|         4 |        3 | LoRD-II   |   0.0 |   0.0 |   0.0 |   0.0 |
|-----------+----------+-----------+-------+-------+-------+-------|
|           |          | gemma2b   | 0.607 |  0.37 |   0.5 |  0.30 |
|           |          | 3.5-turbo | 0.414 | 0.293 | 0.500 | 0.207 |
|-----------+----------+-----------+-------+-------+-------+-------|

** AllenAI-ai2arc [231 SERVER]

|-----------+-------+-----------+-------+-------+-------+-------|
| train-num | rep-t | method    |   acc |    f1 |   pre |   rec |
|-----------+-------+-----------+-------+-------+-------+-------|
|       512 |     2 | LoRD-II   | 0.294 | 0.135 | 0.206 | 0.219 |
|       512 |     2 | LoRD-IV   | 0.278 | 0.087 | 0.056 |   0.2 |
|       512 |     1 | vanilla   | 0.278 | 0.107 | 0.192 | 0.202 |
|       512 |     1 | LoRD-IV   | 0.274 | 0.091 | 0.105 | 0.198 |
|       512 |     1 | LoRD-II   | 0.274 | 0.102 | 0.145 | 0.199 |
|       512 |     3 | LoRD-II   | 0.271 |  0.09 | 0.095 | 0.196 |
|       512 |     3 | LoRD-IV   | 0.271 | 0.095 | 0.112 | 0.196 |
|       512 |     3 | vanilla   | 0.268 | 0.096 | 0.112 | 0.194 |
|       512 |     3 | kd        | 0.268 |  0.09 | 0.088 | 0.193 |
|       512 |     2 | kd        | 0.264 |  0.09 | 0.083 | 0.191 |
|       512 |     1 | kd        | 0.264 |  0.09 | 0.083 | 0.191 |
|       512 |     2 | vanilla   | 0.261 | 0.089 | 0.087 | 0.188 |
|-----------+-------+-----------+-------+-------+-------+-------|
|       256 |     1 | LoRD-IV   | 0.284 | 0.098 | 0.256 | 0.205 |
|       256 |     2 | kd        | 0.274 | 0.097 | 0.151 | 0.199 |
|       256 |     1 | LoRD-II   | 0.274 | 0.102 | 0.295 | 0.199 |
|       256 |     3 | LoRD-IV   | 0.271 | 0.091 | 0.105 | 0.196 |
|       256 |     1 | vanilla   | 0.271 | 0.096 | 0.105 | 0.196 |
|       256 |     2 | LoRD-II   | 0.268 |  0.09 | 0.121 | 0.193 |
|       256 |     1 | kd        | 0.268 |  0.09 | 0.095 | 0.193 |
|       256 |     3 | vanilla   | 0.264 | 0.095 | 0.126 | 0.191 |
|       256 |     3 | LoRD-II   | 0.264 | 0.099 | 0.134 | 0.192 |
|       256 |     2 | LoRD-IV   | 0.264 | 0.089 |  0.12 | 0.191 |
|       256 |     2 | vanilla   | 0.258 | 0.097 | 0.116 | 0.187 |
|       256 |     3 | kd        | 0.258 | 0.083 | 0.053 | 0.186 |
|-----------+-------+-----------+-------+-------+-------+-------|
|       100 |     1 | LoRD-II   | 0.288 | 0.123 | 0.177 | 0.212 |
|       100 |     1 | vanilla   | 0.284 | 0.134 | 0.313 | 0.213 |
|       100 |     1 | LoRD-IV   | 0.278 | 0.087 | 0.056 |   0.2 |
|       100 |     2 | LoRD-IV   | 0.278 | 0.092 | 0.122 |   0.2 |
|       100 |     3 | LoRD-IV   | 0.278 | 0.087 | 0.056 |   0.2 |
|       100 |     1 | kd        | 0.278 | 0.101 | 0.122 | 0.201 |
|       100 |     2 | LoRD-II   | 0.274 | 0.102 | 0.139 | 0.199 |
|       100 |     3 | LoRD-II   | 0.271 | 0.095 |   0.1 | 0.196 |
|       100 |     2 | vanilla   | 0.268 | 0.095 | 0.116 | 0.194 |
|       100 |     3 | kd        | 0.264 | 0.089 | 0.077 | 0.191 |
|       100 |     2 | kd        | 0.261 | 0.089 | 0.076 | 0.188 |
|       100 |     3 | vanilla   | 0.261 | 0.094 | 0.103 | 0.189 |
|-----------+-------+-----------+-------+-------+-------+-------|
|        64 |     2 | vanilla   | 0.274 | 0.121 |  0.14 | 0.203 |
|        64 |     1 | LoRD-IV   | 0.274 | 0.091 | 0.105 | 0.198 |
|        64 |     1 | kd        | 0.271 |   0.1 |  0.11 | 0.196 |
|        64 |     3 | LoRD-II   | 0.271 | 0.095 |   0.1 | 0.196 |
|        64 |     2 | LoRD-II   | 0.271 | 0.095 | 0.105 | 0.196 |
|        64 |     3 | kd        | 0.268 |  0.09 | 0.088 | 0.193 |
|        64 |     2 | LoRD-IV   | 0.268 | 0.085 | 0.055 | 0.193 |
|        64 |     2 | kd        | 0.268 | 0.094 | 0.099 | 0.193 |
|        64 |     3 | LoRD-IV   | 0.264 |  0.09 | 0.083 | 0.191 |
|        64 |     1 | LoRD-II   | 0.264 |  0.09 | 0.083 | 0.191 |
|        64 |     1 | vanilla   | 0.258 | 0.084 | 0.054 | 0.186 |
|        64 |     3 | vanilla   | 0.247 | 0.081 | 0.052 | 0.178 |
|-----------+-------+-----------+-------+-------+-------+-------|
|        32 |     3 | LoRD-IV   | 0.278 | 0.087 | 0.056 |   0.2 |
|        32 |     1 | LoRD-IV   | 0.278 | 0.087 | 0.056 |   0.2 |
|        32 |     2 | LoRD-IV   | 0.278 | 0.087 | 0.056 |   0.2 |
|        32 |     3 | LoRD-II   | 0.274 | 0.096 | 0.135 | 0.198 |
|        32 |     2 | LoRD-II   | 0.271 | 0.091 | 0.094 | 0.196 |
|        32 |     3 | vanilla   | 0.268 |   0.1 | 0.144 | 0.195 |
|        32 |     2 | kd        | 0.268 | 0.119 | 0.161 | 0.198 |
|        32 |     1 | vanilla   | 0.268 | 0.095 | 0.134 | 0.194 |
|        32 |     3 | kd        | 0.264 | 0.089 |  0.08 | 0.191 |
|        32 |     1 | kd        | 0.264 | 0.089 |  0.08 | 0.191 |
|        32 |     1 | LoRD-II   | 0.261 | 0.089 | 0.088 | 0.188 |
|        32 |     2 | vanilla   | 0.237 |   0.1 |  0.08 | 0.175 |
|-----------+-------+-----------+-------+-------+-------+-------|
|        16 |     2 | LoRD-IV   | 0.281 | 0.103 | 0.289 | 0.204 |
|        16 |     2 | vanilla   | 0.278 | 0.128 | 0.104 | 0.209 |
|        16 |     1 | kd        | 0.274 | 0.087 | 0.056 | 0.198 |
|        16 |     3 | LoRD-IV   | 0.274 | 0.086 | 0.055 | 0.198 |
|        16 |     1 | LoRD-IV   | 0.271 | 0.085 | 0.055 | 0.195 |
|        16 |     3 | kd        | 0.271 |  0.09 | 0.104 | 0.196 |
|        16 |     1 | vanilla   | 0.264 |  0.11 | 0.116 | 0.195 |
|        16 |     3 | vanilla   | 0.264 | 0.095 | 0.099 | 0.192 |
|        16 |     2 | kd        | 0.261 | 0.085 | 0.055 | 0.188 |
|        16 |     1 | LoRD-II   | 0.261 | 0.111 | 0.154 | 0.191 |
|        16 |     2 | LoRD-II   | 0.244 |  0.11 | 0.084 | 0.182 |
|        16 |     3 | LoRD-II   | 0.241 | 0.096 | 0.077 | 0.176 |
|-----------+-------+-----------+-------+-------+-------+-------|
|         8 |     2 | LoRD-IV   | 0.284 | 0.099 | 0.456 | 0.206 |
|         8 |     1 | LoRD-IV   | 0.278 | 0.092 | 0.096 |   0.2 |
|         8 |     2 | vanilla   | 0.278 | 0.087 | 0.056 |   0.2 |
|         8 |     3 | kd        | 0.274 | 0.091 | 0.105 | 0.198 |
|         8 |     1 | kd        | 0.271 | 0.113 | 0.143 | 0.199 |
|         8 |     1 | LoRD-II   | 0.271 | 0.086 | 0.055 | 0.195 |
|         8 |     3 | LoRD-IV   | 0.264 | 0.085 | 0.054 |  0.19 |
|         8 |     3 | vanilla   | 0.264 |  0.09 | 0.095 | 0.191 |
|         8 |     1 | vanilla   | 0.264 | 0.108 | 0.153 | 0.193 |
|         8 |     2 | kd        | 0.261 | 0.084 | 0.054 | 0.188 |
|         8 |     3 | LoRD-II   | 0.254 | 0.083 | 0.053 | 0.183 |
|         8 |     2 | LoRD-II   | 0.247 | 0.107 | 0.092 | 0.183 |
|-----------+-------+-----------+-------+-------+-------+-------|
|         4 |     2 | vanilla   | 0.278 | 0.092 | 0.123 |   0.2 |
|         4 |     1 | LoRD-II   | 0.274 | 0.087 | 0.056 | 0.198 |
|         4 |     3 | LoRD-II   | 0.274 | 0.091 | 0.105 | 0.198 |
|         4 |     2 | LoRD-IV   | 0.274 | 0.087 | 0.055 | 0.198 |
|         4 |     3 | kd        | 0.274 | 0.107 | 0.156 |   0.2 |
|         4 |     3 | LoRD-IV   | 0.271 | 0.128 | 0.149 | 0.202 |
|         4 |     1 | kd        | 0.271 | 0.092 | 0.096 | 0.196 |
|         4 |     2 | kd        | 0.271 | 0.095 | 0.122 | 0.196 |
|         4 |     1 | vanilla   | 0.268 |  0.09 | 0.105 | 0.193 |
|         4 |     2 | LoRD-II   | 0.264 | 0.085 | 0.055 |  0.19 |
|         4 |     3 | vanilla   | 0.261 | 0.139 | 0.104 |  0.21 |
|         4 |     1 | LoRD-IV   | 0.251 | 0.146 | 0.184 | 0.202 |
|-----------+-------+-----------+-------+-------+-------+-------|
|           |       | gemma2b   | 0.241 | 0.149 | 0.200 | 0.181 |
|           |       | 3.5-turbo | 0.274 | 0.111 | 0.208 | 0.200 |
|-----------+-------+-----------+-------+-------+-------+-------|





























* LoRD reports

** Local Model 
** REVIEW LoRD-II new hyper parameters

#+BEGIN_SRC shell
export epoch=1
export period=1
export sub_set_num=33
export sub_stage_num=6
export train_num=100
export max_new_tokens=64

#+END_SRC


#+BEGIN_SRC python
LoRD-II336256cs-en64__hyper-para-search_ckpt___period5': {'bertscore': {'f1': 0.865009069442749,
                                                                                                        'p': 0.9317170977592468,
                                                                                                        'r': 0.8088542222976685},
                                                                                          'bleu': {'1': 0.00015319452012949644,
                                                                                                   '2': 0.0,
                                                                                                   '3': 0.0,
                                                                                                   '4': 0.0},
                                                                                          'rouge-l': {'f1': 0.13256355766956218,
                                                                                                      'p': 0.88,
                                                                                                      'r': 0.07541975227410604}}}

#+END_SRC

** REVIEW LoRD-IV results: 使用了更激進的tau，效果反而下降了。

#+BEGIN_SRC python
LoRD-IV1003256cs-en64__long_stage_style_ckpt___period2': {'bertscore': {'f1': 0.8108768463134766,
                                                                                                         'p': 0.7916164994239807,
                                                                                                         'r': 0.8330056667327881},
                                                                                           'bleu': {'1': 0.04351020949386707,
                                                                                                    '2': 0.007814536487901117,
                                                                                                    '3': 0.0,
                                                                                                    '4': 0.0},
                                                                                           'rouge-l': {'f1': 0.11228535223416233,
                                                                                                       'p': 0.20837741670094612,
                                                                                                       'r': 0.09534738810343459}},

#+END_SRC

** REVIEW LoRD-IV results: Strange. Not sensitive to $\tau$.
#+BEGIN_SRC python
LoRD-IV1003256cs-en64__long_stage_style_ckpt___period2/': {'bertscore': {'f1': 0.8335102796554565,
                                                                                                          'p': 0.8287380933761597,
                                                                                                          'r': 0.8393020033836365},
                                                                                            'bleu': {'1': 0.08503401823527726,
                                                                                                     '2': 0.014048052681664116,
                                                                                                     '3': 0.0,
                                                                                                     '4': 0.0},
                                                                                            'rouge-l': {'f1': 0.14820793023504303,
                                                                                                        'p': 0.24574538398515383,
                                                                                                        'r': 0.11684685503511699}}}

#+END_SRC

** REVIEW LoRD-II old with 4 samples version

#+BEGIN_SRC python
LoRD-II43256cs-en4__long_stage_style_ckpt___period2/': {'bertscore': {'f1': 0.8588850498199463,
                                                                                                      'p': 0.9043752551078796,
                                                                                                      'r': 0.8202759623527527},
                                                                                        'bleu': {'1': 0.02002250004071359,
                                                                                                 '2': 0.007277881017808857,
                                                                                                 '3': 0.0031653509899885073,
                                                                                                 '4': 0.0},
                                                                                        'rouge-l': {'f1': 0.15249048028677348,
                                                                                                    'p': 0.6692454767454769,
                                                                                                    'r': 0.1012258496142213}}}

#+END_SRC

** REVIEW LoRD-II old version

#+BEGIN_SRC python
LoRD-II1003256cs-en64__long_stage_style_ckpt___period2/': {'bertscore': {'f1': 0.8509846329689026,
                                                                                                         'p': 0.8581267595291138,
                                                                                                         'r': 0.8464218378067017},
                                                                                           'bleu': {'1': 0.1837484062411199,
                                                                                                    '2': 0.07312301954539126,
                                                                                                    '3': 0.032024450578806506,
                                                                                                    '4': 0.0},
                                                                                           'rouge-l': {'f1': 0.20952309440966274,
                                                                                                       'p': 0.4001896000800025,
                                                                                                       'r': 0.16908922868206605}}}

#+END_SRC

** REVIEW nolog, exp(y_{t-1}/y_{t-1})

#+BEGIN_SRC json
 'cs-en-----./POD_SAVE_CKPTs/vary_period0306cs-en/nolog--Complex-lord_256cs-en_test___period2/': {'bertscore': {'f1': 0.8177661299705505,
                                                                                                                'p': 0.7982205152511597,
                                                                                                                'r': 0.8404489755630493},
                                                                                                  'bleu': {'1': 0.12210805534582106,
                                                                                                           '2': 0.05704303756146297,
                                                                                                           '3': 0.03120224839684385,
                                                                                                           '4': 0.019423944126848308},
                                                                                                  'rouge-l': {'f1': 0.12436128101977616,
                                                                                                              'p': 0.16300501808824427,
                                                                                                              'r': 0.11230529785257476}}}
#+END_SRC


#+BEGIN_SRC python

mask = torch.logical_or(mask1, mask2).long()
# print(mask1)
# print(mask2)
# print(mask)
# print("_____________")
term1 = log_clip(-old_logits1+logits1)
term2 = (old_logits2-logits2_cons)

if is_black_box == 0:
    term3 = \
        (vic_logits2[:, :, 0]-logits2_cons)
else:
    term3 = - logits2_cons

loss_1 = term2 + term3
loss_2 = torch.exp(term1)

loss = sigmoid(loss_1)*loss_2

if torch.sum(mask[:, :-1]) >= 1:
    loss = torch.sum(loss*mask[:, :-1])
    # / torch.sum(mask[:, :-1])
else:
    loss = 0.

#+END_SRC
** REVIEW nolog, $y_{t-1}/y_{t-1}$ , without exp, and use +

#+BEGIN_SRC json
nolog--Complex-lord_256cs-en_test___period2/': {'bertscore': {'f1': 0.8299728631973267,
                                                                                                                'p': 0.8093873858451843,
                                                                                                                'r': 0.8528457880020142},
                                                                                                  'bleu': {'1': 0.17163788592360021,
                                                                                                           '2': 0.08542652015463408,
                                                                                                           '3': 0.04739198225193413,
                                                                                                           '4': 0.027734961721448008},
                                                                                                  'rouge-l': {'f1': 0.19153642568433896,
                                                                                                              'p': 0.22649654224830218,
                                                                                                              'r': 0.17875681995848097}}}

#+END_SRC

#+BEGIN_SRC python
    mask = torch.logical_or(mask1, mask2).long()
    # print(mask1)
    # print(mask2)
    # print(mask)
    # print("_____________")
    term1 = log_clip(-old_logits1+logits1)
    term2 = (old_logits2-logits2_cons)

    if is_black_box == 0:
        term3 = \
            (vic_logits2[:, :, 0]-logits2_cons)
    else:
        term3 = - logits2_cons

    loss_1 = term2 + term3
    # loss_2 = torch.exp(term1)
    loss_2 = term1

    loss = sigmoid(loss_1)+loss_2

    if torch.sum(mask[:, :-1]) >= 1:
        loss = torch.sum(loss*mask[:, :-1])
        # / torch.sum(mask[:, :-1])
    else:
        loss = 0.
    if loss == torch.tensor(float("nan")):
        print("++++++++++++++++++++++")
        print(f"term1: {term1}")
        print(f"term2: {term3}")
        print(f"loss1: {loss_1}")
        print(f"loss2: {loss_2}")
        print(f"loss: {loss}")
        print(f"mask: {mask[:,:-1]}")
        print("++++++++DEBUG DONE.++++++++")

    loss_constractive = loss

    loss_constractive_past = 0.
    loss_constractive_good = 0.
    loss_logits = 0.

    overall_loss += loss_constractive + loss_logits
#+END_SRC
** REVIEW nolog, same to before, but $log(\sigma)$

#+BEGIN_SRC python
nolog--Complex-lord_256cs-en_test___period2/': {'bertscore': {'f1': 0.8048646450042725,
                                                                                                                'p': 0.7763920426368713,
                                                                                                                'r': 0.8381577134132385},
                                                                                                  'bleu': {'1': 0.1424546362630487,
                                                                                                           '2': 0.06282700954931907,
                                                                                                           '3': 0.02700494647300017,
                                                                                                           '4': 0.010667637167496042},
                                                                                                  'rouge-l': {'f1': 0.14550569632496638,
                                                                                                              'p': 0.19002411304127878,
                                                                                                              'r': 0.13424617477761605}}}

#+END_SRC
** CANCELED new nolog complex training with separated aggregation
CLOSED: [2024-03-30 Sat 19:16]
Failed.
** CANCELED outside without exp
CLOSED: [2024-03-25 Mon 11:24]

#+BEGIN_SRC python
/nolog--Complex-lord_256cs-en_test___period2/': {'bertscore': {'f1': 0.7254393696784973,
                                                                                                                'p': 0.6584640741348267,
                                                                                                                'r': 0.8081263899803162},
                                                                                                  'bleu': {'1': 0.0,
                                                                                                           '2': 0.0,
                                                                                                           '3': 0.0,
                                                                                                           '4': 0.0},
                                                                                                  'rouge-l': {'f1': 0.0,
                                                                                                              'p': 0.0,
                                                                                                              'r': 0.0}}}

#+END_SRC
** CANCELED logits2 outside, logits1 and logits2 inside:  *failed* 
CLOSED: [2024-03-25 Mon 11:24]

#+BEGIN_SRC python
                mask = torch.logical_or(mask1, mask2).long()

                term1 = (-old_logits1+logits1)
                term2 = log_clip(old_logits2-logits2_cons)

                if is_black_box == 0:
                    term3 = \
                        (vic_logits2[:, :, 0]-logits2_cons)
                else:
                    term3 = - logits2_cons

                loss_1 = term1 + term3
                loss_2 = torch.exp(term2)

                loss = sigmoid(loss_1)*loss_2
#+END_SRC

** CANCELED Complex V3: failed
CLOSED: [2024-03-25 Mon 11:24]

#+BEGIN_SRC python
                mask = torch.logical_or(mask1, mask2).long()

                term1 = (-old_logits1+logits1)
                term2 = log_clip(old_logits2-logits2_cons)

                if is_black_box == 0:
                    term3 = \
                        (vic_logits2[:, :, 0]-logits2_cons)
                else:
                    term3 = - logits2_cons

                loss_1 = term1 + term3
                loss_2 = torch.exp(term2)

                loss = sigmoid(loss_1)*loss_2
#+END_SRC

So I add =log_clip= on =term2=

** CANCELED Very complex:
CLOSED: [2024-03-25 Mon 11:24]


#+BEGIN_SRC 
Very--Complex-lord_256cs-en_test___period2/': {'bertscore': {'f1': 0.8080261945724487,
                                                                                                               'p': 0.799115002155304,
                                                                                                               'r': 0.8189542889595032},
                                                                                                 'bleu': {'1': 0.13413304252998906,
                                                                                                          '2': 0.06397435463303668,
                                                                                                          '3': 0.030153920565313845,
                                                                                                          '4': 0.015185027231458436},
                                                                                                 'rouge-l': {'f1': 0.1309445596024662,
                                                                                                             'p': 0.16905697525579064,
                                                                                                             'r': 0.12311342787372885}},

#+END_SRC


#+BEGIN_SRC python
    term1 = -torch.exp(old_logits1)*(
        log_clip(old_logits1-logits1))

    if is_black_box == 0:
        term3 = torch.exp(vic_logits2[:, :, 0])\
            * (
            (vic_logits2[:, :, 0]-logits2_cons))\
            + (old_logits2 - logits2_cons)
    else:
        term3 = - logits2_cons*2

    loss_constractive_past = torch.sum(
        term1*mask1[:, :-1])
    loss_constractive_good = torch.sum(
        term3*mask2[:, :-1])

    loss_constractive = loss_constractive_good +\
        loss_constractive_past

#+END_SRC

KL divergence not worked well


